{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        ignore_wi = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wf = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wg = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wo = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "        self.wi_weights = V(ignore_wi.weight.data, requires_grad=True)\n",
    "        self.wi_bias = V(ignore_wi.bias.data, requires_grad=True)\n",
    "        self.wf_weights = V(ignore_wf.weight.data, requires_grad=True)\n",
    "        self.wf_bias = V(ignore_wf.bias.data, requires_grad=True)\n",
    "        self.wg_weights = V(ignore_wg.weight.data, requires_grad=True)\n",
    "        self.wg_bias = V(ignore_wg.bias.data, requires_grad=True)\n",
    "        self.wo_weights = V(ignore_wo.weight.data, requires_grad=True)\n",
    "        self.wo_bias = V(ignore_wo.bias.data, requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        input_plus_hidden = torch.cat((inp, hx), 2)\n",
    "\n",
    "        i_tpre = F.linear(input_plus_hidden, self.wi_weights, self.wi_bias)\n",
    "        i_t = torch.sigmoid(i_tpre)\n",
    "        f_tpre = F.linear(input_plus_hidden, self.wf_weights, self.wf_bias)\n",
    "        f_t = torch.sigmoid(f_tpre)\n",
    "        g_tpre = F.linear(input_plus_hidden, self.wg_weights, self.wg_bias)\n",
    "        g_t = torch.tanh(g_tpre)\n",
    "        fred = F.linear(input_plus_hidden, self.wo_weights, self.wo_bias)\n",
    "        o_t = torch.sigmoid(fred)\n",
    "        #o_t = torch.sigmoid(F.linear(input_plus_hidden, self.wo_weights, self.wo_bias))\n",
    "        #print(i_t)\n",
    "        #print(f_t)\n",
    "        #print(g_t)\n",
    "        #print(o_t)\n",
    "        \n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * torch.tanh(cx)\n",
    "\n",
    "        #myhook = input_plus_hidden.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), fred, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-3.0128e+01, -3.0085e+01, -9.0308e+00, -1.5620e+01, -1.7085e+01,\n",
       "           -1.4166e+01, -1.2249e+01, -1.8494e+01, -1.8177e+01, -1.7592e+01,\n",
       "           -1.6007e+01, -1.6758e+01, -1.5217e+01, -1.8921e+01, -1.6830e+01,\n",
       "           -1.1985e+01, -1.8906e+01, -2.0146e+01, -1.9777e+01, -1.8053e+01,\n",
       "           -1.7694e+01, -2.0871e+01, -1.5915e+01, -2.0352e+01, -1.7058e+01,\n",
       "           -2.1277e+01, -1.8803e+01, -1.7422e+01, -1.8139e+01, -1.7179e+01,\n",
       "           -1.6860e+01, -1.8955e+01, -1.6675e+01, -1.3243e-04]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.1458e+01, -3.1514e+01, -1.2696e+01, -2.0725e+01, -2.1216e+01,\n",
       "           -1.8132e+01, -1.2413e+01, -2.6787e+01, -2.3340e+01, -2.1781e+01,\n",
       "           -1.9499e+01, -1.9349e+01, -2.0554e+01, -1.4396e+01, -1.0657e+01,\n",
       "           -1.2007e-03, -1.3031e+01, -1.6695e+01, -1.4922e+01, -8.5660e+00,\n",
       "           -1.0671e+01, -1.6810e+01, -9.6966e+00, -1.2941e+01, -8.8985e+00,\n",
       "           -1.7387e+01, -1.4689e+01, -1.1344e+01, -1.1337e+01, -9.5578e+00,\n",
       "           -8.1104e+00, -1.4052e+01, -7.9513e+00, -1.2510e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.2465e+01, -3.2365e+01, -1.6064e+01, -1.1356e+01, -1.4245e+01,\n",
       "           -8.2012e+00, -3.6138e-04, -1.8208e+01, -1.7230e+01, -1.1003e+01,\n",
       "           -1.1052e+01, -1.0182e+01, -1.2524e+01, -2.2286e+01, -2.3338e+01,\n",
       "           -1.7358e+01, -2.2957e+01, -2.0891e+01, -2.2635e+01, -2.0161e+01,\n",
       "           -2.3131e+01, -2.5460e+01, -2.0603e+01, -2.4952e+01, -2.0323e+01,\n",
       "           -2.5752e+01, -2.2925e+01, -1.9799e+01, -2.2043e+01, -2.1411e+01,\n",
       "           -1.9796e+01, -2.1836e+01, -2.3103e+01, -1.4551e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.2987e+01, -3.2914e+01, -1.3691e+01, -2.0671e+01, -2.2749e+01,\n",
       "           -1.8437e+01, -1.3046e+01, -2.3440e+01, -2.3050e+01, -2.1586e+01,\n",
       "           -1.8183e+01, -2.0100e+01, -1.9761e+01, -1.9127e+01, -2.0199e+01,\n",
       "           -1.5839e+01, -1.8839e+01, -1.6359e+01, -1.8088e+01, -1.5985e+01,\n",
       "           -2.0927e+01, -2.2315e+01, -1.8692e+01, -2.2667e+01, -1.7069e+01,\n",
       "           -2.3079e+01, -2.1155e+01, -1.7164e+01, -1.9332e+01, -1.8724e+01,\n",
       "           -1.8634e+01, -2.0929e+01, -1.9456e+01, -3.6955e-06]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.6415e+01, -3.6358e+01, -1.2040e-05, -2.3023e+01, -2.2614e+01,\n",
       "           -2.1812e+01, -1.5537e+01, -2.5296e+01, -2.2420e+01, -2.2387e+01,\n",
       "           -2.0461e+01, -2.1042e+01, -2.0835e+01, -1.7155e+01, -1.7199e+01,\n",
       "           -1.4250e+01, -1.5113e+01, -1.8089e+01, -1.5141e+01, -1.7194e+01,\n",
       "           -1.7197e+01, -1.9182e+01, -1.5890e+01, -1.9463e+01, -1.7088e+01,\n",
       "           -1.9927e+01, -1.8929e+01, -1.6561e+01, -2.0800e+01, -1.7856e+01,\n",
       "           -1.6943e+01, -1.8801e+01, -1.6585e+01, -1.1486e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-27.9288, -28.0642,  -2.3062,  -7.8507,  -8.6287,  -6.1329,  -0.1412,\n",
       "            -9.7078, -10.2659,  -7.1892,  -4.5243,  -5.4892,  -7.1302, -14.0984,\n",
       "           -15.1886, -14.2714, -14.8629, -12.2572, -12.8246, -13.9344, -16.8249,\n",
       "           -15.3634, -15.2735, -17.0711, -13.2576, -15.2824, -13.1528, -14.4109,\n",
       "           -14.7091, -14.2334, -16.0161, -12.6451, -16.0406,  -4.3724]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.5014e+01, -3.4737e+01, -6.8128e+00, -2.2027e+01, -1.9905e+01,\n",
       "           -2.0129e+01, -1.5486e+01, -2.0703e+01, -1.9608e+01, -2.2208e+01,\n",
       "           -1.5759e+01, -1.7618e+01, -1.8821e+01, -2.0122e+01, -2.0876e+01,\n",
       "           -1.7207e+01, -1.8882e+01, -1.9501e+01, -1.6435e+01, -2.0230e+01,\n",
       "           -2.0468e+01, -2.1936e+01, -2.0557e+01, -2.1120e+01, -1.8938e+01,\n",
       "           -2.1123e+01, -2.3179e+01, -2.1242e+01, -2.2503e+01, -2.1627e+01,\n",
       "           -2.1847e+01, -1.9320e+01, -1.9698e+01, -1.1006e-03]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.9274e+01, -3.9287e+01, -2.9802e-06, -2.4830e+01, -2.3584e+01,\n",
       "           -2.4027e+01, -1.9031e+01, -2.4410e+01, -2.3356e+01, -2.4749e+01,\n",
       "           -2.0333e+01, -2.2700e+01, -2.2306e+01, -1.9177e+01, -1.8897e+01,\n",
       "           -1.6983e+01, -1.6628e+01, -1.9552e+01, -1.4786e+01, -1.9500e+01,\n",
       "           -1.9998e+01, -1.9989e+01, -2.0119e+01, -1.9580e+01, -1.9138e+01,\n",
       "           -2.0696e+01, -2.2025e+01, -1.9423e+01, -2.2180e+01, -2.0938e+01,\n",
       "           -2.0638e+01, -1.7053e+01, -1.8260e+01, -1.2934e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-27.8383, -28.0507,  -0.6987,  -6.3898,  -7.0933,  -5.4513,  -1.0779,\n",
       "            -6.9799,  -7.7686,  -6.5610,  -2.2347,  -3.6949,  -4.8496, -14.1548,\n",
       "           -14.4744, -14.2765, -12.8747, -12.4243, -10.9267, -13.8121, -14.5782,\n",
       "           -12.3825, -15.4713, -14.4637, -12.8009, -12.6800, -13.0846, -13.7006,\n",
       "           -14.0884, -14.3322, -15.6531,  -9.9442, -15.0174,  -4.3330]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.2645e+01, -3.2367e+01, -8.1642e+00, -2.0752e+01, -1.8869e+01,\n",
       "           -1.9269e+01, -1.5694e+01, -1.8576e+01, -1.8438e+01, -2.1107e+01,\n",
       "           -1.4617e+01, -1.7463e+01, -1.6868e+01, -2.0119e+01, -1.8112e+01,\n",
       "           -1.7045e+01, -1.7309e+01, -2.0862e+01, -1.5466e+01, -2.0579e+01,\n",
       "           -2.0677e+01, -1.9868e+01, -2.0750e+01, -1.8243e+01, -1.9832e+01,\n",
       "           -1.9952e+01, -2.2226e+01, -2.0518e+01, -2.2224e+01, -2.1083e+01,\n",
       "           -2.1512e+01, -1.7520e+01, -1.8028e+01, -2.8558e-04]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.5466e+01, -3.5457e+01, -4.7684e-06, -2.3781e+01, -2.2853e+01,\n",
       "           -2.3457e+01, -1.9038e+01, -2.2918e+01, -2.2891e+01, -2.4680e+01,\n",
       "           -1.9463e+01, -2.3302e+01, -2.0196e+01, -1.9247e+01, -1.7248e+01,\n",
       "           -1.5855e+01, -1.4282e+01, -2.0358e+01, -1.4185e+01, -1.8191e+01,\n",
       "           -1.9266e+01, -1.8270e+01, -1.9422e+01, -1.6952e+01, -1.8536e+01,\n",
       "           -1.9806e+01, -2.1122e+01, -1.7992e+01, -2.0807e+01, -2.0245e+01,\n",
       "           -1.9446e+01, -1.5228e+01, -1.6378e+01, -1.2767e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-25.9425, -26.1977,  -0.7024,  -5.7740,  -6.0758,  -4.8556,  -1.1780,\n",
       "            -6.8624,  -7.4578,  -7.3844,  -2.0653,  -3.9224,  -3.4733, -13.6955,\n",
       "           -13.1301, -12.8799, -11.8519, -12.2258, -10.6362, -12.5530, -14.0947,\n",
       "           -11.7296, -14.8745, -13.2687, -12.1447, -12.7530, -12.7501, -12.5773,\n",
       "           -12.4757, -14.0312, -14.2592,  -8.8587, -13.4588,  -5.6650]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.9940e+01, -2.9785e+01, -9.7295e+00, -1.9949e+01, -1.7571e+01,\n",
       "           -1.7958e+01, -1.4672e+01, -1.8041e+01, -1.7562e+01, -2.0501e+01,\n",
       "           -1.3658e+01, -1.6952e+01, -1.5395e+01, -2.0518e+01, -1.7080e+01,\n",
       "           -1.6069e+01, -1.7052e+01, -2.1009e+01, -1.6360e+01, -1.9564e+01,\n",
       "           -2.0651e+01, -1.9635e+01, -2.0181e+01, -1.8535e+01, -1.9384e+01,\n",
       "           -2.0462e+01, -2.1120e+01, -1.9433e+01, -2.0126e+01, -1.9947e+01,\n",
       "           -1.9979e+01, -1.7877e+01, -1.7095e+01, -6.1749e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.3557e+01, -3.3541e+01, -1.5259e-05, -2.3735e+01, -2.1249e+01,\n",
       "           -2.2663e+01, -1.8231e+01, -2.1996e+01, -2.1488e+01, -2.4198e+01,\n",
       "           -1.7641e+01, -2.3045e+01, -1.8602e+01, -1.8820e+01, -1.5719e+01,\n",
       "           -1.3839e+01, -1.2974e+01, -1.9875e+01, -1.4055e+01, -1.6652e+01,\n",
       "           -1.8133e+01, -1.7766e+01, -1.7919e+01, -1.6088e+01, -1.7359e+01,\n",
       "           -1.9163e+01, -2.0058e+01, -1.7350e+01, -1.9144e+01, -1.9162e+01,\n",
       "           -1.7625e+01, -1.4713e+01, -1.4228e+01, -1.1558e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-20.8260, -21.1733,  -0.6053,  -7.0447,  -5.5198,  -5.3989,  -1.6135,\n",
       "            -7.6411,  -7.3186,  -8.6872,  -1.6201,  -4.7676,  -3.3245, -12.2729,\n",
       "           -11.7427, -11.1264, -10.7609, -11.3918, -10.0808, -10.9457, -13.4032,\n",
       "           -11.1733, -13.1933, -12.1166, -11.1962, -12.5260, -11.9866, -11.4632,\n",
       "           -11.1259, -12.8698, -12.8855,  -8.4987, -11.3822,  -6.5512]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.4798e+01, -2.4774e+01, -8.8244e+00, -1.6735e+01, -1.5244e+01,\n",
       "           -1.5626e+01, -1.2416e+01, -1.5166e+01, -1.4554e+01, -1.7494e+01,\n",
       "           -1.1470e+01, -1.4053e+01, -1.2932e+01, -1.7819e+01, -1.5614e+01,\n",
       "           -1.4281e+01, -1.4994e+01, -1.7823e+01, -1.4865e+01, -1.6149e+01,\n",
       "           -1.8897e+01, -1.7740e+01, -1.6927e+01, -1.7958e+01, -1.7490e+01,\n",
       "           -1.9705e+01, -1.7615e+01, -1.5706e+01, -1.6651e+01, -1.8034e+01,\n",
       "           -1.6876e+01, -1.6644e+01, -1.5728e+01, -1.6819e-04]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.1813e+01, -3.1696e+01, -4.0292e-05, -2.2123e+01, -2.0343e+01,\n",
       "           -2.1613e+01, -1.7160e+01, -2.0126e+01, -1.9306e+01, -2.2767e+01,\n",
       "           -1.6203e+01, -2.1051e+01, -1.7188e+01, -1.7583e+01, -1.5030e+01,\n",
       "           -1.2786e+01, -1.2716e+01, -1.8387e+01, -1.3794e+01, -1.4942e+01,\n",
       "           -1.7605e+01, -1.6967e+01, -1.5846e+01, -1.6406e+01, -1.6788e+01,\n",
       "           -1.9365e+01, -1.8512e+01, -1.5937e+01, -1.7619e+01, -1.8612e+01,\n",
       "           -1.6469e+01, -1.5117e+01, -1.2954e+01, -1.0436e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-18.2117, -18.5349,  -0.3312,  -8.4592,  -6.7470,  -6.2414,  -1.9525,\n",
       "            -8.7655,  -8.0599,  -9.0609,  -2.1418,  -5.6591,  -4.3938, -11.4134,\n",
       "           -10.5294, -10.1046,  -9.7163, -10.3116,  -9.7695,  -9.7731, -12.2992,\n",
       "           -10.7776, -12.2207, -11.4165, -10.4658, -12.1115, -11.1709, -10.4850,\n",
       "            -9.9518, -12.1798, -11.2339,  -7.6711, -10.2312,  -6.2803]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.1044e+01, -2.1067e+01, -6.9122e+00, -1.4262e+01, -1.2890e+01,\n",
       "           -1.3191e+01, -9.8869e+00, -1.2810e+01, -1.2138e+01, -1.4869e+01,\n",
       "           -9.1233e+00, -1.1805e+01, -1.0465e+01, -1.5755e+01, -1.4270e+01,\n",
       "           -1.3088e+01, -1.3400e+01, -1.5801e+01, -1.3405e+01, -1.3916e+01,\n",
       "           -1.7148e+01, -1.6082e+01, -1.5451e+01, -1.6264e+01, -1.5527e+01,\n",
       "           -1.8174e+01, -1.5508e+01, -1.3519e+01, -1.4519e+01, -1.6598e+01,\n",
       "           -1.4716e+01, -1.4558e+01, -1.4297e+01, -1.2166e-03]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.9951e+01, -2.9762e+01, -1.4459e-04, -2.0293e+01, -1.9868e+01,\n",
       "           -2.0143e+01, -1.5782e+01, -1.9154e+01, -1.8468e+01, -2.1309e+01,\n",
       "           -1.5743e+01, -1.9378e+01, -1.6108e+01, -1.6847e+01, -1.3968e+01,\n",
       "           -1.2354e+01, -1.1914e+01, -1.7626e+01, -1.3412e+01, -1.4043e+01,\n",
       "           -1.6265e+01, -1.6335e+01, -1.5671e+01, -1.5677e+01, -1.5877e+01,\n",
       "           -1.8323e+01, -1.7492e+01, -1.5181e+01, -1.6662e+01, -1.7615e+01,\n",
       "           -1.5450e+01, -1.4290e+01, -1.2114e+01, -9.0061e+00]]],\n",
       "        grad_fn=<LogSoftmaxBackward>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([\"do\"])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a3ab30d4aec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"do\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-a3ab30d4aec0>\u001b[0m in \u001b[0;36mfit_example\u001b[0;34m(model, inp, outp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, meta, train, test, lr_inner, batch_size, update_embeddings)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Compute the loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Backprop the loss; setting create_graph as True enables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, batch, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(model(inp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mseq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "vocab = \"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split()\n",
    "\n",
    "def fit_example(model, inp, outp):\n",
    "    _, _, model = fit_task(model, [[[inp, outp]], [], [], vocab], test=False)\n",
    "    return model\n",
    "\n",
    "fit_example(model, \"d\", \"do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', ''],\n",
       " ['xuOuu', '.xu.O.u.u.'],\n",
       " ['xur', '.xu.'],\n",
       " ['uuxu', '.u.u.xu.'],\n",
       " ['rtrO', '.rO.'],\n",
       " ['rOtuO', '.rO.tu.O.'],\n",
       " ['OtO', '.O.tO.'],\n",
       " ['ttxO', '.xO.'],\n",
       " ['Ott', '.O.'],\n",
       " ['xr', ''],\n",
       " ['uxtO', '.u.tO.'],\n",
       " ['uuxta', '.u.u.ta.'],\n",
       " ['Oaaxt', '.O.a.a.'],\n",
       " ['r', ''],\n",
       " ['Orxuu', '.O.xu.u.'],\n",
       " ['xx', ''],\n",
       " ['xrO', '.rO.'],\n",
       " ['raxr', '.ra.'],\n",
       " ['ratt', '.ra.'],\n",
       " ['Oaxat', '.O.a.xa.'],\n",
       " ['aaaOa', '.a.a.a.O.a.'],\n",
       " ['uux', '.u.u.'],\n",
       " ['OrrOr', '.O.rO.'],\n",
       " ['t', ''],\n",
       " ['xtrrt', ''],\n",
       " ['rtrxa', '.xa.'],\n",
       " ['uOxu', '.u.O.xu.'],\n",
       " ['Oxxa', '.O.xa.'],\n",
       " ['rxurO', '.xu.rO.'],\n",
       " ['Ou', '.O.u.'],\n",
       " ['xrtu', '.tu.'],\n",
       " ['aOOuu', '.a.O.O.u.u.'],\n",
       " ['tOrt', '.tO.'],\n",
       " ['aauua', '.a.a.u.u.a.'],\n",
       " ['aOxr', '.a.O.'],\n",
       " ['r', ''],\n",
       " ['aux', '.a.u.'],\n",
       " ['xOuO', '.xO.u.O.'],\n",
       " ['atrxr', '.a.'],\n",
       " ['OOrxx', '.O.O.'],\n",
       " ['aOr', '.a.O.'],\n",
       " ['xtttr', ''],\n",
       " ['uuOra', '.u.u.O.ra.'],\n",
       " ['OOuO', '.O.O.u.O.'],\n",
       " ['rOaa', '.rO.a.a.'],\n",
       " ['auuaa', '.a.u.u.a.a.'],\n",
       " ['ratO', '.ra.tO.'],\n",
       " ['OOuO', '.O.O.u.O.'],\n",
       " ['txtt', ''],\n",
       " ['aOrut', '.a.O.ru.'],\n",
       " ['axxOx', '.a.xO.'],\n",
       " ['uxau', '.u.xa.u.'],\n",
       " ['aaux', '.a.a.u.'],\n",
       " ['rOat', '.rO.a.'],\n",
       " ['uxrta', '.u.ta.'],\n",
       " ['aruuO', '.a.ru.u.O.'],\n",
       " ['auatt', '.a.u.a.'],\n",
       " ['xOOrr', '.xO.O.'],\n",
       " ['Otrrt', '.O.'],\n",
       " ['Oxtar', '.O.ta.'],\n",
       " ['taxru', '.ta.ru.'],\n",
       " ['xOtOO', '.xO.tO.O.'],\n",
       " ['tO', '.tO.'],\n",
       " ['xxatO', '.xa.tO.'],\n",
       " ['axxO', '.a.xO.'],\n",
       " ['OarO', '.O.a.rO.'],\n",
       " ['tttrt', ''],\n",
       " ['ratuu', '.ra.tu.u.'],\n",
       " ['uxr', '.u.'],\n",
       " ['OOxaa', '.O.O.xa.a.'],\n",
       " ['uut', '.u.u.'],\n",
       " ['Oau', '.O.a.u.'],\n",
       " ['txxtt', ''],\n",
       " ['Oa', '.O.a.'],\n",
       " ['xxOO', '.xO.O.'],\n",
       " ['xtxu', '.xu.'],\n",
       " ['OaOat', '.O.a.O.a.'],\n",
       " ['tOax', '.tO.a.'],\n",
       " ['xaxta', '.xa.ta.'],\n",
       " ['auxu', '.a.u.xu.'],\n",
       " ['arOa', '.a.rO.a.'],\n",
       " ['axx', '.a.'],\n",
       " ['ttrOa', '.rO.a.'],\n",
       " ['Oua', '.O.u.a.'],\n",
       " ['ttxux', '.xu.'],\n",
       " ['urra', '.u.ra.'],\n",
       " ['uuOO', '.u.u.O.O.'],\n",
       " ['taur', '.ta.u.'],\n",
       " ['uauu', '.u.a.u.u.'],\n",
       " ['xOuOx', '.xO.u.O.'],\n",
       " ['rtxOt', '.xO.'],\n",
       " ['Oxrx', '.O.'],\n",
       " ['uxuuu', '.u.xu.u.u.'],\n",
       " ['uautr', '.u.a.u.'],\n",
       " ['xxrtu', '.tu.'],\n",
       " ['rrru', '.ru.'],\n",
       " ['Ouura', '.O.u.u.ra.'],\n",
       " ['raurt', '.ra.u.'],\n",
       " ['ruxOa', '.ru.xO.a.'],\n",
       " ['r', '']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n",
    "test_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.xu.xua..EOS.EOS.EOSa.EOSa.EOS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(['uuxta'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-084786cb4064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uuxta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a3ab30d4aec0>\u001b[0m in \u001b[0;36mfit_example\u001b[0;34m(model, inp, outp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, meta, train, test, lr_inner, batch_size, update_embeddings)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Compute the loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Backprop the loss; setting create_graph as True enables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, batch, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(model(inp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mseq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "fit_example(model, 'r', '')\n",
    "model(['uuxta'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-486765bafaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, meta, train, test, lr_inner, batch_size, update_embeddings)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Compute the loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Backprop the loss; setting create_graph as True enables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, batch, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(model(inp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mseq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "_, _, model = fit_task(model, test_set[0], lr_inner=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.xu.xua..EOS.EOS.EOSa.EOSa.EOS']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2c640cc03105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uuxta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uuxta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a3ab30d4aec0>\u001b[0m in \u001b[0;36mfit_example\u001b[0;34m(model, inp, outp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mfit_task\u001b[0;34m(model, task, meta, train, test, lr_inner, batch_size, update_embeddings)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# Compute the loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Backprop the loss; setting create_graph as True enables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metaug_js/training.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, batch, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(model(inp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mseq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "print(model(['uuxta'])[0])\n",
    "for elt in test_set[0][0]:\n",
    "    model = fit_example(model, elt[0], elt[1])\n",
    "print(model(['uuxta'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.uu.EOSu.EOSu.EOSu.EOSEOS.EOSEOS.EOS']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model('u')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        ignore_wi = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wf = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wg = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wo = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "        self.wi_weights = V(ignore_wi.weight.data, requires_grad=True)\n",
    "        self.wi_bias = V(ignore_wi.bias.data, requires_grad=True)\n",
    "        self.wf_weights = V(ignore_wf.weight.data, requires_grad=True)\n",
    "        self.wf_bias = V(ignore_wf.bias.data, requires_grad=True)\n",
    "        self.wg_weights = V(ignore_wg.weight.data, requires_grad=True)\n",
    "        self.wg_bias = V(ignore_wg.bias.data, requires_grad=True)\n",
    "        self.wo_weights = V(ignore_wo.weight.data, requires_grad=True)\n",
    "        self.wo_bias = V(ignore_wo.bias.data, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        input_plus_hidden = torch.cat((inp, hx), 2)\n",
    "        \n",
    "        \n",
    "        #myhook = hx.register_hook(print_grad)\n",
    "\n",
    "\n",
    "        i_tpre = F.linear(input_plus_hidden, self.wi_weights, self.wi_bias)\n",
    "        i_t = torch.sigmoid(i_tpre)\n",
    "        f_tpre = F.linear(input_plus_hidden, self.wf_weights, self.wf_bias)\n",
    "        f_t = torch.sigmoid(f_tpre)\n",
    "        g_tpre = F.linear(input_plus_hidden, self.wg_weights, self.wg_bias)\n",
    "        g_t = torch.tanh(g_tpre)\n",
    "        fred = F.linear(input_plus_hidden, self.wo_weights, self.wo_bias)\n",
    "        o_t = torch.sigmoid(fred)\n",
    "        #o_t = torch.sigmoid(F.linear(input_plus_hidden, self.wo_weights, self.wo_bias))\n",
    "        \n",
    "        #myhook = f_tpre.register_hook(print_grad)\n",
    "\n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * torch.tanh(cx)\n",
    "\n",
    "        #if cx.requires_grad:\n",
    "        #    myhook = cx.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), fred, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n",
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "\n",
    "\n",
    "    def forward(self, sequence_list, correct):\n",
    "        computation_graph = {}\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (V(torch.zeros(1, len(sequence_list), self.hidden_size)),\n",
    "                  V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "        \n",
    "        computation_graph[\"enc_h-1\"] = [\"init\", [(\"ZERO\", hidden[0].detach())]]\n",
    "        computation_graph[\"enc_c-1\"] = [\"init\", [(\"ZERO\", hidden[1].detach())]]\n",
    "        \n",
    "        hidden_prev = hidden\n",
    "\n",
    "        # The input is a list of sequences. Here the sequences are converted\n",
    "        # into integer keys\n",
    "        all_seqs = []\n",
    "        for sequence in sequence_list:\n",
    "            this_seq = []\n",
    "            # Iterate over the sequence\n",
    "            for elt in sequence:\n",
    "                ind = self.char2ind[elt]\n",
    "                this_seq.append(ind)\n",
    "            all_seqs.append(torch.LongTensor(this_seq))\n",
    "        max_length = max([len(x) for x in sequence_list])\n",
    "        \n",
    "        index = 0\n",
    "        if max_length > 0:\n",
    "            # Pad the sequences to allow batching \n",
    "            all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "\n",
    "            all_seqs_onehot = (all_seqs > 0).type(torch.FloatTensor)\n",
    "\n",
    "            index = 0\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(all_seqs):\n",
    "                cprev_name = \"enc_c\" + str(index-1)\n",
    "                hprev_name = \"enc_h\" + str(index-1)\n",
    "                \n",
    "                # Embed the character\n",
    "                emb = self.embedding(elt.unsqueeze(0))\n",
    "                \n",
    "                computation_graph[\"enc_input\" + str(index)] = [\"emb\", [(\"onehot\", elt), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "                computation_graph[\"enc_inputhidden\" + str(index)] = [\"concat\", [(\"enc_input\" + str(index), emb), (hprev_name, hidden)]]\n",
    "\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, o_t, iph, i_tpre, f_tpre, g_tpre = self.enc_lstm(emb, hidden)\n",
    "                \n",
    "                \n",
    "                \n",
    "                computation_graph[\"enc_h\" + str(index)] = [\"tanhsigmoideltwisemul\", [(\"enc_c\" + str(index), hidden_new[1].detach()), (\"enc_o\" + str(index), o_t.detach())]]\n",
    "                computation_graph[\"enc_c\" + str(index)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"enc_f\" + str(index), f_tpre.detach()), (\"enc_i\" + str(index), i_tpre.detach()), (\"enc_g\" + str(index), g_tpre.detach())]]\n",
    "                computation_graph[\"enc_o\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wo\", self.enc_lstm.wo_weights),(\"enc_bo\", self.enc_lstm.wo_bias)]]\n",
    "                computation_graph[\"enc_f\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wf\", self.enc_lstm.wf_weights),(\"enc_bf\", self.enc_lstm.wf_bias)]]\n",
    "                computation_graph[\"enc_i\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wi\", self.enc_lstm.wi_weights),(\"enc_bi\", self.enc_lstm.wi_bias)]]\n",
    "                computation_graph[\"enc_g\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wg\", self.enc_lstm.wg_weights),(\"enc_bg\", self.enc_lstm.wg_bias)]]\n",
    "\n",
    "                \n",
    "                \n",
    "                hidden_prev = hidden_new\n",
    "\n",
    "                # Awkward solution to variable length inputs: For each sequence in the batch, use the\n",
    "                # new hidden state if the sequence is still being updated, or retain the old\n",
    "                # hidden state if the sequence is over and we're now in the padding\n",
    "                hx = hidden_prev[0] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)) + hidden_new[0] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)\n",
    "                cx = hidden_prev[1] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)) + hidden_new[1] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)\n",
    "\n",
    "                hidden = (hx, cx)\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = [\"SOS\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_strings = [\"\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "        \n",
    "        cprev_name = \"enc_c\" + str(index)\n",
    "        hprev_name = \"enc_h\" + str(index)\n",
    "\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            if correct[i-1] == \"EOS\":\n",
    "                #print(correct[:i-1])\n",
    "                break\n",
    "            \n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            prev_outputs = []\n",
    "            for elt in prev_output:\n",
    "                ind = self.char2ind[elt]\n",
    "                prev_outputs.append(ind)\n",
    "\n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding(torch.LongTensor([prev_outputs]))\n",
    "            \n",
    "            computation_graph[\"dec_input\" + str(i)] = [\"emb\", [(\"onehot\", ind), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "            \n",
    "            computation_graph[\"dec_inputhidden\" + str(i)] = [\"concat\", [(\"dec_input\" + str(i), emb), (hprev_name, hidden)]]\n",
    "\n",
    "            hidden_prev = hidden\n",
    "            \n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output(output)\n",
    "            probs = F.log_softmax(pred, dim=2)\n",
    "            logits.append(probs)\n",
    "\n",
    "            \n",
    "            computation_graph[\"logit\" + str(i)] = [\"logsoftmax\", [(\"pred\" + str(i), pred.detach()), self.char2ind[correct[i]]]]\n",
    "            computation_graph[\"pred\" + str(i)] = [\"weightbias\", [(\"dec_h\" + str(i), output.detach()),(\"output_weights\", self.dec_output.weights),(\"output_bias\", self.dec_output.bias)]]\n",
    "            computation_graph[\"dec_h\" + str(i)] = [\"tanhsigmoideltwisemul\", [(\"dec_c\" + str(i), hidden[1].detach()), (\"dec_o\" + str(i), o_t.detach())]]\n",
    "            computation_graph[\"dec_c\" + str(i)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"dec_f\" + str(i), f_tpre.detach()), (\"dec_i\" + str(i), i_tpre.detach()), (\"dec_g\" + str(i), g_tpre.detach())]]\n",
    "            computation_graph[\"dec_o\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wo\", self.dec_lstm.wo_weights),(\"dec_bo\", self.dec_lstm.wo_bias)]]\n",
    "            computation_graph[\"dec_f\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wf\", self.dec_lstm.wf_weights),(\"dec_bf\", self.dec_lstm.wf_bias)]]\n",
    "            computation_graph[\"dec_i\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wi\", self.dec_lstm.wi_weights),(\"dec_bi\", self.dec_lstm.wi_bias)]]\n",
    "            computation_graph[\"dec_g\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wg\", self.dec_lstm.wg_weights),(\"dec_bg\", self.dec_lstm.wg_bias)]]\n",
    "\n",
    "            \n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            label = topi[0]\n",
    "\n",
    "            prev_output = []\n",
    "            for index, elt in enumerate(label):\n",
    "                char = self.ind2char[elt.item()]\n",
    "\n",
    "                out_strings[index] += char\n",
    "                prev_output.append(char)\n",
    "                \n",
    "            cprev_name = \"dec_c\" + str(i)\n",
    "            hprev_name = \"dec_h\" + str(i)\n",
    "\n",
    "        return out_strings, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts, computation_graph\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grad(args, result):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_grad(args):\n",
    "    name, pred = args[0]\n",
    "    correct_ind = args[1]\n",
    "    \n",
    "    \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][correct_ind] = 1.0\n",
    "\n",
    "    exped = torch.exp(pred)\n",
    "    sm = (exped/torch.sum(exped)).view(-1).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    mat_grad = torch.transpose(sm,0,1).expand(34,34) - torch.eye(34)\n",
    "    \n",
    "    #print(mat_grad)\n",
    "\n",
    "    grad_pred = torch.matmul(onehot, torch.transpose(mat_grad,0,1))\n",
    "    #print(grad_pred)\n",
    "    #14/0\n",
    "\n",
    "    return [(name, grad_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightbias_grad(args, result):\n",
    "    name_inp, inp = args[0]\n",
    "    inp = inp.view(-1).unsqueeze(0)\n",
    "    \n",
    "    name_weight, weight = args[1]\n",
    "    name_bias, bias = args[2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    grad_bias = result\n",
    "    grad_weight = torch.transpose(torch.mm(torch.transpose(inp,0,1),grad_bias),0,1)\n",
    "    grad_inp = torch.mm(grad_bias, weight).view(-1).unsqueeze(0)\n",
    "    \n",
    "    if name_weight == \"dec_wo\":\n",
    "        print(\"going now\")\n",
    "        print(grad_weight)\n",
    "        #14/0\n",
    "    \n",
    "    #print(\"going now\")\n",
    "    #print(grad_inp)\n",
    "    #14/0\n",
    "    \n",
    "    return [(name_weight, grad_weight), (name_bias, grad_bias), (name_inp, grad_inp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_grad(args, result):\n",
    "    name_ind, ind = args[0]\n",
    "    name_weight, weight = args[1]\n",
    "        \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][ind] = 1.0\n",
    "    \n",
    "    grad_weight = torch.mm(torch.transpose(onehot,0,1),result)\n",
    "    \n",
    "    return [(name_weight, grad_weight)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanhsigmoideltwisemul_grad(args, result):\n",
    "\n",
    "    name_c, ct = args[0]\n",
    "    name_o, ot = args[1]\n",
    "    \n",
    "    grad_ot = torch.sigmoid(ot) * (1 - torch.sigmoid(ot)) * torch.tanh(ct) * result\n",
    "    grad_ct = torch.sigmoid(ot) * result * (1 - torch.pow(torch.tanh(ct),2))\n",
    "    \n",
    "    grad_ot = grad_ot.view(-1).unsqueeze(0)\n",
    "    grad_ct = grad_ct.view(-1).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    #print(name_c, name_o, grad_ot)\n",
    "    #14/0\n",
    "    \n",
    "    return [(name_c, grad_ct), (name_o, grad_ot)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newc_grad(args, result):\n",
    "    name_cprev, cprev = args[0]\n",
    "    name_f, ft = args[1]\n",
    "    name_i, it = args[2]\n",
    "    name_g, gt = args[3]\n",
    "    \n",
    "    #print(name_f, result)\n",
    "    \n",
    "    grad_cprev = result * torch.sigmoid(ft) # Might be wrong\n",
    "    grad_ft = cprev * result * torch.sigmoid(ft) * (1 - torch.sigmoid(ft))\n",
    "    grad_it = torch.sigmoid(it) * (1 - torch.sigmoid(it)) * torch.tanh(gt) * result\n",
    "    grad_gt = (1 - torch.pow(torch.tanh(gt),2)) * torch.sigmoid(it) * result\n",
    "    \n",
    "    grad_cprev = grad_cprev.view(-1).unsqueeze(0)\n",
    "    grad_ft = grad_ft.view(-1).unsqueeze(0)\n",
    "    grad_it = grad_it.view(-1).unsqueeze(0)\n",
    "    grad_gt = grad_gt.view(-1).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    return [(name_cprev, grad_cprev), (name_f, grad_ft), (name_i, grad_it), (name_g, grad_gt)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_grad(args, result):\n",
    "    \n",
    "    inp_name, inp = args[0]\n",
    "    hprev_name, hprev = args[1]\n",
    "    \n",
    "    #print(inp_name)\n",
    "    #if inp_name == \"dec_input1\":\n",
    "    #    print(result)\n",
    "\n",
    "    \n",
    "    grad_inp, grad_hprev = torch.split(result, [10,256], dim=1)\n",
    "    \n",
    "    return [(inp_name, grad_inp), (hprev_name, grad_hprev)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {}\n",
    "def init_grads():\n",
    "    gradients[\"dec_wi\"] = None\n",
    "    gradients[\"dec_wf\"] = None\n",
    "    gradients[\"dec_wg\"] = None\n",
    "    gradients[\"dec_wo\"] = None\n",
    "\n",
    "    gradients[\"dec_bi\"] = None\n",
    "    gradients[\"dec_bf\"] = None\n",
    "    gradients[\"dec_bg\"] = None\n",
    "    gradients[\"dec_bo\"] = None\n",
    "\n",
    "    gradients[\"enc_wi\"] = None\n",
    "    gradients[\"enc_wf\"] = None\n",
    "    gradients[\"enc_wg\"] = None\n",
    "    gradients[\"enc_wo\"] = None\n",
    "\n",
    "    gradients[\"enc_bi\"] = None\n",
    "    gradients[\"enc_bf\"] = None\n",
    "    gradients[\"enc_bg\"] = None\n",
    "    gradients[\"enc_bo\"] = None\n",
    "\n",
    "    gradients[\"output_weights\"] = None\n",
    "    gradients[\"output_bias\"] = None\n",
    "\n",
    "    gradients[\"emb_mat\"] = None\n",
    "    \n",
    "init_grads()\n",
    "\n",
    "function2grad = {}\n",
    "function2grad[\"init\"] = init_grad\n",
    "function2grad[\"logsoftmax\"] = logsoftmax_grad\n",
    "function2grad[\"weightbias\"] = weightbias_grad\n",
    "function2grad[\"emb\"] = emb_grad\n",
    "function2grad[\"tanhsigmoideltwisemul\"] = tanhsigmoideltwisemul_grad\n",
    "function2grad[\"newc\"] = newc_grad\n",
    "function2grad[\"concat\"] = concat_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_gradient(cg, name, gradients):\n",
    "    done = False\n",
    "        \n",
    "    grad_type, args = cg[name]\n",
    "    grad_function = function2grad[grad_type]\n",
    "    \n",
    "    results = {}\n",
    "    for result in grad_function(args):\n",
    "        name, grad = result\n",
    "        if name in results:\n",
    "            results[name] = results[name] + grad\n",
    "        else:\n",
    "            results[name] = grad\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if len(results.keys()) == 0:\n",
    "            done = True\n",
    "            break\n",
    "                    \n",
    "        results_new = {}\n",
    "        for name in results:\n",
    "            grad = results[name]\n",
    "            \n",
    "            if name in gradients:\n",
    "                if gradients[name] is None:\n",
    "                    gradients[name] = grad\n",
    "                else:\n",
    "                    gradients[name] = gradients[name] + grad\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                grad_type, args = cg[name]\n",
    "                grad_function = function2grad[grad_type]\n",
    "\n",
    "                to_add = grad_function(args, grad)\n",
    "                for result in to_add:\n",
    "                    name, grad = result\n",
    "                    if name in results_new:\n",
    "                        results_new[name] = results_new[name] + grad\n",
    "                    else:\n",
    "                        results_new[name] = grad\n",
    "\n",
    "        results = results_new \n",
    "        \n",
    "    results = None\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "\n",
    "\n",
    "    def forward(self, sequence_list,outp_length=20):\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (V(torch.zeros(1, len(sequence_list), self.hidden_size)),\n",
    "                  V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "\n",
    "        # The input is a list of sequences. Here the sequences are converted\n",
    "        # into integer keys\n",
    "        all_seqs = []\n",
    "        for sequence in sequence_list:\n",
    "            this_seq = []\n",
    "            # Iterate over the sequence\n",
    "            for elt in sequence:\n",
    "                ind = self.char2ind[elt]\n",
    "                this_seq.append(ind)\n",
    "            all_seqs.append(torch.LongTensor(this_seq))\n",
    "\n",
    "\n",
    "        max_length = max([len(x) for x in sequence_list])\n",
    "        if max_length > 0:\n",
    "            # Pad the sequences to allow batching \n",
    "            all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "\n",
    "            all_seqs_onehot = (all_seqs > 0).type(torch.FloatTensor)\n",
    "\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(all_seqs):\n",
    "                # Embed the character\n",
    "                emb = self.embedding(elt.unsqueeze(0))\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, _, _, i_tpre, f_tpre, g_tpre = self.enc_lstm(emb, hidden)\n",
    "                hidden_prev = hidden\n",
    "\n",
    "                # Awkward solution to variable length inputs: For each sequence in the batch, use the\n",
    "                # new hidden state if the sequence is still being updated, or retain the old\n",
    "                # hidden state if the sequence is over and we're now in the padding\n",
    "                hx = hidden_prev[0] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)) + hidden_new[0] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)\n",
    "                cx = hidden_prev[1] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)) + hidden_new[1] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)\n",
    "\n",
    "                hidden = (hx, cx)\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = [\"SOS\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_strings = [\"\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "\n",
    "\n",
    "        for i in range(min(self.max_length,outp_length)):\n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            prev_outputs = []\n",
    "            for elt in prev_output:\n",
    "                ind = self.char2ind[elt]\n",
    "                prev_outputs.append(ind)\n",
    "\n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding(torch.LongTensor([prev_outputs]))\n",
    "\n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output(output)\n",
    "            probs = F.log_softmax(pred, dim=2)\n",
    "            logits.append(probs)\n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            label = topi[0]\n",
    "\n",
    "            prev_output = []\n",
    "            for index, elt in enumerate(label):\n",
    "                char = self.ind2char[elt.item()]\n",
    "\n",
    "                out_strings[index] += char\n",
    "                prev_output.append(char)\n",
    "\n",
    "        return out_strings, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model([\"pupper\"])[1]\n",
    "\n",
    "loss = nn.NLLLoss()(logits[5][0], torch.LongTensor([2]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3515e-03,  2.9408e-02,  1.1093e-02,  6.6330e-04,  1.4253e-02,\n",
       "        -8.4675e-03,  2.5246e-02,  7.3320e-03,  2.5733e-05, -7.6111e-03,\n",
       "        -1.6332e-02,  2.5448e-02,  2.4310e-02, -4.7012e-03,  5.6451e-03,\n",
       "        -1.3913e-02,  4.6702e-03,  3.8348e-03, -8.6943e-04,  5.8168e-03,\n",
       "        -4.0098e-03,  1.5037e-06, -1.6368e-02, -1.3672e-03, -5.3236e-02,\n",
       "         5.1594e-07,  1.0429e-03, -7.8096e-04,  9.3993e-03, -1.3862e-05,\n",
       "        -1.2612e-04, -1.9402e-02, -4.0748e-03,  6.2396e-02, -2.4651e-04,\n",
       "         1.2660e-03, -1.5943e-04, -4.5980e-03,  6.1830e-03, -2.5843e-02,\n",
       "        -2.4863e-03,  2.2018e-03,  7.2560e-03,  2.1281e-01, -2.2434e-03,\n",
       "        -3.8275e-02, -8.8932e-03,  2.4336e-02, -3.8336e-03,  6.0414e-03,\n",
       "         5.0022e-03, -5.1081e-02, -1.1953e-03,  1.3771e-02,  5.4724e-03,\n",
       "         6.8106e-04,  3.7398e-04,  6.4555e-03,  3.5177e-04, -3.1556e-03,\n",
       "         1.2770e-02, -1.8671e-03, -5.2609e-03,  3.9564e-03,  4.4370e-02,\n",
       "        -2.0329e-02, -7.2770e-02, -7.8968e-02,  1.0307e-02,  7.3801e-03,\n",
       "         3.7272e-04, -6.3720e-04,  9.0285e-03, -3.7326e-04,  2.5464e-03,\n",
       "         7.8380e-02, -2.4505e-03,  1.4380e-04,  3.0192e-03, -3.2496e-03,\n",
       "         1.6851e-03,  6.1902e-03, -8.9758e-03, -8.3941e-03, -2.3274e-02,\n",
       "        -1.6546e-02,  4.0122e-03, -6.5781e-04, -3.3985e-05, -3.0052e-03,\n",
       "        -7.7295e-03, -8.7071e-03,  2.2500e-04,  9.6690e-03, -3.6383e-03,\n",
       "        -9.7151e-03,  3.4217e-05, -2.9865e-02,  8.2524e-04, -1.8187e-03,\n",
       "        -7.9067e-03, -8.6677e-03, -2.2641e-03, -1.9649e-03,  2.7078e-03,\n",
       "         1.4472e-02,  3.1209e-03, -1.3045e-02,  2.4107e-04,  1.0940e-01,\n",
       "         5.9338e-03, -2.9008e-04, -3.1570e-03, -6.7938e-03, -4.7455e-03,\n",
       "         1.0529e-02,  1.5032e-03,  3.3469e-02,  8.2341e-03,  4.2870e-03,\n",
       "         1.7257e-03, -3.9033e-06,  2.5588e-02,  9.2900e-03,  1.1947e-03,\n",
       "        -5.7786e-02,  4.2986e-04,  2.5360e-03,  1.9688e-02,  2.8989e-03,\n",
       "        -6.4170e-03,  8.3123e-04, -1.4966e-03,  8.1783e-03,  1.1540e-01,\n",
       "         2.3600e-01,  1.3386e-03, -2.6609e-02, -9.4232e-03, -1.3904e-02,\n",
       "         5.5787e-03,  1.0891e-02, -4.3300e-03,  4.4282e-03, -1.2537e-02,\n",
       "        -8.6103e-03,  1.0670e-02, -3.2385e-03,  2.4021e-03,  6.9921e-03,\n",
       "        -2.3894e-03, -1.0529e-02,  4.2493e-02, -3.0617e-02,  1.6389e-02,\n",
       "        -3.3295e-03, -1.3716e-03,  1.4371e-02,  8.4612e-04, -3.2942e-02,\n",
       "         2.7643e-03, -7.3317e-03,  1.3390e-02, -1.8266e-02, -1.3463e-03,\n",
       "         4.1898e-05,  1.5042e-02, -2.3161e-03,  2.8877e-02,  1.3257e-04,\n",
       "         7.1077e-03, -1.8163e-03, -3.2128e-03, -3.4507e-03,  2.2509e-03,\n",
       "        -5.2227e-03,  6.8916e-03, -1.9739e-02, -1.4498e-02,  2.6387e-03,\n",
       "        -2.6875e-03, -1.0347e-03, -3.1298e-03, -5.7367e-02,  2.7296e-03,\n",
       "         4.8189e-03, -4.2974e-03, -2.0258e-04,  9.2461e-03,  1.6784e-03,\n",
       "         4.0326e-03,  4.4436e-02,  2.1710e-03,  1.0722e-02,  1.0003e-03,\n",
       "         3.1014e-03, -2.1822e-04,  1.3985e-03, -2.5167e-03, -3.3154e-02,\n",
       "        -5.9960e-03,  4.0120e-03, -3.2847e-03, -7.9145e-03, -2.7071e-02,\n",
       "         1.3650e-02, -2.9919e-05,  3.7062e-04,  3.8209e-02,  1.9657e-04,\n",
       "         2.0106e-03, -1.4383e-04,  1.1880e-02,  3.6426e-02,  1.5372e-02,\n",
       "        -1.7699e-02, -3.7161e-02, -6.7927e-03,  9.3401e-03,  3.1360e-03,\n",
       "        -1.4703e-02,  4.9757e-02, -2.1713e-03,  3.8105e-03,  1.0965e-02,\n",
       "        -5.2394e-04, -1.9846e-03,  2.9683e-02, -4.2627e-03,  9.3629e-03,\n",
       "         1.7551e-03, -9.5521e-03, -5.0363e-03, -3.7765e-03,  7.3153e-03,\n",
       "        -1.9446e-03, -1.3280e-04,  6.7150e-03,  6.4508e-03,  2.5354e-02,\n",
       "         1.7349e-02,  1.9241e-02,  3.9878e-05, -7.0419e-04,  1.4144e-01,\n",
       "         2.7811e-02, -4.9810e-03,  6.3134e-02, -2.7278e-03, -1.6695e-02,\n",
       "        -1.7503e-02,  6.0225e-04, -2.2876e-03, -4.7099e-05,  1.8540e-03,\n",
       "        -4.5342e-03])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dec_lstm.wo_bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_grads()\n",
    "output = backprop_gradient(cg,\"logit2\",gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients[\"dec_wi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model([\"do\"], \"ddgsqmrt.................\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_grads()\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name_dict = {}\n",
    "param_name_dict[\"embedding.weights\"] = \"emb_mat\"\n",
    "param_name_dict[\"enc_lstm.wi_weights\"] = \"enc_wi\"\n",
    "param_name_dict[\"enc_lstm.wi_bias\"] = \"enc_bi\"\n",
    "param_name_dict[\"enc_lstm.wf_weights\"] = \"enc_wf\"\n",
    "param_name_dict[\"enc_lstm.wf_bias\"] = \"enc_bf\"\n",
    "param_name_dict[\"enc_lstm.wg_weights\"] = \"enc_wg\"\n",
    "param_name_dict[\"enc_lstm.wg_bias\"] = \"enc_bg\"\n",
    "param_name_dict[\"enc_lstm.wo_weights\"] = \"enc_wo\"\n",
    "param_name_dict[\"enc_lstm.wo_bias\"] = \"enc_bo\"\n",
    "param_name_dict[\"dec_lstm.wi_weights\"] = \"dec_wi\"\n",
    "param_name_dict[\"dec_lstm.wi_bias\"] = \"dec_bi\"\n",
    "param_name_dict[\"dec_lstm.wf_weights\"] = \"dec_wf\"\n",
    "param_name_dict[\"dec_lstm.wf_bias\"] = \"dec_bf\"\n",
    "param_name_dict[\"dec_lstm.wg_weights\"] = \"dec_wg\"\n",
    "param_name_dict[\"dec_lstm.wg_bias\"] = \"dec_bg\"\n",
    "param_name_dict[\"dec_lstm.wo_weights\"] = \"dec_wo\"\n",
    "param_name_dict[\"dec_lstm.wo_bias\"] = \"dec_bo\"\n",
    "param_name_dict[\"dec_output.weights\"] = \"output_weights\"\n",
    "param_name_dict[\"dec_output.bias\"] = \"output_bias\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set):\n",
    "    model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "    \n",
    "    for index, elt in enumerate(train_set):\n",
    "        print(index)\n",
    "        \n",
    "        inp, outp = elt\n",
    "        print(inp)\n",
    "        \n",
    "        all_outs = model([inp], list(outp) + [\"EOS\", \"z\", \"z\", \"z\"])\n",
    "        \n",
    "        print(outp)\n",
    "        print(all_outs[0][0])\n",
    "        print(outp == all_outs[0][0][:-3])\n",
    "        print(\"\")\n",
    "        cg = all_outs[11]\n",
    "\n",
    "        init_grads()\n",
    "        gc.collect()\n",
    "        if len(outp) > 0:\n",
    "            for i in range(len(outp) + 1):\n",
    "                backprop_gradient(cg,\"logit\"+str(i),gradients)\n",
    "\n",
    "            for name, param in model.named_params():\n",
    "                 model.set_param(name, param - 0.01 * gradients[param_name_dict[name]])\n",
    "                    \n",
    "        cg = None\n",
    "        all_outs = None\n",
    "            \n",
    "      \n",
    "    init_grads()\n",
    "    gc.collect()\n",
    "    cg = None\n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,test_set[1][0][:20])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model([\"r\"], \"...........................\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
