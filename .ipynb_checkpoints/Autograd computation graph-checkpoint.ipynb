{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        ignore_wi = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wf = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wg = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wo = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "        self.wi_weights = V(ignore_wi.weight.data, requires_grad=True)\n",
    "        self.wi_bias = V(ignore_wi.bias.data, requires_grad=True)\n",
    "        self.wf_weights = V(ignore_wf.weight.data, requires_grad=True)\n",
    "        self.wf_bias = V(ignore_wf.bias.data, requires_grad=True)\n",
    "        self.wg_weights = V(ignore_wg.weight.data, requires_grad=True)\n",
    "        self.wg_bias = V(ignore_wg.bias.data, requires_grad=True)\n",
    "        self.wo_weights = V(ignore_wo.weight.data, requires_grad=True)\n",
    "        self.wo_bias = V(ignore_wo.bias.data, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        input_plus_hidden = torch.cat((inp, hx), 2)\n",
    "        \n",
    "        \n",
    "        #myhook = hx.register_hook(print_grad)\n",
    "\n",
    "\n",
    "        i_tpre = F.linear(input_plus_hidden, self.wi_weights, self.wi_bias)\n",
    "        i_t = torch.sigmoid(i_tpre)\n",
    "        f_tpre = F.linear(input_plus_hidden, self.wf_weights, self.wf_bias)\n",
    "        f_t = torch.sigmoid(f_tpre)\n",
    "        g_tpre = F.linear(input_plus_hidden, self.wg_weights, self.wg_bias)\n",
    "        g_t = torch.tanh(g_tpre)\n",
    "        fred = F.linear(input_plus_hidden, self.wo_weights, self.wo_bias)\n",
    "        o_t = torch.sigmoid(fred)\n",
    "        #o_t = torch.sigmoid(F.linear(input_plus_hidden, self.wo_weights, self.wo_bias))\n",
    "        \n",
    "        #myhook = f_tpre.register_hook(print_grad)\n",
    "\n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * torch.tanh(cx)\n",
    "\n",
    "        if cx.requires_grad:\n",
    "            myhook = cx.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), fred, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "\n",
    "\n",
    "    def forward(self, sequence_list, correct):\n",
    "        computation_graph = {}\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (V(torch.zeros(1, len(sequence_list), self.hidden_size)),\n",
    "                  V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "        \n",
    "        computation_graph[\"enc_h-1\"] = [\"init\", [(\"ZERO\", hidden[0].detach())]]\n",
    "        computation_graph[\"enc_c-1\"] = [\"init\", [(\"ZERO\", hidden[1].detach())]]\n",
    "        \n",
    "        hidden_prev = hidden\n",
    "\n",
    "        # The input is a list of sequences. Here the sequences are converted\n",
    "        # into integer keys\n",
    "        all_seqs = []\n",
    "        for sequence in sequence_list:\n",
    "            this_seq = []\n",
    "            # Iterate over the sequence\n",
    "            for elt in sequence:\n",
    "                ind = self.char2ind[elt]\n",
    "                this_seq.append(ind)\n",
    "            all_seqs.append(torch.LongTensor(this_seq))\n",
    "        max_length = max([len(x) for x in sequence_list])\n",
    "        if max_length > 0:\n",
    "            # Pad the sequences to allow batching \n",
    "            all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "\n",
    "            all_seqs_onehot = (all_seqs > 0).type(torch.FloatTensor)\n",
    "\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(all_seqs):\n",
    "                cprev_name = \"enc_c\" + str(index-1)\n",
    "                hprev_name = \"enc_h\" + str(index-1)\n",
    "                \n",
    "                # Embed the character\n",
    "                emb = self.embedding(elt.unsqueeze(0))\n",
    "                \n",
    "                computation_graph[\"enc_input\" + str(index)] = [\"emb\", [(\"onehot\", elt), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "                computation_graph[\"enc_inputhidden\" + str(index)] = [\"concat\", [(\"enc_input\" + str(index), emb), (hprev_name, hidden)]]\n",
    "\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, o_t, iph, i_tpre, f_tpre, g_tpre = self.enc_lstm(emb, hidden)\n",
    "                \n",
    "                \n",
    "                \n",
    "                computation_graph[\"enc_h\" + str(index)] = [\"tanhsigmoideltwisemul\", [(\"enc_c\" + str(index), hidden_new[1].detach()), (\"enc_o\" + str(index), o_t.detach())]]\n",
    "                computation_graph[\"enc_c\" + str(index)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"enc_f\" + str(index), f_tpre.detach()), (\"enc_i\" + str(index), i_tpre.detach()), (\"enc_g\" + str(index), g_tpre.detach())]]\n",
    "                computation_graph[\"enc_o\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wo\", self.enc_lstm.wo_weights),(\"enc_bo\", self.enc_lstm.wo_bias)]]\n",
    "                computation_graph[\"enc_f\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wf\", self.enc_lstm.wf_weights),(\"enc_bf\", self.enc_lstm.wf_bias)]]\n",
    "                computation_graph[\"enc_i\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wi\", self.enc_lstm.wi_weights),(\"enc_bi\", self.enc_lstm.wi_bias)]]\n",
    "                computation_graph[\"enc_g\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wg\", self.enc_lstm.wg_weights),(\"enc_bg\", self.enc_lstm.wg_bias)]]\n",
    "\n",
    "                \n",
    "                \n",
    "                hidden_prev = hidden_new\n",
    "\n",
    "                # Awkward solution to variable length inputs: For each sequence in the batch, use the\n",
    "                # new hidden state if the sequence is still being updated, or retain the old\n",
    "                # hidden state if the sequence is over and we're now in the padding\n",
    "                hx = hidden_prev[0] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)) + hidden_new[0] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)\n",
    "                cx = hidden_prev[1] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)) + hidden_new[1] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)\n",
    "\n",
    "                hidden = (hx, cx)\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = [\"SOS\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_strings = [\"\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "        \n",
    "        cprev_name = \"enc_c\" + str(index)\n",
    "        hprev_name = \"enc_h\" + str(index)\n",
    "\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            prev_outputs = []\n",
    "            for elt in prev_output:\n",
    "                ind = self.char2ind[elt]\n",
    "                prev_outputs.append(ind)\n",
    "\n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding(torch.LongTensor([prev_outputs]))\n",
    "            \n",
    "            computation_graph[\"dec_input\" + str(i)] = [\"emb\", [(\"onehot\", ind), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "            \n",
    "            computation_graph[\"dec_inputhidden\" + str(i)] = [\"concat\", [(\"dec_input\" + str(i), emb), (hprev_name, hidden)]]\n",
    "\n",
    "            hidden_prev = hidden\n",
    "            \n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output(output)\n",
    "            probs = F.log_softmax(pred, dim=2)\n",
    "            logits.append(probs)\n",
    "\n",
    "            \n",
    "            computation_graph[\"logit\" + str(i)] = [\"logsoftmax\", [(\"pred\" + str(i), pred.detach()), self.char2ind[correct[i]]]]\n",
    "            computation_graph[\"pred\" + str(i)] = [\"weightbias\", [(\"dec_h\" + str(i), output.detach()),(\"output_weights\", self.dec_output.weights),(\"output_bias\", self.dec_output.bias)]]\n",
    "            computation_graph[\"dec_h\" + str(i)] = [\"tanhsigmoideltwisemul\", [(\"dec_c\" + str(i), hidden[1].detach()), (\"dec_o\" + str(i), o_t.detach())]]\n",
    "            computation_graph[\"dec_c\" + str(i)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"dec_f\" + str(i), f_tpre.detach()), (\"dec_i\" + str(i), i_tpre.detach()), (\"dec_g\" + str(i), g_tpre.detach())]]\n",
    "            computation_graph[\"dec_o\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wo\", self.dec_lstm.wo_weights),(\"dec_bo\", self.dec_lstm.wo_bias)]]\n",
    "            computation_graph[\"dec_f\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wf\", self.dec_lstm.wf_weights),(\"dec_bf\", self.dec_lstm.wf_bias)]]\n",
    "            computation_graph[\"dec_i\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wi\", self.dec_lstm.wi_weights),(\"dec_bi\", self.dec_lstm.wi_bias)]]\n",
    "            computation_graph[\"dec_g\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wg\", self.dec_lstm.wg_weights),(\"dec_bg\", self.dec_lstm.wg_bias)]]\n",
    "\n",
    "            \n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            label = topi[0]\n",
    "\n",
    "            prev_output = []\n",
    "            for index, elt in enumerate(label):\n",
    "                char = self.ind2char[elt.item()]\n",
    "\n",
    "                out_strings[index] += char\n",
    "                prev_output.append(char)\n",
    "                \n",
    "            cprev_name = \"dec_c\" + str(i)\n",
    "            hprev_name = \"dec_h\" + str(i)\n",
    "\n",
    "        return out_strings, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts, computation_graph\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grad(args, result):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_grad(args):\n",
    "    name, pred = args[0]\n",
    "    correct_ind = args[1]\n",
    "    \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][correct_ind] = 1.0\n",
    "\n",
    "    exped = torch.exp(pred)\n",
    "    sm = (exped/torch.sum(exped)).view(-1).unsqueeze(0)\n",
    "\n",
    "    mat_grad = torch.transpose(sm,0,1).expand(34,34) - torch.eye(34)\n",
    "\n",
    "    grad_pred = torch.matmul(onehot, torch.transpose(mat_grad,0,1))\n",
    "\n",
    "    return [(name, grad_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightbias_grad(args, result):\n",
    "    name_inp, inp = args[0]\n",
    "    inp = inp.view(-1).unsqueeze(0)\n",
    "    name_weight, weight = args[1]\n",
    "    name_bias, bias = args[2]\n",
    "    \n",
    "    grad_bias = result\n",
    "    grad_weight = torch.transpose(torch.mm(torch.transpose(inp,0,1),grad_bias),0,1)\n",
    "    grad_inp = torch.mm(grad_bias, weight).view(-1).unsqueeze(0)\n",
    "    \n",
    "    return [(name_weight, grad_weight), (name_bias, grad_bias), (name_inp, grad_inp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_grad(args, result):\n",
    "    name_ind, ind = args[0]\n",
    "    name_weight, weight = args[1]\n",
    "    \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][ind] = 1.0\n",
    "    \n",
    "    grad_weight = torch.mm(torch.transpose(onehot,0,1),result)\n",
    "    \n",
    "    return [(name_weight, grad_weight)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanhsigmoideltwisemul_grad(args, result):\n",
    "    name_c, ct = args[0]\n",
    "    name_o, ot = args[1]\n",
    "    \n",
    "    grad_ot = torch.sigmoid(ot) * (1 - torch.sigmoid(ot)) * torch.tanh(ct) * result\n",
    "    grad_ct = torch.sigmoid(ot) * result * (1 - torch.pow(torch.tanh(ct),2))\n",
    "    \n",
    "    grad_ot = grad_ot.view(-1).unsqueeze(0)\n",
    "    grad_ct = grad_ct.view(-1).unsqueeze(0)\n",
    "    \n",
    "    return [(name_c, grad_ct), (name_o, grad_ot)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newc_grad(args, result):\n",
    "    name_cprev, cprev = args[0]\n",
    "    name_f, ft = args[1]\n",
    "    name_i, it = args[2]\n",
    "    name_g, gt = args[3]\n",
    "    \n",
    "    grad_cprev = result * torch.sigmoid(ft) # Might be wrong\n",
    "    grad_ft = cprev * result * torch.sigmoid(ft) * (1 - torch.sigmoid(ft))\n",
    "    grad_it = torch.sigmoid(it) * (1 - torch.sigmoid(it)) * torch.tanh(gt) * result\n",
    "    grad_gt = (1 - torch.pow(torch.tanh(gt),2)) * torch.sigmoid(it) * result\n",
    "    \n",
    "    grad_cprev = grad_cprev.view(-1).unsqueeze(0)\n",
    "    grad_ft = grad_ft.view(-1).unsqueeze(0)\n",
    "    grad_it = grad_it.view(-1).unsqueeze(0)\n",
    "    grad_gt = grad_gt.view(-1).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    return [(name_cprev, grad_cprev), (name_f, grad_ft), (name_i, grad_it), (name_g, grad_gt)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_grad(args, result):\n",
    "    inp_name, inp = args[0]\n",
    "    hprev_name, hprev = args[1]\n",
    "    \n",
    "    grad_inp, grad_hprev = torch.split(result, [10,256], dim=1)\n",
    "    \n",
    "    return [(inp_name, grad_inp), (hprev_name, grad_hprev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {}\n",
    "gradients[\"dec_wi\"] = None\n",
    "gradients[\"dec_wf\"] = None\n",
    "gradients[\"dec_wg\"] = None\n",
    "gradients[\"dec_wo\"] = None\n",
    "\n",
    "gradients[\"dec_bi\"] = None\n",
    "gradients[\"dec_bf\"] = None\n",
    "gradients[\"dec_bg\"] = None\n",
    "gradients[\"dec_bo\"] = None\n",
    "\n",
    "gradients[\"enc_wi\"] = None\n",
    "gradients[\"enc_wf\"] = None\n",
    "gradients[\"enc_wg\"] = None\n",
    "gradients[\"enc_wo\"] = None\n",
    "\n",
    "gradients[\"enc_bi\"] = None\n",
    "gradients[\"enc_bf\"] = None\n",
    "gradients[\"enc_bg\"] = None\n",
    "gradients[\"enc_bo\"] = None\n",
    "\n",
    "gradients[\"output_weights\"] = None\n",
    "gradients[\"output_bias\"] = None\n",
    "\n",
    "gradients[\"emb_mat\"] = None\n",
    "\n",
    "function2grad = {}\n",
    "function2grad[\"init\"] = init_grad\n",
    "function2grad[\"logsoftmax\"] = logsoftmax_grad\n",
    "function2grad[\"weightbias\"] = weightbias_grad\n",
    "function2grad[\"emb\"] = emb_grad\n",
    "function2grad[\"tanhsigmoideltwisemul\"] = tanhsigmoideltwisemul_grad\n",
    "function2grad[\"newc\"] = newc_grad\n",
    "function2grad[\"concat\"] = concat_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_results(results):\n",
    "    decs = []\n",
    "    encs = []\n",
    "    others = []\n",
    "    \n",
    "    for result in results:\n",
    "        name = result[0]\n",
    "        if name[:3] == \"enc\":\n",
    "            encs.append(result)\n",
    "        elif name[:3] == \"dec\":\n",
    "            decs.append(result)\n",
    "        else:\n",
    "            others.append(result)\n",
    "            \n",
    "    return decs + encs + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_gradient(cg, name, gradients):\n",
    "    done = False\n",
    "    \n",
    "    interm_gradients = {}\n",
    "    \n",
    "    grad_type, args = cg[name]\n",
    "    grad_function = function2grad[grad_type]\n",
    "    \n",
    "    results = grad_function(args)\n",
    "    \n",
    "    while not done:\n",
    "        print([r[0] for r in results])\n",
    "        if results == []:\n",
    "            done = True\n",
    "            break\n",
    "            \n",
    "        contains_dec = results[0][0][:3] == \"dec\"\n",
    "        print(contains_dec)\n",
    "            \n",
    "        \n",
    "        results_new = []\n",
    "        for result in results:\n",
    "            name, grad = result\n",
    "            if name == \"enc_c0\":\n",
    "                print(\"here it is!\")\n",
    "            if contains_dec and result[0][:3] != \"dec\":\n",
    "                results_new.append(result)\n",
    "                #print(\"adding\", result[0])\n",
    "                continue\n",
    "                \n",
    "            name, grad = result\n",
    "            if name in gradients:\n",
    "                if name == \"enc_c0\":\n",
    "                    print(\"updating\")\n",
    "                    print(grad)\n",
    "                if gradients[name] is None:\n",
    "                    gradients[name] = grad\n",
    "                else:\n",
    "                    gradients[name] = gradients[name] + grad\n",
    "            else:\n",
    "                if name == \"enc_c0\":\n",
    "                    print(\"updating\")\n",
    "                    #print(grad)\n",
    "                if name in interm_gradients:\n",
    "                    interm_gradients[name] = interm_gradients[name] + grad\n",
    "                    \n",
    "                else:\n",
    "                    interm_gradients[name] = grad\n",
    "                grad_type, args = cg[name]\n",
    "                grad_function = function2grad[grad_type]\n",
    "\n",
    "                results_new = results_new + grad_function(args, grad)\n",
    "\n",
    "        results = sort_results(results_new)\n",
    "        \n",
    "        \n",
    "    return results_new, interm_gradients\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "cg = model([\"d\"], \".....................\")[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.3931e-01,  6.9094e-02,  4.0202e-03, -8.8672e-03, -1.1825e-02,\n",
      "          -4.1266e-02,  3.4893e-02,  1.0484e-01,  3.2202e-03,  9.5029e-02,\n",
      "           2.0861e-01,  2.0864e-01,  1.1967e-01, -6.4139e-02,  4.9992e-04,\n",
      "           1.1397e-01,  3.5812e-01,  4.7062e-01,  9.0996e-02,  3.7696e-01,\n",
      "          -1.2074e-01,  1.4445e-02, -6.2341e-02, -1.3772e-03,  3.3585e-02,\n",
      "           1.6912e-01,  1.1109e-01,  1.4714e-04, -5.0178e-02, -1.4443e-02,\n",
      "          -9.1349e-02,  1.3468e-01,  3.1671e-02,  3.3135e-03,  1.8037e-01,\n",
      "          -5.2121e-02, -2.4404e-01,  2.4586e-01,  1.3810e-01, -6.1891e-02,\n",
      "          -9.1680e-02, -8.0907e-03, -4.5414e-03, -4.9270e-02, -7.8413e-03,\n",
      "           1.2321e-02,  2.0757e-03, -4.3205e-02,  4.8133e-02, -3.2909e-01,\n",
      "          -2.3658e-02, -4.2870e-02, -5.5963e-02,  4.1287e-02, -2.1622e-01,\n",
      "           3.7514e-04,  7.5480e-02, -2.1279e-02,  3.5426e-02,  5.9570e-02,\n",
      "           1.9885e-01,  1.4978e-01,  6.1599e-02, -3.1100e-02,  1.5463e-01,\n",
      "          -6.4519e-02, -2.8873e-03,  1.5562e-01,  1.9168e-02, -1.7698e-02,\n",
      "          -7.6496e-04, -6.7920e-04,  1.1507e-02,  1.1578e-02,  2.0321e-01,\n",
      "           5.0254e-02,  6.7661e-02,  1.8154e-01,  9.8879e-05, -1.1837e-03,\n",
      "           8.3826e-03,  6.9075e-02, -2.1047e-01,  1.2498e-03,  4.8295e-02,\n",
      "           1.3891e-04,  3.1336e-02,  4.8198e-02, -1.4540e-01, -1.7261e-02,\n",
      "           2.6339e-01,  1.3609e-01,  5.7544e-03,  1.8020e-01, -6.3895e-02,\n",
      "           3.7491e-03,  9.6351e-02, -4.6581e-01, -4.8168e-01, -6.4757e-02,\n",
      "           2.3643e-01,  1.2526e-02,  2.2488e-03,  2.3297e-01, -2.1381e-03,\n",
      "           2.9102e-03,  4.7860e-02, -5.5950e-03,  8.6526e-04,  2.8819e-04,\n",
      "          -2.1024e-01, -1.3705e-01, -2.2997e-02,  1.1825e-04,  1.4405e-02,\n",
      "          -2.4105e-01, -6.8992e-02,  8.1960e-02, -1.0694e-01, -2.2651e-01,\n",
      "           1.4938e-02, -1.6743e-01, -6.2379e-03,  8.7943e-02,  2.6939e-02,\n",
      "           3.1663e-02,  2.2990e-01,  8.1357e-04,  1.1039e-02,  5.3287e-02,\n",
      "           1.1018e-02,  7.1237e-04,  6.2676e-04,  3.4892e-01,  1.5296e-02,\n",
      "           7.9193e-02, -6.5687e-02, -1.0803e-02,  1.1330e-02, -1.2870e-01,\n",
      "          -6.8205e-02,  7.4864e-02,  1.5430e-02, -1.3101e-03, -1.3401e-04,\n",
      "          -6.1739e-01,  1.0569e-03, -6.3667e-05, -2.5126e-01, -4.1157e-02,\n",
      "           1.6773e-02, -2.1034e-02, -4.0609e-02,  1.0990e-02,  9.2572e-03,\n",
      "           8.8445e-02, -6.1083e-01,  3.5766e-02, -2.1644e-01,  8.9683e-03,\n",
      "          -9.5367e-03,  2.5152e-01,  4.5261e-01,  8.2423e-03, -3.4176e-02,\n",
      "           8.8392e-02,  1.8527e-01,  1.6699e-03, -6.1231e-02, -1.4753e-01,\n",
      "          -4.0980e-02,  1.0456e-01,  1.8684e-03, -3.2011e-01,  6.2012e-03,\n",
      "          -5.1310e-04,  6.8863e-02,  2.1044e-01,  6.5800e-03, -3.7918e-02,\n",
      "           4.4557e-03,  5.0085e-04, -3.7447e-02, -4.7498e-02,  2.1347e-02,\n",
      "           1.1899e-01, -1.3045e-02,  1.1108e-04, -5.1856e-01,  1.5576e-03,\n",
      "          -6.5821e-03,  7.4184e-02, -1.4829e-02, -2.0660e-02,  5.3579e-01,\n",
      "           1.3290e-02, -3.3630e-01, -2.1375e-01, -1.5846e-02, -1.1078e-02,\n",
      "          -1.5489e-01, -1.6478e-01,  1.9013e-01,  4.2243e-02, -3.5052e-02,\n",
      "          -3.2517e-01, -6.8869e-01, -5.9990e-03,  8.4277e-02,  1.6593e-01,\n",
      "           2.3488e-03, -8.3154e-03,  2.6188e-02, -1.8878e-01, -1.5426e-03,\n",
      "           1.1289e-01, -2.1829e-01, -4.5980e-05,  1.1099e-01, -1.6945e-03,\n",
      "           2.3707e-02, -1.0632e-02, -1.0303e-01,  2.6970e-02,  4.4413e-02,\n",
      "          -1.9752e-01,  2.7581e-04,  2.0331e-03,  8.6971e-01,  4.7720e-02,\n",
      "           4.2228e-03,  1.9248e-04,  4.4373e-02, -3.9964e-05,  7.7293e-03,\n",
      "           9.0170e-02,  9.5476e-03,  1.9534e-02, -5.7648e-04,  4.4717e-01,\n",
      "          -3.1762e-03, -1.6377e-01,  7.1125e-01, -6.6788e-02,  8.5141e-02,\n",
      "           3.8781e-03, -3.5083e-02,  1.1667e-01,  1.7282e-01,  2.9020e-04,\n",
      "          -8.5299e-02, -1.0908e-01,  2.9723e-03, -1.8618e-02, -3.1349e-01,\n",
      "          -5.9712e-02]]], grad_fn=<TanhBackwardBackward>)\n",
      "tensor([[[-7.6343e-01,  2.8821e-01,  2.2603e-02,  4.2585e-02,  4.5970e-01,\n",
      "           1.1911e-01,  2.1751e-03,  1.0366e-01,  1.3653e-01,  9.5653e-02,\n",
      "          -3.0947e-01,  3.0301e-01,  1.6842e-01, -9.7699e-02,  5.0340e-03,\n",
      "          -1.1438e-01,  3.5180e-01, -6.7858e-01,  5.3403e-02,  2.0384e-01,\n",
      "          -2.5872e-01,  8.1468e-02, -9.4247e-02, -2.7366e-02,  4.9285e-02,\n",
      "           2.1124e-01,  6.1366e-02,  9.7330e-03,  5.1393e-02,  6.6620e-03,\n",
      "           2.7665e-01,  2.2864e-01, -2.1797e-01, -3.8722e-02,  1.2291e-01,\n",
      "          -4.3216e-02, -1.8936e-01,  1.4102e-01,  3.3920e-02, -7.8083e-02,\n",
      "           4.5861e-02, -6.7152e-02, -2.1536e-02,  2.8714e-01, -1.0896e-02,\n",
      "           3.4571e-02, -1.0766e-03,  2.2614e-01,  2.5757e-01, -1.5307e-01,\n",
      "          -1.6521e-01,  5.5468e-01, -1.0896e-01,  3.6422e-01,  1.9840e-02,\n",
      "          -5.0932e-02, -1.8607e-01,  1.1711e-02,  4.8586e-02,  4.6913e-02,\n",
      "          -2.9490e-01, -1.4798e-01,  6.9339e-02,  1.4208e-02,  3.6916e-02,\n",
      "           5.5357e-01,  3.2576e-01, -4.7346e-02,  1.2000e-02, -8.1189e-02,\n",
      "           1.1231e-01, -2.0894e-02, -1.3697e-01,  4.5023e-02,  7.5746e-02,\n",
      "           4.4500e-01,  9.2803e-02,  2.6044e-01, -1.4763e-02, -7.8066e-02,\n",
      "          -4.4290e-01, -8.7786e-02, -1.6237e-01,  6.7865e-02,  5.0619e-02,\n",
      "          -6.2497e-02,  1.6187e-01, -1.3459e-01,  4.5314e-01,  1.3903e-01,\n",
      "           1.0621e-02,  1.0571e-01,  6.2487e-02,  1.1841e-01,  1.3967e-03,\n",
      "           7.8476e-02, -1.2981e-01, -5.2621e-02,  1.1500e+00,  3.7614e-02,\n",
      "           7.9934e-01,  7.0987e-02,  7.1847e-02,  1.2843e-03, -1.1151e-01,\n",
      "          -3.1870e-02,  6.3806e-02,  5.1533e-02,  7.1897e-02,  6.7329e-02,\n",
      "          -1.4159e-02, -8.1313e-02, -2.6139e-01,  1.5527e-01, -1.6033e-02,\n",
      "          -2.3192e-01,  5.0020e-03,  2.0519e-01, -3.3510e-02, -5.9432e-02,\n",
      "           1.4101e-01,  3.5592e-02, -1.8532e-02, -1.9221e-01,  1.1814e-01,\n",
      "          -1.1608e-01,  1.6855e-01, -1.1264e-01, -8.0351e-02, -3.1921e-01,\n",
      "           1.5224e-01,  8.2570e-02,  1.4532e-02,  2.3941e-01, -3.2410e-02,\n",
      "           1.8084e-02, -2.3743e-02, -2.3599e-02, -1.1451e-01, -1.2654e-01,\n",
      "          -1.5404e-01,  1.3952e-02,  7.7692e-02,  4.4102e-02,  2.0639e-01,\n",
      "          -4.2935e-01, -1.0550e-01,  7.7590e-03, -2.9839e-01, -1.4503e-02,\n",
      "           3.7760e-02, -9.0827e-03,  1.1245e-01,  6.2807e-02, -2.4377e-02,\n",
      "          -9.5748e-02,  2.8358e-01,  2.0157e-01, -3.3068e-01,  5.3426e-02,\n",
      "           2.1610e-02,  2.4137e-01,  1.4506e-01,  6.2709e-02,  1.8820e-01,\n",
      "          -1.2981e-01,  3.8326e-01,  2.5713e-02,  1.6495e-01, -1.0361e-01,\n",
      "          -1.4465e-02,  1.5944e-01, -1.3618e-02, -3.1066e-01,  1.2025e-01,\n",
      "           1.2365e-03, -2.0549e-01, -8.8077e-02,  3.0499e-02, -3.6857e-02,\n",
      "          -5.6895e-02, -1.8388e-02, -2.0636e-02, -2.6131e-02, -3.0903e-01,\n",
      "          -1.2217e-01,  6.1675e-02,  2.6150e-02,  5.3662e-02, -2.0276e-02,\n",
      "          -1.4630e-02,  5.5124e-02, -7.3035e-02, -1.4240e-01,  6.9095e-01,\n",
      "          -2.9851e-02, -2.3702e-01, -1.8780e-01,  1.9100e-01, -3.3801e-02,\n",
      "          -6.2614e-02, -1.3609e-01,  4.0990e-01, -1.4340e-01,  2.3991e-02,\n",
      "          -3.6657e-01, -8.9971e-01, -3.6620e-02,  8.9612e-02,  1.7962e-03,\n",
      "          -1.8210e-03, -9.2921e-03,  3.3990e-02,  1.3680e-01,  2.4007e-01,\n",
      "           1.0630e-02, -5.1054e-02, -2.2787e-02, -5.5380e-02, -1.6288e-02,\n",
      "          -5.1411e-02, -6.3608e-02, -6.5958e-02, -1.0731e-03,  2.4170e-01,\n",
      "          -2.3136e-02,  4.7859e-03,  6.7722e-02,  7.7167e-01, -4.9200e-02,\n",
      "           1.2631e-02,  1.3695e-01,  4.0924e-02, -4.5157e-03, -3.5127e-02,\n",
      "           1.2534e-01,  9.4685e-02,  3.4315e-02,  4.7950e-02,  6.0034e-01,\n",
      "          -2.2759e-02, -9.3268e-02, -9.8961e-01, -1.5021e-01,  1.1574e-01,\n",
      "           1.1238e-02, -4.5558e-03,  9.2432e-02,  6.6840e-02,  2.4998e-01,\n",
      "          -7.6201e-02,  2.0755e-02,  5.4941e-03, -1.2155e-02, -3.2388e-01,\n",
      "          -1.0747e-01]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-7.1977e-01,  1.4539e-01, -1.6387e-01,  8.2689e-02,  4.0329e-01,\n",
      "           1.5321e-01,  4.1341e-02, -3.2413e-02, -7.3000e-03,  7.7886e-02,\n",
      "          -2.9893e-01,  4.3827e-02,  2.2747e-01, -1.4946e-01, -4.0394e-01,\n",
      "           1.6689e-01,  2.1212e-01,  1.1071e-01, -7.0017e-02,  2.9882e-02,\n",
      "          -4.7600e-01,  1.8358e-03, -3.6782e-02, -1.7561e-01, -5.7741e-02,\n",
      "           2.0249e-01, -1.1291e-02, -1.0544e-01, -1.6947e-02,  2.0117e-02,\n",
      "          -3.1555e-01, -8.3094e-03, -5.0895e-02, -4.2153e-02,  2.2724e-01,\n",
      "           1.8692e-01, -7.5385e-02, -7.0477e-03, -4.4935e-02, -6.1755e-02,\n",
      "           6.6735e-02, -4.2529e-02, -3.9835e-02,  1.1641e-01, -3.2653e-02,\n",
      "           1.2277e-01, -5.0368e-02,  3.5194e-01,  2.1169e-01,  1.6096e-01,\n",
      "          -1.2598e-01, -3.7930e-01,  7.1646e-02,  4.8571e-01, -2.8319e-02,\n",
      "          -6.5050e-01,  6.8146e-01, -1.9651e-02,  6.5562e-02,  8.0061e-02,\n",
      "          -2.4541e-01,  3.8522e-03,  1.5605e-01, -3.8752e-02,  4.3813e-02,\n",
      "           6.2528e-01,  2.0907e-01, -5.8540e-01,  6.3039e-02, -4.7899e-02,\n",
      "           1.3921e-01,  1.3625e-01, -1.6379e-01,  9.5486e-03,  3.7586e-02,\n",
      "           2.4421e-01, -7.5364e-02,  2.2725e-01,  1.8479e-01,  2.5448e-02,\n",
      "          -2.2037e-01, -1.5792e-01, -1.9170e-01, -7.5791e-01,  4.1324e-02,\n",
      "           1.3758e-01,  1.1581e-01, -3.9741e-01, -6.0176e-01,  1.0285e-01,\n",
      "          -9.7459e-02,  4.1670e-02, -1.3101e-01,  3.6419e-02, -1.5415e-01,\n",
      "          -8.6263e-02, -7.2880e-02, -4.7955e-01, -3.5315e-01,  5.6482e-02,\n",
      "           3.3195e-01,  3.0840e-02,  1.7539e-01,  1.4307e-01, -7.0077e-02,\n",
      "          -8.8970e-02,  9.8667e-03,  3.3696e-02, -3.5031e-01,  1.2029e-01,\n",
      "          -2.1711e-01,  1.8905e-02, -4.0701e-01,  5.4215e-02,  1.7035e-02,\n",
      "          -2.0139e-01, -2.9124e-01,  7.1523e-02,  1.7586e-01, -6.7294e-02,\n",
      "           2.0684e-01,  6.1368e-02,  6.2642e-02, -4.8749e-02,  2.7605e-01,\n",
      "           6.0373e-02, -2.8706e-02, -3.3021e-01,  3.1025e-01,  2.8706e-01,\n",
      "          -1.7681e-01, -6.6184e-02, -2.6356e-02, -8.7089e-02, -7.8395e-02,\n",
      "          -3.6035e-02, -1.4577e-03,  2.3478e-02, -1.2270e-01, -4.4948e-01,\n",
      "          -9.2463e-02,  1.0387e-01,  7.5597e-02,  2.2354e-01,  5.2392e-01,\n",
      "          -2.3690e-01, -1.7408e-01, -6.4488e-02,  1.3585e-01,  9.6816e-03,\n",
      "           6.8141e-03,  9.2246e-04,  1.0294e-01, -1.9314e-02, -6.7495e-02,\n",
      "          -2.7444e-02,  2.6982e-01,  1.8666e-01, -5.8186e-01,  4.6977e-03,\n",
      "          -1.0348e-02,  2.8832e-01, -6.8762e-02,  2.3354e-02,  4.2701e-01,\n",
      "           4.4214e-01,  4.5391e-02, -1.2948e-01,  2.9626e-01,  3.6457e-03,\n",
      "          -2.0664e-02,  1.8896e-02, -1.1768e-01, -2.5014e-01,  1.8439e-01,\n",
      "          -8.1240e-02, -1.5397e-01,  6.5178e-02, -9.3620e-02, -5.0780e-02,\n",
      "           2.0511e-01,  1.3880e-01, -4.3268e-02, -8.0173e-03,  2.4464e-02,\n",
      "           1.1647e-01, -1.4422e-01, -1.9976e-01, -2.6336e-02,  6.0190e-02,\n",
      "           1.6346e-01, -1.0615e-01,  8.1175e-02, -3.1034e-01,  8.0431e-01,\n",
      "           4.0536e-02, -2.1236e-01, -1.5389e-01, -2.4693e-01, -7.6039e-02,\n",
      "           2.4897e-01, -1.4128e-01,  1.3889e-01, -1.5625e-01,  3.8373e-01,\n",
      "          -2.2403e-01, -7.5096e-01, -7.5433e-02, -3.5064e-02,  2.5256e-01,\n",
      "           5.3152e-02,  1.9397e-01, -2.4653e-04,  3.4368e-01,  1.2535e-02,\n",
      "           1.9328e-02,  5.2507e-02,  2.5189e-02, -1.6481e-01, -4.4788e-02,\n",
      "           2.1414e-02,  1.0324e-01,  4.7561e-02,  2.5463e-01,  1.9332e-01,\n",
      "           1.1076e-01, -1.0612e-01,  8.0429e-04,  2.4534e-01, -1.3345e-02,\n",
      "          -7.5170e-02, -6.1184e-02,  5.1403e-02, -7.2987e-02, -7.4184e-02,\n",
      "           6.1569e-02,  1.1187e-01,  2.5583e-01, -7.8525e-02,  5.0729e-01,\n",
      "           1.5425e-01,  1.9200e-01, -2.5075e-01, -1.5818e-01,  3.7926e-01,\n",
      "          -6.4434e-02, -2.1278e-01,  3.2870e-02,  1.5924e-01, -9.8142e-02,\n",
      "          -7.9621e-02,  1.6295e-01, -2.2487e-02,  8.4589e-02, -3.3733e-01,\n",
      "          -1.6543e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()(model([\"d\"],\"....................\")[1][1][0],torch.LongTensor([33]))\n",
    "loss.backward(create_graph=True, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pred1']\n",
      "False\n",
      "['dec_h1', 'output_weights', 'output_bias']\n",
      "True\n",
      "['dec_c1', 'dec_o1', 'output_weights', 'output_bias']\n",
      "True\n",
      "['dec_c0', 'dec_f1', 'dec_i1', 'dec_g1', 'dec_wo', 'dec_bo', 'dec_inputhidden1', 'output_weights', 'output_bias']\n",
      "True\n",
      "['dec_f0', 'dec_i0', 'dec_g0', 'dec_wf', 'dec_bf', 'dec_inputhidden1', 'dec_wi', 'dec_bi', 'dec_inputhidden1', 'dec_wg', 'dec_bg', 'dec_inputhidden1', 'dec_input1', 'dec_h0', 'enc_c0', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "['dec_wf', 'dec_bf', 'dec_inputhidden0', 'dec_wi', 'dec_bi', 'dec_inputhidden0', 'dec_wg', 'dec_bg', 'dec_inputhidden0', 'dec_input1', 'dec_h0', 'dec_input1', 'dec_h0', 'dec_input1', 'dec_h0', 'dec_c0', 'dec_o0', 'enc_c0', 'emb_mat', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "['dec_input0', 'dec_input0', 'dec_input0', 'dec_c0', 'dec_o0', 'dec_c0', 'dec_o0', 'dec_c0', 'dec_o0', 'dec_f0', 'dec_i0', 'dec_g0', 'dec_wo', 'dec_bo', 'dec_inputhidden0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "here it is!\n",
      "['dec_f0', 'dec_i0', 'dec_g0', 'dec_wo', 'dec_bo', 'dec_inputhidden0', 'dec_f0', 'dec_i0', 'dec_g0', 'dec_wo', 'dec_bo', 'dec_inputhidden0', 'dec_f0', 'dec_i0', 'dec_g0', 'dec_wo', 'dec_bo', 'dec_inputhidden0', 'dec_wf', 'dec_bf', 'dec_inputhidden0', 'dec_wi', 'dec_bi', 'dec_inputhidden0', 'dec_wg', 'dec_bg', 'dec_inputhidden0', 'dec_input0', 'enc_c0', 'enc_c0', 'enc_c0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "['dec_wf', 'dec_bf', 'dec_inputhidden0', 'dec_wi', 'dec_bi', 'dec_inputhidden0', 'dec_wg', 'dec_bg', 'dec_inputhidden0', 'dec_input0', 'dec_wf', 'dec_bf', 'dec_inputhidden0', 'dec_wi', 'dec_bi', 'dec_inputhidden0', 'dec_wg', 'dec_bg', 'dec_inputhidden0', 'dec_input0', 'dec_wf', 'dec_bf', 'dec_inputhidden0', 'dec_wi', 'dec_bi', 'dec_inputhidden0', 'dec_wg', 'dec_bg', 'dec_inputhidden0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'enc_c0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "['dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'dec_input0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'enc_c0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'output_weights', 'output_bias']\n",
      "True\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "here it is!\n",
      "['enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'enc_c0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_h0', 'enc_c0', 'enc_c0', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'output_weights', 'output_bias']\n",
      "False\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "['enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c0', 'enc_o0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0']\n",
      "False\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "here it is!\n",
      "updating\n",
      "['enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_c-1', 'enc_f0', 'enc_i0', 'enc_g0', 'enc_wo', 'enc_bo', 'enc_inputhidden0', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0']\n",
      "False\n",
      "['enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_wf', 'enc_bf', 'enc_inputhidden0', 'enc_wi', 'enc_bi', 'enc_inputhidden0', 'enc_wg', 'enc_bg', 'enc_inputhidden0', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1']\n",
      "False\n",
      "['enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'enc_input0', 'enc_h-1', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat']\n",
      "False\n",
      "['emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat', 'emb_mat']\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "output, interm_gradients = backprop_gradient(cg,\"logit1\",gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.6343e-01,  2.8821e-01,  2.2603e-02,  4.2585e-02,  4.5970e-01,\n",
       "          1.1911e-01,  2.1751e-03,  1.0366e-01,  1.3653e-01,  9.5653e-02,\n",
       "         -3.0947e-01,  3.0301e-01,  1.6842e-01, -9.7699e-02,  5.0340e-03,\n",
       "         -1.1438e-01,  3.5180e-01, -6.7858e-01,  5.3403e-02,  2.0384e-01,\n",
       "         -2.5872e-01,  8.1468e-02, -9.4247e-02, -2.7366e-02,  4.9285e-02,\n",
       "          2.1124e-01,  6.1366e-02,  9.7330e-03,  5.1393e-02,  6.6620e-03,\n",
       "          2.7665e-01,  2.2864e-01, -2.1797e-01, -3.8722e-02,  1.2291e-01,\n",
       "         -4.3216e-02, -1.8936e-01,  1.4102e-01,  3.3920e-02, -7.8083e-02,\n",
       "          4.5861e-02, -6.7152e-02, -2.1536e-02,  2.8714e-01, -1.0896e-02,\n",
       "          3.4571e-02, -1.0766e-03,  2.2614e-01,  2.5757e-01, -1.5307e-01,\n",
       "         -1.6521e-01,  5.5468e-01, -1.0896e-01,  3.6422e-01,  1.9840e-02,\n",
       "         -5.0932e-02, -1.8607e-01,  1.1711e-02,  4.8586e-02,  4.6913e-02,\n",
       "         -2.9490e-01, -1.4798e-01,  6.9339e-02,  1.4208e-02,  3.6916e-02,\n",
       "          5.5357e-01,  3.2576e-01, -4.7346e-02,  1.2000e-02, -8.1189e-02,\n",
       "          1.1231e-01, -2.0894e-02, -1.3697e-01,  4.5023e-02,  7.5746e-02,\n",
       "          4.4500e-01,  9.2803e-02,  2.6044e-01, -1.4763e-02, -7.8066e-02,\n",
       "         -4.4290e-01, -8.7786e-02, -1.6237e-01,  6.7865e-02,  5.0619e-02,\n",
       "         -6.2497e-02,  1.6187e-01, -1.3459e-01,  4.5314e-01,  1.3903e-01,\n",
       "          1.0621e-02,  1.0571e-01,  6.2487e-02,  1.1841e-01,  1.3967e-03,\n",
       "          7.8476e-02, -1.2981e-01, -5.2621e-02,  1.1500e+00,  3.7614e-02,\n",
       "          7.9934e-01,  7.0987e-02,  7.1847e-02,  1.2843e-03, -1.1151e-01,\n",
       "         -3.1870e-02,  6.3806e-02,  5.1533e-02,  7.1897e-02,  6.7329e-02,\n",
       "         -1.4159e-02, -8.1313e-02, -2.6139e-01,  1.5527e-01, -1.6033e-02,\n",
       "         -2.3192e-01,  5.0020e-03,  2.0519e-01, -3.3510e-02, -5.9432e-02,\n",
       "          1.4101e-01,  3.5592e-02, -1.8532e-02, -1.9221e-01,  1.1814e-01,\n",
       "         -1.1608e-01,  1.6855e-01, -1.1264e-01, -8.0351e-02, -3.1921e-01,\n",
       "          1.5224e-01,  8.2571e-02,  1.4532e-02,  2.3941e-01, -3.2410e-02,\n",
       "          1.8084e-02, -2.3743e-02, -2.3599e-02, -1.1451e-01, -1.2654e-01,\n",
       "         -1.5404e-01,  1.3952e-02,  7.7692e-02,  4.4102e-02,  2.0639e-01,\n",
       "         -4.2935e-01, -1.0550e-01,  7.7590e-03, -2.9839e-01, -1.4503e-02,\n",
       "          3.7760e-02, -9.0827e-03,  1.1245e-01,  6.2807e-02, -2.4377e-02,\n",
       "         -9.5748e-02,  2.8358e-01,  2.0157e-01, -3.3068e-01,  5.3426e-02,\n",
       "          2.1610e-02,  2.4137e-01,  1.4506e-01,  6.2709e-02,  1.8820e-01,\n",
       "         -1.2981e-01,  3.8326e-01,  2.5713e-02,  1.6495e-01, -1.0361e-01,\n",
       "         -1.4465e-02,  1.5944e-01, -1.3618e-02, -3.1066e-01,  1.2025e-01,\n",
       "          1.2365e-03, -2.0549e-01, -8.8077e-02,  3.0499e-02, -3.6857e-02,\n",
       "         -5.6895e-02, -1.8388e-02, -2.0636e-02, -2.6131e-02, -3.0903e-01,\n",
       "         -1.2217e-01,  6.1675e-02,  2.6150e-02,  5.3662e-02, -2.0276e-02,\n",
       "         -1.4630e-02,  5.5124e-02, -7.3035e-02, -1.4240e-01,  6.9095e-01,\n",
       "         -2.9851e-02, -2.3702e-01, -1.8780e-01,  1.9100e-01, -3.3801e-02,\n",
       "         -6.2614e-02, -1.3609e-01,  4.0990e-01, -1.4340e-01,  2.3991e-02,\n",
       "         -3.6657e-01, -8.9971e-01, -3.6620e-02,  8.9612e-02,  1.7962e-03,\n",
       "         -1.8210e-03, -9.2921e-03,  3.3990e-02,  1.3680e-01,  2.4007e-01,\n",
       "          1.0630e-02, -5.1054e-02, -2.2787e-02, -5.5380e-02, -1.6288e-02,\n",
       "         -5.1411e-02, -6.3608e-02, -6.5958e-02, -1.0731e-03,  2.4170e-01,\n",
       "         -2.3136e-02,  4.7859e-03,  6.7722e-02,  7.7167e-01, -4.9200e-02,\n",
       "          1.2631e-02,  1.3695e-01,  4.0924e-02, -4.5157e-03, -3.5127e-02,\n",
       "          1.2534e-01,  9.4685e-02,  3.4315e-02,  4.7950e-02,  6.0034e-01,\n",
       "         -2.2759e-02, -9.3268e-02, -9.8961e-01, -1.5021e-01,  1.1574e-01,\n",
       "          1.1238e-02, -4.5558e-03,  9.2432e-02,  6.6841e-02,  2.4998e-01,\n",
       "         -7.6201e-02,  2.0755e-02,  5.4941e-03, -1.2155e-02, -3.2388e-01,\n",
       "         -1.0747e-01]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interm_gradients[\"dec_c0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.1716,  0.0012,  0.0583,  0.2247, -0.0837, -0.1303,  0.0570, -0.2255,\n",
       "         -0.1956,  0.2146],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.4136, -0.2879, -0.0391, -0.2931,  0.1614, -0.0781,  0.8840, -0.0861,\n",
       "         -0.4195,  0.2273],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.0362,  0.0947, -0.0662, -0.0934,  0.0298, -0.0349, -0.1049, -0.0228,\n",
       "          0.0141, -0.0166]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients[\"emb_mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.1716,  0.0012,  0.0583,  0.2247, -0.0837, -0.1303,  0.0570, -0.2255,\n",
       "         -0.1956,  0.2146],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.4136, -0.2879, -0.0391, -0.2931,  0.1614, -0.0781,  0.8840, -0.0861,\n",
       "         -0.4195,  0.2273],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.0362,  0.0947, -0.0662, -0.0934,  0.0298, -0.0349, -0.1049, -0.0228,\n",
       "          0.0141, -0.0166]], grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pred0',\n",
       "  tensor([[[-18.2495, -18.2065,   2.8476,  -3.7418,  -5.2067,  -2.2878,  -0.3709,\n",
       "             -6.6155,  -6.2988,  -5.7134,  -4.1288,  -4.8795,  -3.3390,  -7.0429,\n",
       "             -4.9520,  -0.1065,  -7.0276,  -8.2674,  -7.8986,  -6.1748,  -5.8160,\n",
       "             -8.9927,  -4.0365,  -8.4741,  -5.1798,  -9.3990,  -6.9244,  -5.5437,\n",
       "             -6.2603,  -5.3010,  -4.9812,  -7.0763,  -4.7963,  11.8783]]],\n",
       "         grad_fn=<AddBackward0>)),\n",
       " 33]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg[\"logit0\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n",
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.5744e-02, -1.6112e-02, -4.7076e-02,  7.6808e-02,  1.2732e-01,\n",
      "           1.0663e-01, -4.7536e-02, -5.7247e-02,  1.2138e-01, -8.1724e-02,\n",
      "           8.2449e-02, -9.1530e-04,  7.0700e-02, -3.8209e-02,  1.7516e-01,\n",
      "           9.8565e-02, -2.1420e-01, -3.6250e-02, -2.2710e-01, -2.3672e-01,\n",
      "          -2.3202e-01,  1.0897e-01,  2.2687e-02,  1.8941e-02,  2.2788e-01,\n",
      "          -2.4732e-01,  1.2328e-02, -1.2145e+00, -2.8930e-02, -5.0763e-02,\n",
      "           2.2141e-01,  6.5797e-02, -2.4298e-02, -9.0569e-02,  3.4675e-02,\n",
      "          -3.2900e-01, -1.1883e-01,  1.4096e-01,  1.2883e-03,  2.8239e-03,\n",
      "           5.5968e-01,  9.4449e-02,  1.9732e-01, -2.0234e-01, -4.1721e-02,\n",
      "          -1.9965e-01,  6.0732e-04,  2.9987e-02,  2.3678e-02, -3.8946e-02,\n",
      "           2.8809e-02, -2.0510e-01, -4.9838e-02, -1.0804e-01, -8.3539e-02,\n",
      "          -3.2697e-01,  1.9707e-01,  2.0227e-01, -5.0008e-02,  5.2575e-02,\n",
      "          -9.0661e-02,  6.6438e-01, -1.9801e-01, -6.5950e-02,  1.4418e-01,\n",
      "           3.4549e-01, -4.5511e-01,  2.1221e-02,  5.1003e-01, -4.3592e-02,\n",
      "           3.2098e-02, -1.3690e-01, -3.3536e-01, -1.4622e-01, -1.4195e-01,\n",
      "          -9.8902e-02, -3.3100e-02,  4.2009e-01,  1.0399e-01,  2.3556e-01,\n",
      "          -1.7881e-01, -4.9332e-02, -2.3963e-01,  2.2098e-01, -4.7986e-02,\n",
      "           1.7367e-01, -2.0876e-02, -1.5209e-01, -1.8815e-01, -2.1582e-01,\n",
      "          -1.7004e-01, -1.8280e-01, -1.0383e-01,  6.1805e-02,  3.1778e-01,\n",
      "           1.3541e-02, -1.0971e-01,  3.0532e-01,  7.4340e-01, -5.7662e-02,\n",
      "           2.8266e-01,  6.5651e-02,  4.9752e-01,  3.7438e-01, -1.2590e-01,\n",
      "           1.5475e-01, -1.1957e-01,  1.2247e-01,  9.8623e-01, -1.4577e-02,\n",
      "           1.2234e+00, -1.7580e-01,  2.2744e-03, -8.5365e-02, -9.1107e-04,\n",
      "           9.9178e-02,  4.4872e-02, -3.1625e-02,  2.3025e-01, -1.5736e-02,\n",
      "           4.8301e-01,  7.3907e-02,  9.8140e-02,  2.3094e-01, -9.0732e-03,\n",
      "          -6.4418e-02, -2.4090e-01,  6.9304e-02,  1.1133e-01, -5.9628e-02,\n",
      "           1.8924e-02,  1.1203e-01,  2.0608e-02, -7.8025e-02, -2.3112e-02,\n",
      "           6.2074e-02,  1.1686e-01,  4.5354e-02, -8.5527e-02,  1.6480e-01,\n",
      "           2.6307e-01,  2.4442e-01,  3.3854e-02, -2.2155e-02,  1.2496e-01,\n",
      "           1.3213e-01, -1.8805e-02, -1.5205e-01,  6.1246e-02, -3.2487e-02,\n",
      "          -1.8065e-02,  4.6147e-02, -7.4556e-02, -9.4487e-02,  1.3024e-02,\n",
      "          -4.1813e-03,  1.1280e-01,  6.4607e-02, -5.0862e-01, -2.3701e-03,\n",
      "           6.3831e-02, -2.1680e-01,  4.8385e-02, -2.9427e-02,  8.2901e-02,\n",
      "          -7.1139e-02,  1.0289e-01,  4.7389e-02,  2.0862e-02,  3.8516e-02,\n",
      "          -9.1290e-02,  9.5202e-02,  1.0565e-01,  3.2509e-02,  2.1121e-01,\n",
      "          -4.8071e-01,  1.7806e-01, -6.1039e-02,  3.3197e-02,  7.4494e-02,\n",
      "           4.2085e-02, -8.3731e-02, -9.3624e-02, -1.2424e-01,  1.3808e-01,\n",
      "          -4.0404e-02, -2.1302e-02, -5.2604e-02, -5.1880e-02, -1.1540e-01,\n",
      "          -5.8143e-02, -2.0128e-01, -1.4877e-01,  3.8110e-02, -5.1399e-01,\n",
      "          -3.5987e-01,  2.3436e-01,  7.2880e-02,  1.0220e-01,  1.0269e-01,\n",
      "          -1.1232e-01, -2.3195e-01, -2.5033e-02,  4.0589e-02,  8.8206e-02,\n",
      "           8.5545e-02, -1.4291e-02, -1.0094e-01,  1.7378e-02,  2.3095e-01,\n",
      "          -1.2508e-01, -1.4739e-01,  2.1292e-01,  4.8640e-02, -1.8713e-02,\n",
      "          -6.9330e-02, -1.2536e-01,  7.1717e-03, -3.0771e-01, -4.6900e-01,\n",
      "          -9.0200e-02,  6.7511e-02,  2.7140e-01,  1.7343e-02,  1.4162e-01,\n",
      "          -5.3656e-02,  2.2332e-01, -5.2421e-02,  2.2803e-01,  3.5165e-03,\n",
      "           1.0150e-01, -6.9844e-02, -1.4897e-01,  1.0624e-01, -1.1650e-01,\n",
      "          -1.3709e-01,  1.5515e-01,  1.7320e-01,  1.0201e-01,  1.4212e-01,\n",
      "          -7.9356e-02,  1.9284e-01, -1.6250e-01, -7.0363e-04,  7.7652e-02,\n",
      "           1.1904e-01, -4.2491e-02, -7.2340e-02, -2.3761e-01,  2.5113e-02,\n",
      "          -1.1251e-01, -1.2640e-02, -8.6458e-01, -1.9774e-01, -2.8564e-01,\n",
      "           2.6151e-02,  1.1492e-01, -7.8074e-02, -6.4589e-02,  1.8355e+00,\n",
      "          -1.7333e-01,  4.5625e-02, -1.0569e-01, -1.0172e-02,  6.8619e-02,\n",
      "           1.3966e-01]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 6.4625e-02,  1.9190e-01,  1.4802e-04,  3.6461e-02,  2.9468e-02,\n",
      "          -9.1035e-02, -1.9032e-02,  1.8691e-01,  1.1026e-02, -2.0533e-01,\n",
      "           1.4045e-01, -2.7280e-01,  5.8887e-01, -4.7730e-02,  1.5940e-01,\n",
      "          -4.6278e-01, -1.6202e-01,  3.8794e-02, -2.2440e-01, -3.3761e-01,\n",
      "           8.6243e-02,  6.1809e-01,  2.2924e-01,  1.9480e-02,  4.7225e-01,\n",
      "          -7.7218e-01, -2.7650e-01, -1.8308e+00,  4.6404e-02, -3.6098e-01,\n",
      "          -4.6111e-02,  6.2889e-01, -7.5292e-01, -1.6761e-01,  7.3514e-02,\n",
      "          -5.8948e-02, -1.1863e-01, -2.1080e-03, -1.5997e-01,  3.9923e-01,\n",
      "           1.8175e-01,  2.5837e-01,  8.5243e-02, -7.8631e-01, -1.1011e-01,\n",
      "          -4.1907e-01,  2.5094e-01, -3.4923e-02, -1.6376e-01,  7.8917e-02,\n",
      "           8.7223e-02, -1.0528e-01, -1.9068e-01, -5.0257e-01, -2.1739e-01,\n",
      "          -1.3048e-02, -2.9012e-02,  1.9868e-01, -4.2415e-01, -1.4053e-03,\n",
      "          -8.3018e-02, -1.7151e-01, -2.4634e-01,  1.6883e-01, -2.5277e-01,\n",
      "           2.8378e-01, -6.8300e-01, -8.5242e-02,  1.3024e+00, -2.9539e-01,\n",
      "           2.3737e-02, -4.9069e-02, -2.7770e-01, -2.2765e-01, -2.4929e-01,\n",
      "          -2.8269e-02, -4.7650e-01, -1.5083e-01,  1.9696e-01,  3.9011e-01,\n",
      "          -4.3065e-01, -2.3833e-01, -1.6731e-01,  1.1965e-01,  3.0756e-02,\n",
      "          -1.7594e-01, -7.7423e-02, -5.0339e-01, -4.0635e-01, -4.6227e-01,\n",
      "          -6.9055e-01,  4.5172e-02, -4.9656e-01,  1.4062e-01,  5.7784e-01,\n",
      "           1.2707e-01,  7.1904e-03,  1.8559e-01,  5.7673e-01, -1.1333e-01,\n",
      "           2.2072e-01,  1.0518e-01,  5.2006e-01,  6.3453e-01, -2.1902e-01,\n",
      "           3.4433e-01, -8.8917e-01,  2.9076e-01,  5.8824e-01, -3.0796e-01,\n",
      "           2.3757e+00, -4.3857e-01,  4.7702e-02,  9.3099e-02, -2.6085e-02,\n",
      "          -1.2156e-01, -1.9464e-02, -8.0476e-01, -3.4182e-01,  4.2799e-02,\n",
      "           1.5830e-02,  2.8970e-01,  2.6637e-01, -3.0930e-02, -1.2681e-01,\n",
      "          -3.3835e-01, -6.2054e-01,  2.4935e-01,  2.6025e-01, -3.1086e-02,\n",
      "           3.9279e-01,  7.9726e-01, -2.3936e-01, -1.1311e-01,  1.5769e-01,\n",
      "           7.6016e-01,  2.8506e-01,  2.3874e-01, -4.6905e-01,  1.6942e-01,\n",
      "          -1.1826e-01,  9.7137e-02,  2.5159e-01, -6.6065e-02, -2.0119e-01,\n",
      "           1.6175e-01, -6.6949e-02,  1.6921e-02,  2.2277e-01, -1.5790e-01,\n",
      "          -3.8770e-01,  2.8108e-02,  3.4244e-01, -2.9633e-01, -5.3241e-01,\n",
      "          -9.6563e-02,  1.9137e-01,  4.2685e-01, -3.1513e-02,  1.4899e-01,\n",
      "          -6.1421e-02, -4.5298e-01,  1.6542e-01,  2.9470e-01, -5.3076e-02,\n",
      "           2.0395e-01, -2.4671e-01, -1.2812e-01,  1.0474e-01,  4.3871e-01,\n",
      "           5.4525e-02, -9.7742e-02,  2.7871e-02,  2.5241e-01, -6.6276e-01,\n",
      "          -6.7155e-01,  2.1442e-01, -4.8687e-02,  2.1856e-01,  1.8827e-01,\n",
      "           7.4217e-01,  1.1256e-01,  8.2649e-02, -4.0949e-01,  2.0865e-01,\n",
      "          -3.2573e-01,  3.4651e-02,  7.2933e-02, -2.4795e-01,  8.6745e-02,\n",
      "           1.2033e-01,  6.4066e-01, -1.8818e-01, -2.3133e-01, -2.9023e-01,\n",
      "           3.2988e-01,  3.6688e-01, -1.3394e-01, -1.5780e-01, -4.2519e-02,\n",
      "          -4.3447e-01, -6.7608e-01, -7.7272e-02,  1.7340e-01,  1.7151e-01,\n",
      "          -2.6641e-01,  1.9864e-01,  4.1972e-03,  2.4070e-01,  1.9165e-01,\n",
      "           3.5793e-01, -2.5315e-01,  3.0622e-01,  2.1453e-01,  2.1603e-01,\n",
      "          -2.6066e-01, -2.2312e-01, -6.0905e-02, -1.3018e-01,  1.0209e-01,\n",
      "           1.4384e-02,  1.6221e-01,  2.5001e-01,  5.0263e-02,  2.8935e-01,\n",
      "           3.1408e-01,  4.5271e-01, -9.1147e-02,  5.3538e-01, -1.7075e-01,\n",
      "           4.3132e-01,  3.4266e-01,  8.6850e-02,  4.2100e-01, -2.4919e-01,\n",
      "           1.2935e-01,  1.4431e-01,  2.1778e-01,  2.2515e-01,  2.0467e-01,\n",
      "          -5.2566e-02, -1.8437e-01,  1.0937e-01, -2.9957e-03,  2.0320e-01,\n",
      "          -1.9619e-01, -7.6061e-02, -4.7204e-03, -1.4447e-01,  1.5409e-01,\n",
      "           7.2557e-03, -1.8384e-01, -4.1493e-01, -2.3686e-01, -3.3336e-01,\n",
      "           3.0768e-01, -1.0573e-01, -3.1181e-03,  1.6888e-01,  3.1619e+00,\n",
      "          -1.9762e-01, -3.1179e-01, -2.3358e-01, -3.3917e-03,  1.7086e-02,\n",
      "           3.3670e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "loss = nn.NLLLoss()(model([\"d\"],\"....................\")[1][0][0],torch.LongTensor([33]))\n",
    "loss.backward(create_graph=True, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.2341e-14,  8.5960e-14,  1.1966e-04,  1.6452e-07,  3.8022e-08,\n",
       "         7.0423e-07,  4.7883e-06,  9.2937e-09,  1.2756e-08,  2.2909e-08,\n",
       "         1.1173e-07,  5.2740e-08,  2.4615e-07,  6.0619e-09,  4.9053e-08,\n",
       "         6.2377e-06,  6.1550e-09,  1.7816e-09,  2.5762e-09,  1.4442e-08,\n",
       "         2.0673e-08,  8.6259e-10,  1.2254e-07,  1.4489e-09,  3.9060e-08,\n",
       "         5.7459e-10,  6.8244e-09,  2.7145e-08,  1.3257e-08,  3.4601e-08,\n",
       "         4.7638e-08,  5.8625e-09,  5.7316e-08, -1.3244e-04],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dec_output.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
