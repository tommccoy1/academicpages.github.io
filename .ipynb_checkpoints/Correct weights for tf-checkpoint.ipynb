{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of abstract language descriptors\n",
    "def load_languages(language_file):\n",
    "    fi = open(language_file, \"r\")\n",
    "    lang_list = []\n",
    "\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        ranking = [int(x) for x in parts[0].split(\",\")]\n",
    "        vowel_inventory = parts[1].split(\",\")\n",
    "        consonant_inventory = parts[2].split(\",\")\n",
    "\n",
    "        lang = [ranking, vowel_inventory, consonant_inventory]\n",
    "\n",
    "        lang_list.append(lang)\n",
    "\n",
    "    return lang_list\n",
    "\n",
    "# Load the file input/output correspondences\n",
    "def load_io(io_file):\n",
    "    fi = open(io_file, \"r\")\n",
    "\n",
    "    io_correspondences = {}\n",
    "\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        ranking = tuple([int(x) for x in parts[0].split(\",\")])\n",
    "\n",
    "        value = parts[1]\n",
    "        value_groups = value.split(\"&\")\n",
    "\n",
    "        value_list = []\n",
    "\n",
    "        for group in value_groups:\n",
    "            components = group.split(\"#\")\n",
    "            inp = components[0]\n",
    "            outp = components[1]\n",
    "            steps = components[2].split(\",\")\n",
    "\n",
    "            value_list.append([inp, outp, steps])\n",
    "\n",
    "        io_correspondences[ranking] = value_list\n",
    "\n",
    "    return io_correspondences\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        dev_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[2].split()]\n",
    "        vocab = parts[3].split()\n",
    "        key_string = parts[4].split(\",\")\n",
    "\n",
    "        v_list = key_string[0].split()\n",
    "        c_list = key_string[1].split()\n",
    "        ranking = [int(x) for x in key_string[2].split()]\n",
    "\n",
    "        key = [v_list, c_list, ranking]\n",
    "\n",
    "        langs.append([train_set, dev_set, test_set, vocab, key])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset_scramble(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    all_train_sets = []\n",
    "    all_dev_sets = []\n",
    "    all_test_sets = []\n",
    "\n",
    "    n_tasks = 0\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        dev_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[2].split()]\n",
    "        all_train_sets += train_set\n",
    "        all_dev_sets += dev_set\n",
    "        all_test_sets += test_set\n",
    "\n",
    "        vocab = parts[3].split()\n",
    "\n",
    "        n_tasks += 1\n",
    "\n",
    "    shuffle(all_train_sets)\n",
    "    shuffle(all_dev_sets)\n",
    "    shuffle(all_test_sets)\n",
    "\n",
    "    train_len = len(train_set)\n",
    "    dev_len = len(dev_set)\n",
    "    test_len = len(test_set)\n",
    "\n",
    "\n",
    "    for i in range(n_tasks):\n",
    "        train_set = all_train_sets[i*train_len:(i+1)*train_len]\n",
    "        dev_set = all_dev_sets[i*dev_len:(i+1)*dev_len]\n",
    "        test_set = all_test_sets[i*test_len:(i+1)*test_len]\n",
    "\n",
    "        v_list = \"scrambled\"\n",
    "        c_list = \"scrambled\"\n",
    "        ranking = \"scrambled\"\n",
    "\n",
    "        key = [v_list, c_list, ranking]\n",
    "\n",
    "        langs.append([train_set, dev_set, test_set, vocab, key])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset_cv(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        vocab = parts[2].split()\n",
    "\n",
    "        langs.append([train_set, test_set, vocab])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break a list into batches of the desired size\n",
    "def batchify_list(lst, batch_size=100):\n",
    "    batches = []\n",
    "    this_batch_in = []\n",
    "    this_batch_out = []\n",
    "\n",
    "    for index, elt in enumerate(lst):\n",
    "        #print(elt)\n",
    "        this_batch_in.append(elt[0])\n",
    "        this_batch_out.append(elt[1])\n",
    "\n",
    "        if (index + 1) % batch_size == 0:\n",
    "            batches.append([this_batch_in, this_batch_out])\n",
    "            this_batch_in = []\n",
    "            this_batch_out = []\n",
    "\n",
    "    if this_batch_in != []:\n",
    "        batches.append([this_batch_in, this_batch_out])\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Trim the excess from the end of an output string\n",
    "def process_output(output):\n",
    "    if \"EOS\" in output:\n",
    "        return output[:output.index(\"EOS\")]\n",
    "    else:\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine a basic PyTorch model to allow\n",
    "# for double gradients and manual modification\n",
    "# of weights\n",
    "class ModifiableModule():\n",
    "    def params(self):\n",
    "        return [p for _, p in self.named_params()]\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return []\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return []\n",
    "\n",
    "    def named_params(self):\n",
    "        subparams = []\n",
    "        for name, mod in self.named_submodules():\n",
    "            for subname, param in mod.named_params():\n",
    "                subparams.append((name + '.' + subname, param))\n",
    "        return self.named_leaves() + subparams\n",
    "\n",
    "    def set_param(self, name, param):\n",
    "        if '.' in name:\n",
    "            n = name.split('.')\n",
    "            module_name = n[0]\n",
    "            rest = '.'.join(n[1:])\n",
    "            for name, mod in self.named_submodules():\n",
    "                if module_name == name:\n",
    "                    mod.set_param(rest, param)\n",
    "                    break\n",
    "        else:\n",
    "            setattr(self, name, param)\n",
    "\n",
    "    def copy(self, other, same_var=False):\n",
    "        for name, param in other.named_params():\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "            self.set_param(name, param)\n",
    "\n",
    "\n",
    "    def load_state_dict(self, sdict, same_var=False):\n",
    "        for name in sdict:\n",
    "            param = sdict[name]\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "\n",
    "            self.set_param(name, param)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return OrderedDict(self.named_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined linear layer\n",
    "class GradLinear(ModifiableModule):\n",
    "    def __init__(self, inp_size, outp_size):\n",
    "        super(GradLinear, self).__init__()\n",
    "        self.weights = np.random.rand(outp_size, inp_size)\n",
    "        self.bias = np.random.rand(outp_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return np.matmul(self.weights,x) + self.bias\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights), ('bias', self.bias)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def logsoftmax(x):\n",
    "    return np.log(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.wi_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wi_bias = np.random.rand(hidden_size)\n",
    "        self.wf_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wf_bias = np.random.rand(hidden_size)\n",
    "        self.wg_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wg_bias = np.random.rand(hidden_size)\n",
    "        self.wo_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wo_bias = np.random.rand(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        \n",
    "        input_plus_hidden = np.concatenate([inp.flatten(), hx.flatten()])\n",
    "\n",
    "        i_tpre = np.matmul(self.wi_weights,input_plus_hidden) + self.wi_bias\n",
    "        i_t = sigmoid(i_tpre)\n",
    "        f_tpre = np.matmul(self.wf_weights,input_plus_hidden) + self.wf_bias\n",
    "        f_t = sigmoid(f_tpre)\n",
    "        g_tpre = np.matmul(self.wg_weights,input_plus_hidden) + self.wg_bias\n",
    "        g_t = tanh(g_tpre)\n",
    "        o_tpre = np.matmul(self.wo_weights,input_plus_hidden) + self.wo_bias\n",
    "        o_t = sigmoid(o_tpre)\n",
    "        #print(i_t)\n",
    "        #print(f_t)\n",
    "        #print(g_t)\n",
    "        #print(o_t)\n",
    "\n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * tanh(cx)\n",
    "\n",
    "        #myhook = input_plus_hidden.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), o_tpre, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined embedding layer\n",
    "class GradEmbedding(ModifiableModule):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(GradEmbedding, self).__init__()\n",
    "        self.weights = np.random.rand(emb_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.matmul(self.weights,x)\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(ind):\n",
    "    oh = np.zeros(34)\n",
    "    oh[ind] = 1.0\n",
    "    \n",
    "    return oh\n",
    "\n",
    "onehot(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "        \n",
    "        self.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "\n",
    "\n",
    "    def forward(self, inp, outp_length=20):\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (np.zeros([1,self.hidden_size]), np.zeros([1,self.hidden_size]))\n",
    "\n",
    "        this_seq = []\n",
    "        # Iterate over the sequence\n",
    "        for elt in inp:\n",
    "            ind = self.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        \n",
    "        inp_length = len(inp)\n",
    "        if inp_length > 0:\n",
    "\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(this_seq):\n",
    "                # Embed the character\n",
    "                emb = self.embedding.forward(onehot(elt))\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, _, _, i_tpre, f_tpre, g_tpre = self.enc_lstm.forward(emb, hidden)\n",
    "                hidden_prev = hidden\n",
    "\n",
    "\n",
    "                hidden = hidden_new\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = \"SOS\"\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_string = \"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(min(self.max_length,outp_length)):\n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            \n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding.forward(onehot(self.char2ind[prev_output]))\n",
    "\n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm.forward(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output.forward(output.flatten())\n",
    "            probs = logsoftmax(pred)\n",
    "            logits.append(probs)\n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            label = np.argmax(probs)\n",
    "\n",
    "            char = self.ind2char[label]\n",
    "            out_string += char\n",
    "            prev_output = char\n",
    "            \n",
    "\n",
    "        return out_string, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec = EncoderDecoder(34,10,256)\n",
    "\n",
    "encdec.enc_lstm.wo_weights = np.loadtxt(\"enc_lstm.wo_weights\")\n",
    "encdec.enc_lstm.wi_weights = np.loadtxt(\"enc_lstm.wi_weights\")\n",
    "encdec.enc_lstm.wg_weights = np.loadtxt(\"enc_lstm.wg_weights\")\n",
    "encdec.enc_lstm.wf_weights = np.loadtxt(\"enc_lstm.wf_weights\")\n",
    "encdec.enc_lstm.wo_bias = np.loadtxt(\"enc_lstm.wo_bias\")\n",
    "encdec.enc_lstm.wi_bias = np.loadtxt(\"enc_lstm.wi_bias\")\n",
    "encdec.enc_lstm.wg_bias = np.loadtxt(\"enc_lstm.wg_bias\")\n",
    "encdec.enc_lstm.wf_bias = np.loadtxt(\"enc_lstm.wf_bias\")\n",
    "\n",
    "encdec.dec_lstm.wo_weights = np.loadtxt(\"dec_lstm.wo_weights\")\n",
    "encdec.dec_lstm.wi_weights = np.loadtxt(\"dec_lstm.wi_weights\")\n",
    "encdec.dec_lstm.wg_weights = np.loadtxt(\"dec_lstm.wg_weights\")\n",
    "encdec.dec_lstm.wf_weights = np.loadtxt(\"dec_lstm.wf_weights\")\n",
    "encdec.dec_lstm.wo_bias = np.loadtxt(\"dec_lstm.wo_bias\")\n",
    "encdec.dec_lstm.wi_bias = np.loadtxt(\"dec_lstm.wi_bias\")\n",
    "encdec.dec_lstm.wg_bias = np.loadtxt(\"dec_lstm.wg_bias\")\n",
    "encdec.dec_lstm.wf_bias = np.loadtxt(\"dec_lstm.wf_bias\")\n",
    "\n",
    "encdec.embedding.weights = np.loadtxt(\"embedding.weights\").transpose()\n",
    "encdec.dec_output.weights = np.loadtxt(\"dec_output.weights\")\n",
    "encdec.dec_output.bias = np.loadtxt(\"dec_output.bias\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec.forward(\"do\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *\n",
    "\n",
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    new_list = []\n",
    "    for elt in lst:\n",
    "        new_list = new_list + elt\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.embedding.weights.data.shape) # weights_emb; weights_emb_bias is zeroes\n",
    "print(model.enc_lstm.wo_weights.transpose(0,1).data.shape)\n",
    "print(model.enc_lstm.wo_bias.data.shape)\n",
    "print(model.dec_output.weights.transpose(0,1).data.shape) # weights_out_weights\n",
    "print(model.dec_output.bias.data.shape) # weights_out_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = flatten(model.enc_lstm.wi_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wi = flatten(model.enc_lstm.wi_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bi = model.enc_lstm.wi_bias.data.numpy().tolist()\n",
    "\n",
    "xf = flatten(model.enc_lstm.wf_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wf = flatten(model.enc_lstm.wf_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bf = model.enc_lstm.wf_bias.data.numpy().tolist()\n",
    "\n",
    "xg = flatten(model.enc_lstm.wg_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wg = flatten(model.enc_lstm.wg_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bg = model.enc_lstm.wg_bias.data.numpy().tolist()\n",
    "\n",
    "xo = flatten(model.enc_lstm.wo_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wo = flatten(model.enc_lstm.wo_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bo = model.enc_lstm.wo_bias.data.numpy().tolist()\n",
    "\n",
    "full_x = xi + xf + xg + xo\n",
    "full_w = wi + wf + wg + wo\n",
    "full_b = bi + bf + bg + bo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xid = flatten(model.dec_lstm.wi_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wid = flatten(model.dec_lstm.wi_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bid = model.dec_lstm.wi_bias.data.numpy().tolist()\n",
    "\n",
    "xfd = flatten(model.dec_lstm.wf_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wfd = flatten(model.dec_lstm.wf_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bfd = model.dec_lstm.wf_bias.data.numpy().tolist()\n",
    "\n",
    "xgd = flatten(model.dec_lstm.wg_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wgd = flatten(model.dec_lstm.wg_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bgd = model.dec_lstm.wg_bias.data.numpy().tolist()\n",
    "\n",
    "xod = flatten(model.dec_lstm.wo_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wod = flatten(model.dec_lstm.wo_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bod = model.dec_lstm.wo_bias.data.numpy().tolist()\n",
    "\n",
    "full_xd = xid + xfd + xgd + xod\n",
    "full_wd = wid + wfd + wgd + wod\n",
    "full_bd = bid + bfd + bgd + bod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_lst(lst):\n",
    "    joined = \", \".join([str(x) for x in lst])\n",
    "    return \"[\" + joined + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_weights = open(\"tf_weights.js\", \"w\")\n",
    "tf_weights.write(\"emb_wg = \" + stringify_lst(flatten(model.embedding.weights.data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"full_x = \" + stringify_lst(full_x) + \";\\n\")\n",
    "tf_weights.write(\"full_w = \" + stringify_lst(full_w) + \";\\n\")\n",
    "tf_weights.write(\"full_b = \" + stringify_lst(full_b) + \";\\n\")\n",
    "tf_weights.write(\"full_xd = \" + stringify_lst(full_xd) + \";\\n\")\n",
    "tf_weights.write(\"full_wd = \" + stringify_lst(full_wd) + \";\\n\")\n",
    "tf_weights.write(\"full_bd = \" + stringify_lst(full_bd) + \";\\n\")\n",
    "tf_weights.write(\"out_wg = \" + stringify_lst(flatten(model.dec_output.weights.transpose(0,1).data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"out_wb = \" + stringify_lst(model.dec_output.bias.data.numpy().tolist()) + \";\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_lstm.wo_weights.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"embedding.weights\",model.embedding.weights.data.numpy())\n",
    "\n",
    "np.savetxt(\"enc_lstm.wi_weights\",model.enc_lstm.wi_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wi_bias\",model.enc_lstm.wi_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wf_weights\",model.enc_lstm.wf_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wf_bias\",model.enc_lstm.wf_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wg_weights\",model.enc_lstm.wg_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wg_bias\",model.enc_lstm.wg_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wo_weights\",model.enc_lstm.wo_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wo_bias\",model.enc_lstm.wo_bias.data.numpy())\n",
    "\n",
    "np.savetxt(\"dec_lstm.wi_weights\",model.dec_lstm.wi_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wi_bias\",model.dec_lstm.wi_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wf_weights\",model.dec_lstm.wf_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wf_bias\",model.dec_lstm.wf_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wg_weights\",model.dec_lstm.wg_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wg_bias\",model.dec_lstm.wg_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wo_weights\",model.dec_lstm.wo_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wo_bias\",model.dec_lstm.wo_bias.data.numpy())\n",
    "\n",
    "np.savetxt(\"dec_output.weights\",model.dec_output.weights.data.numpy())\n",
    "np.savetxt(\"dec_output.bias\",model.dec_output.bias.data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in model.named_params()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *\n",
    "\n",
    "model = EncoderDecoder(34,10,256, recurrent_unit=\"GRU\")\n",
    "model.load_state_dict(torch.load(\"yonc_maml_gru_256_5.weights\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    new_list = []\n",
    "    for elt in lst:\n",
    "        new_list = new_list + elt\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.embedding.weights.data.shape) # weights_emb; weights_emb_bias is zeroes\n",
    "print(model.enc_lstm.wr_weights.transpose(0,1).data.shape)\n",
    "print(model.enc_lstm.wr_bias.data.shape)\n",
    "print(model.dec_output.weights.transpose(0,1).data.shape) # weights_out_weights\n",
    "print(model.dec_output.bias.data.shape) # weights_out_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = flatten(model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wr = flatten(model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "br = model.enc_lstm.wr_bias.data.numpy().tolist()\n",
    "\n",
    "xz = flatten(model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wz = flatten(model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bz = model.enc_lstm.wz_bias.data.numpy().tolist()\n",
    "\n",
    "xx = flatten(model.enc_lstm.wx_weights.transpose(0,1).data.numpy().tolist())\n",
    "#wx = flatten(model.enc_lstm.wx_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bx = model.enc_lstm.wx_bias.data.numpy()\n",
    "\n",
    "#xrh = flatten(model.enc_lstm.wrh_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrh = flatten(model.enc_lstm.wrh_weights.transpose(0,1).data.numpy().tolist())\n",
    "brh = model.enc_lstm.wrh_bias.data.numpy()\n",
    "\n",
    "bxrh = (bx + brh).tolist()\n",
    "\n",
    "full_x = xz + xr + xx\n",
    "full_w = wz + wr + wrh\n",
    "full_b = bz + br + bxrh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xrd = flatten(model.dec_lstm.wr_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrd = flatten(model.dec_lstm.wr_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "brd = model.dec_lstm.wr_bias.data.numpy().tolist()\n",
    "\n",
    "xzd = flatten(model.dec_lstm.wz_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wzd = flatten(model.dec_lstm.wz_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bzd = model.dec_lstm.wz_bias.data.numpy().tolist()\n",
    "\n",
    "xxd = flatten(model.dec_lstm.wx_weights.transpose(0,1).data.numpy().tolist())\n",
    "#wxd = flatten(model.dec_lstm.wx_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bxd = model.dec_lstm.wx_bias.data.numpy()\n",
    "\n",
    "#xrhd = flatten(model.dec_lstm.wrh_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrhd = flatten(model.dec_lstm.wrh_weights.transpose(0,1).data.numpy().tolist())\n",
    "brhd = model.dec_lstm.wrh_bias.data.numpy()\n",
    "\n",
    "bxrhd = (bxd + brhd).tolist()\n",
    "\n",
    "full_xd = xzd + xrd + xxd\n",
    "full_wd = wzd + wrd + wrhd\n",
    "full_bd = bzd + brd + bxrhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE IT IS!!!\n",
    "xr = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy()#.transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy() #.transpose()\n",
    "br = np.expand_dims(model.enc_lstm.wr_bias.data.numpy(),axis=0)\n",
    "\n",
    "xz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy()#.transpose()\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy() #.transpose()\n",
    "bz = np.expand_dims(model.enc_lstm.wz_bias.data.numpy(), axis=0)\n",
    "\n",
    "xx = model.enc_lstm.wx_weights.transpose(0,1).data.numpy()#.transpose()\n",
    "bx = np.expand_dims(model.enc_lstm.wx_bias.data.numpy(), axis=0)\n",
    "\n",
    "#xrh = flatten(model.enc_lstm.wrh_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrh = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy() #.transpose()\n",
    "brh = np.expand_dims(model.enc_lstm.wrh_bias.data.numpy(),axis=0)\n",
    "\n",
    "bxrh = bx + brh\n",
    "\n",
    "full_w = flatten(np.concatenate([wz, wr, wrh], axis=1).tolist())\n",
    "full_x = flatten(np.concatenate([xz, xr, xx], axis=1).transpose().tolist())\n",
    "\n",
    "\n",
    "full_b = flatten(np.concatenate([bz,br,bxrh], axis=0).tolist())\n",
    "#print(full_b)\n",
    "\n",
    "xrd = flatten(model.dec_lstm.wr_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrd = flatten(model.dec_lstm.wr_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "brd = model.dec_lstm.wr_bias.data.numpy().tolist()\n",
    "\n",
    "xzd = flatten(model.dec_lstm.wz_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wzd = flatten(model.dec_lstm.wz_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bzd = model.dec_lstm.wz_bias.data.numpy().tolist()\n",
    "\n",
    "xxd = flatten(model.dec_lstm.wx_weights.transpose(0,1).data.numpy().tolist())\n",
    "#wxd = flatten(model.dec_lstm.wx_weights.transpose(0,1)[10:].data.numpy().tolist())\n",
    "bxd = model.dec_lstm.wx_bias.data.numpy()\n",
    "\n",
    "#xrhd = flatten(model.dec_lstm.wrh_weights.transpose(0,1)[:10].data.numpy().tolist())\n",
    "wrhd = flatten(model.dec_lstm.wrh_weights.transpose(0,1).data.numpy().tolist())\n",
    "brhd = model.dec_lstm.wrh_bias.data.numpy()\n",
    "\n",
    "bxrhd = (bxd + brhd).tolist()\n",
    "\n",
    "full_xd = xzd + xrd + xxd\n",
    "full_wd = wzd + wrd + wrhd\n",
    "full_bd = bzd + brd + bxrhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([bz,br,brh]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_lst(lst):\n",
    "    joined = \", \".join([str(x) for x in lst])\n",
    "    return \"[\" + joined + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_weights = open(\"tf_weights.js\", \"w\")\n",
    "tf_weights.write(\"emb_wg = \" + stringify_lst(flatten(model.embedding.weights.data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"full_x = \" + stringify_lst(full_x) + \";\\n\")\n",
    "tf_weights.write(\"full_w = \" + stringify_lst(full_w) + \";\\n\")\n",
    "tf_weights.write(\"full_b = \" + stringify_lst(full_b) + \";\\n\")\n",
    "tf_weights.write(\"full_xd = \" + stringify_lst(full_xd) + \";\\n\")\n",
    "tf_weights.write(\"full_wd = \" + stringify_lst(full_wd) + \";\\n\")\n",
    "tf_weights.write(\"full_bd = \" + stringify_lst(full_bd) + \";\\n\")\n",
    "tf_weights.write(\"out_wg = \" + stringify_lst(flatten(model.dec_output.weights.transpose(0,1).data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"out_wb = \" + stringify_lst(model.dec_output.bias.data.numpy().tolist()) + \";\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_lstm.wz_weights.transpose(0,1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringify_lst(full_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringify_lst(full_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "model([\"b\"])[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some dimensionalities\n",
    "import numpy as np\n",
    "\n",
    "emb_mat = model.embedding.weights.data.numpy()\n",
    "\n",
    "inp = np.matmul(np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose() # GOOD\n",
    "h = np.array([[0 for _ in range(256)]]).transpose()\n",
    "\n",
    "print(h.shape)\n",
    "print(inp.shape)\n",
    "\n",
    "uz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy() #.transpose()\n",
    "bz = model.enc_lstm.wz_bias.data.numpy() # GOOD\n",
    "\n",
    "ur = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy() #.transpose()\n",
    "br = model.enc_lstm.wr_bias.data.numpy()\n",
    "\n",
    "ux = model.enc_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wx = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy() #.transpose()\n",
    "bx = model.enc_lstm.wx_bias.data.numpy() + model.enc_lstm.wrh_bias.data.numpy() # GOOD\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "print(\"htilde\", htilde.shape)\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "inp = np.matmul(np.array([[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose()\n",
    "print(h)\n",
    "print(\"new_enc\", h.shape)\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "print(\"htilde\", htilde.shape)\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "print(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding(torch.LongTensor([model.char2ind[\"a\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the right thing\n",
    "import numpy as np\n",
    "\n",
    "emb_mat = model.embedding.weights.data.numpy()\n",
    "\n",
    "inp = np.matmul(np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose() # GOOD\n",
    "h = np.array([[0 for _ in range(256)]]).transpose()\n",
    "\n",
    "print(h.shape)\n",
    "print(inp.shape)\n",
    "\n",
    "uz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bz = np.expand_dims(model.enc_lstm.wz_bias.data.numpy(),1) # GOOD\n",
    "\n",
    "ur = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "br = np.expand_dims(model.enc_lstm.wr_bias.data.numpy(),1)\n",
    "\n",
    "ux = model.enc_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wx = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bx = np.expand_dims(model.enc_lstm.wx_bias.data.numpy() + model.enc_lstm.wrh_bias.data.numpy(),1) # GOOD\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "print(\"htilde\", htilde.shape)\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "inp = np.matmul(np.array([[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose()\n",
    "print(h)\n",
    "#print(\"new_enc\", h.shape)\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "#print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "#print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "#print(\"htilde\", htilde.shape)\n",
    "\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "\n",
    "#print(h.shape)\n",
    "print(h)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_w = flatten(np.concatenate([wz, wr, wrh], axis=1).tolist())\n",
    "#full_x = flatten(np.concatenate([xz, xr, xx], axis=1).transpose().tolist())\n",
    "#full_b = flatten(np.concatenate([bz,br,bxrh], axis=0).tolist())\n",
    "\n",
    "full_w = flatten(np.concatenate([wz, wr, wrh], axis=0).transpose().tolist())\n",
    "full_x = flatten(np.concatenate([xz, xr, xx], axis=0).tolist())\n",
    "full_b = np.concatenate([bz,br,bx]).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.char2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good first hidden state\n",
    "uz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bz = model.enc_lstm.wz_bias.data.numpy() # GOOD\n",
    "\n",
    "ur = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "br = model.enc_lstm.wr_bias.data.numpy()\n",
    "\n",
    "ux = model.enc_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wx = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bx = model.enc_lstm.wx_bias.data.numpy() + model.enc_lstm.wrh_bias.data.numpy() # GOOD\n",
    "\n",
    "\n",
    "full_w = flatten(np.concatenate([wz, wr, wx], axis=0).transpose().tolist())\n",
    "full_x = flatten(np.concatenate([uz, ur, ux], axis=0).transpose().tolist()) # CORRECT\n",
    "full_b = flatten(np.concatenate([np.expand_dims(bz,1),np.expand_dims(br,1),np.expand_dims(bx,1)], axis=0).tolist()) # CORRECT\n",
    "\n",
    "# No internal transpose:\n",
    "# 0, no transpose: wrong\n",
    "# 1, no transpose: right first thing, but not rest\n",
    "# 0, transpose: wrong\n",
    "# 1, transpose: wrong\n",
    "\n",
    "# Internal transpose:\n",
    "# 0, no transpose: wrong\n",
    "# 1, no transpose: wrong\n",
    "# 0, transpose: right first thing, but not rest\n",
    "# 1, transpose: wrong\n",
    "\n",
    "\n",
    "tf_weights = open(\"tf_weights.js\", \"w\")\n",
    "tf_weights.write(\"emb_wg = \" + stringify_lst(flatten(model.embedding.weights.data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"full_x = \" + stringify_lst(full_x) + \";\\n\")\n",
    "tf_weights.write(\"full_w = \" + stringify_lst(full_w) + \";\\n\")\n",
    "tf_weights.write(\"full_b = \" + stringify_lst(full_b) + \";\\n\")\n",
    "tf_weights.write(\"full_xd = \" + stringify_lst(full_xd) + \";\\n\")\n",
    "tf_weights.write(\"full_wd = \" + stringify_lst(full_wd) + \";\\n\")\n",
    "tf_weights.write(\"full_bd = \" + stringify_lst(full_bd) + \";\\n\")\n",
    "tf_weights.write(\"out_wg = \" + stringify_lst(flatten(model.dec_output.weights.transpose(0,1).data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"out_wb = \" + stringify_lst(model.dec_output.bias.data.numpy().tolist()) + \";\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.char2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *\n",
    "\n",
    "model = EncoderDecoder(34,10,256, recurrent_unit=\"GRU\")\n",
    "model.load_state_dict(torch.load(\"yonc_maml_gru_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.3812, -0.4205, -0.8237,  0.8546,  0.8499, -0.2240,  0.2781,\n",
       "            0.5281,  0.5563, -0.7545, -0.3346,  0.2481, -0.9950, -0.2187,\n",
       "           -0.8483,  0.2551, -0.3982, -0.7903,  0.5887,  0.6602, -0.6101,\n",
       "            0.6130, -0.5756, -0.8793,  0.1599, -0.1643, -0.0342,  0.5934,\n",
       "           -0.8864, -0.4143,  0.9686, -0.3760, -0.8288, -0.2831,  0.5956,\n",
       "           -0.9481,  0.5187,  0.6435,  0.1961, -0.9974, -0.4230, -0.7563,\n",
       "           -0.9837,  0.9690,  0.1212, -0.8425, -0.2087, -0.0198,  0.3525,\n",
       "            0.9036, -0.3997, -0.2741,  0.5559,  0.7476, -0.8179, -0.9807,\n",
       "           -0.3296,  0.5416, -0.6303,  0.6871, -0.8596,  0.8338,  0.6590,\n",
       "           -0.4631,  0.0190, -0.1234,  0.1824,  0.9628, -0.4491, -0.5275,\n",
       "            0.4260,  0.9060,  0.3895,  0.9231, -0.4454,  0.8645,  0.2057,\n",
       "           -0.3400, -0.5472, -0.7450,  0.0307,  0.8704, -0.9603,  0.2843,\n",
       "            0.6638, -0.2122,  0.2270, -0.9739, -0.5123, -0.9031,  0.9681,\n",
       "           -0.6376, -0.7842,  0.1731, -0.4057, -0.5838,  0.8412, -0.2634,\n",
       "           -0.6174,  0.7438,  0.6310, -0.4340, -0.9856, -0.7873, -0.7407,\n",
       "            0.1277, -0.9694,  0.2557,  0.3824, -0.9589, -0.4986, -0.1293,\n",
       "            0.3780,  0.9545,  0.7270,  0.2399, -0.8994,  0.5194, -0.4163,\n",
       "            0.8268, -0.7077, -0.6212,  0.7849,  0.9043,  0.9359, -0.9606,\n",
       "           -0.7138,  0.3611,  0.3627,  0.3125, -0.9954,  0.9793, -0.4192,\n",
       "           -0.9168, -0.1114, -0.0588, -0.5605, -0.0406, -0.4694,  0.8814,\n",
       "           -0.2869, -0.7818,  0.8303, -0.2935, -0.4245, -0.1485,  0.0402,\n",
       "           -0.7369,  0.3136, -0.9742, -0.9562, -0.3490,  0.9137,  0.8309,\n",
       "           -0.4868,  0.9323,  0.3075, -0.8405, -0.2630, -0.9244, -0.8218,\n",
       "            0.1556, -0.7659, -0.9685,  0.5607,  0.1768, -0.8190,  0.3754,\n",
       "           -0.9533, -0.1528,  0.9273, -0.4673, -0.5995,  0.3608,  0.7877,\n",
       "            0.9572, -0.1961,  0.3786, -0.3628, -0.9832, -0.1784,  0.9245,\n",
       "           -0.8942,  0.0255, -0.2796,  0.6957, -0.3661,  0.9867,  0.4889,\n",
       "            0.2189, -0.1224, -0.9239,  0.3676, -0.3875, -0.1795, -0.4371,\n",
       "           -0.5533, -0.3437,  0.1820,  0.6381,  0.8114,  0.9039,  0.5808,\n",
       "            0.0585,  0.0198, -0.0267, -0.1704,  0.2220, -0.9644, -0.7345,\n",
       "            0.9241, -0.9249,  0.3840,  0.3821, -0.5023, -0.6180,  0.0340,\n",
       "           -0.9706, -0.8315,  0.5707,  0.1925, -0.4887,  0.7961,  0.2545,\n",
       "            0.8105,  0.3944,  0.8713, -0.5695,  0.3392, -0.0653,  0.9797,\n",
       "           -0.9271,  0.7383, -0.3556, -0.1454, -0.1401,  0.8455,  0.9874,\n",
       "            0.3781,  0.8180,  0.0173,  0.8377, -0.8587, -0.9026,  0.6800,\n",
       "            0.9167,  0.9154,  0.4167, -0.3405, -0.2975,  0.9023, -0.7803,\n",
       "            0.9104, -0.9708,  0.2224, -0.1279]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-3.0461e-01, -4.1930e-01, -6.0897e-01,  8.5521e-01, -9.6645e-01,\n",
       "           -1.7539e-01,  2.7734e-01,  5.3196e-01,  4.3943e-01, -4.8491e-01,\n",
       "            4.5243e-01,  1.7995e-01,  7.2456e-01, -2.3282e-01, -9.6056e-01,\n",
       "            2.4193e-01, -2.8462e-01, -7.9055e-01,  2.3175e-01,  6.6020e-01,\n",
       "           -2.9036e-01,  6.1222e-01, -5.6822e-01, -8.7911e-01,  9.9903e-01,\n",
       "            6.8399e-01, -1.3140e-01,  3.2422e-01,  9.9505e-01, -9.9420e-01,\n",
       "            7.1219e-01, -6.9211e-01, -4.9225e-01, -2.9348e-01,  5.9722e-01,\n",
       "            9.9212e-01,  4.9796e-01,  6.5701e-01,  1.9186e-01, -6.3203e-01,\n",
       "           -9.7644e-01,  9.4942e-01,  9.7728e-01, -4.3751e-01,  1.0799e-01,\n",
       "           -8.1545e-01, -1.6425e-01,  6.9193e-02, -7.5512e-01, -9.7397e-01,\n",
       "            1.3545e-01, -2.7512e-01,  3.5591e-01,  4.2434e-01, -4.3676e-01,\n",
       "            9.8556e-01, -3.2846e-01,  5.4416e-01,  9.9924e-01,  7.1522e-01,\n",
       "           -8.5590e-01, -9.9625e-01,  6.5093e-01, -4.5566e-01,  1.9346e-02,\n",
       "           -2.2321e-01,  1.7987e-01,  9.9872e-01, -4.2876e-01, -5.2310e-01,\n",
       "           -2.5051e-01,  9.0505e-01,  4.0029e-01,  9.2932e-01, -4.4490e-01,\n",
       "            8.0412e-01,  2.2788e-01,  1.7691e-01,  1.5678e-02, -8.5726e-01,\n",
       "            9.1926e-02,  8.1640e-01, -9.4142e-01, -8.9295e-01,  6.5002e-01,\n",
       "            1.8618e-01,  8.8324e-01, -9.7374e-01,  2.7703e-01, -8.7480e-01,\n",
       "           -2.1690e-01, -6.3621e-01,  9.9090e-01, -5.0192e-01, -9.9838e-01,\n",
       "           -5.8455e-01, -5.3060e-01, -2.2476e-02, -6.1436e-01, -4.5182e-01,\n",
       "            4.6775e-01, -4.4566e-01,  8.9251e-01, -7.8781e-01, -6.7483e-01,\n",
       "           -3.7632e-01,  6.3948e-01,  1.6646e-01,  6.3281e-01,  8.4719e-01,\n",
       "           -4.9858e-01, -1.3115e-01,  5.0201e-01,  6.3343e-01,  5.2671e-01,\n",
       "           -9.7404e-01, -9.8705e-01,  5.1883e-01, -4.7778e-01,  8.2174e-01,\n",
       "           -9.9996e-01, -9.0393e-01,  8.0857e-01,  9.0399e-01, -4.5038e-01,\n",
       "            7.5398e-01, -7.1045e-01,  2.9036e-01, -5.9648e-02,  3.0866e-01,\n",
       "           -9.4066e-01, -9.3685e-01, -6.8451e-01,  9.2701e-01, -5.0619e-01,\n",
       "           -2.7839e-01, -4.6891e-01, -1.2417e-01, -9.9962e-01,  8.7389e-01,\n",
       "           -7.9895e-02, -7.8269e-01, -9.7875e-01, -2.9590e-01, -3.9344e-01,\n",
       "           -1.4673e-01,  7.8729e-01, -6.2610e-01,  3.4940e-01,  9.9061e-01,\n",
       "           -2.6366e-01, -3.4765e-01,  9.1119e-01,  8.4842e-01, -4.5443e-01,\n",
       "           -6.2702e-01,  3.4834e-01, -8.6229e-01,  8.6025e-01, -9.9882e-01,\n",
       "            9.2622e-01,  6.6160e-02, -7.7616e-01, -9.6846e-01,  5.5428e-01,\n",
       "            1.7630e-01,  9.9560e-01, -5.1281e-01, -9.5121e-01,  8.7267e-01,\n",
       "           -9.9444e-01, -4.6343e-01, -6.0289e-01,  3.0689e-01,  7.0989e-01,\n",
       "            4.6448e-01, -1.9836e-01,  4.2308e-01, -3.8463e-01,  7.5540e-01,\n",
       "           -6.0631e-01,  9.2488e-01, -9.3783e-01,  2.5282e-02, -2.8088e-01,\n",
       "            6.6355e-03, -4.4848e-01, -8.9822e-01,  4.8914e-01,  2.1924e-01,\n",
       "            9.1430e-01,  3.3197e-01,  3.6423e-01, -4.3652e-01,  7.5889e-02,\n",
       "           -4.6072e-01, -5.2628e-01, -8.8886e-01,  5.2144e-02, -2.8270e-02,\n",
       "            8.0140e-01, -1.2161e-01,  6.6674e-01,  1.3140e-01, -1.8143e-02,\n",
       "            3.6221e-02,  1.1665e-01,  2.7423e-01, -4.4228e-01, -7.1611e-01,\n",
       "            9.2069e-01, -9.1417e-01,  5.5888e-01,  1.0830e-01, -3.1567e-01,\n",
       "           -6.1858e-01,  3.2373e-02,  7.9429e-01, -8.3048e-01,  9.4587e-01,\n",
       "            9.9599e-01, -5.4870e-01, -9.9992e-01, -3.3990e-04,  1.7413e-01,\n",
       "            3.9348e-01,  8.7912e-01, -6.3477e-01,  4.5305e-01, -2.6718e-01,\n",
       "            9.9999e-01, -8.8374e-03,  7.3954e-01, -3.9882e-01,  2.5657e-01,\n",
       "            1.5365e-01,  7.6933e-01,  9.3814e-01,  3.3837e-01,  8.1896e-01,\n",
       "            2.0719e-02,  9.3692e-01, -8.5859e-01, -9.1749e-01,  3.8769e-01,\n",
       "           -4.7103e-01,  5.3401e-03,  3.2568e-01,  2.7439e-01, -1.5015e-01,\n",
       "           -2.3005e-01, -7.8612e-01, -9.9923e-01, -4.2257e-01,  4.1823e-01,\n",
       "            8.5368e-02]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.6044, -0.2301,  0.7049,  0.8744,  0.1193, -0.8715,  0.5821,\n",
       "            0.5309,  0.0541, -0.6183, -0.9514, -0.1118,  0.4345, -0.4864,\n",
       "            0.6499, -0.7278,  0.8494, -0.8062, -0.9676,  0.6573, -0.2391,\n",
       "            0.9599, -0.2212, -0.8696,  0.9365, -0.7403, -0.8036,  0.8871,\n",
       "            0.9483,  0.8736,  0.9971, -0.9449, -0.5181, -0.3953,  0.7035,\n",
       "            0.9993,  0.5004,  0.9057, -0.1692, -0.9982,  0.7062,  0.0417,\n",
       "            0.3003, -0.8924, -0.6489,  0.4496,  0.6980, -0.0876, -0.2821,\n",
       "           -0.9374,  0.9819, -0.2643,  0.8849, -0.5078, -0.1282, -0.9141,\n",
       "           -0.4039,  0.5434, -0.9816,  0.9035, -0.8265, -0.9856,  0.6159,\n",
       "           -0.4378,  0.0214,  0.9014, -0.0612, -0.5875, -0.7194, -0.4702,\n",
       "           -0.1866,  0.9102,  0.3060,  0.9970, -0.4356,  0.9673,  0.9373,\n",
       "            0.7201, -0.6446, -0.9762,  0.3217,  0.6149, -0.6694, -0.8903,\n",
       "           -0.1662, -0.1061, -0.7702, -0.9731, -0.9943, -0.7632, -0.3624,\n",
       "           -0.8485, -0.6732, -0.9581,  0.9203, -0.6397,  0.4668,  0.1368,\n",
       "           -0.7761, -0.0341,  0.4951, -0.5967,  0.8918, -0.9419, -0.7451,\n",
       "           -0.9043,  0.6154,  0.0337,  0.9583, -0.6485, -0.3590, -0.6654,\n",
       "            0.6262,  0.7355,  0.8156, -0.9691, -0.8777,  0.1900, -0.9979,\n",
       "            0.1252,  0.9967,  0.9886,  0.7183,  0.9073, -0.9941, -0.3008,\n",
       "           -0.3782,  0.8691,  0.7848,  0.2539, -0.8970, -0.9198,  0.5533,\n",
       "           -0.9070,  0.8166, -0.7657, -0.0404, -0.4019,  0.9376,  0.9311,\n",
       "            0.8467,  0.9753, -0.9808, -0.3017, -0.8711, -0.0838,  0.6659,\n",
       "            0.7669,  0.9985, -0.9746, -0.6203, -0.3699,  0.8674,  0.5431,\n",
       "           -0.2496, -0.9203,  0.3353, -0.3699,  0.9912, -0.7513,  0.9956,\n",
       "            0.1671, -0.9312, -0.3527,  0.3852, -0.5659, -0.9039,  0.9677,\n",
       "           -0.9361, -0.8876, -0.9697,  0.7062, -0.8235, -0.6153,  0.9615,\n",
       "            0.5470,  0.5215,  0.9602, -0.4711,  0.6736, -0.8618, -0.8932,\n",
       "            0.7964,  0.0453, -0.3431,  0.1377, -0.7304,  0.9856,  0.3280,\n",
       "           -0.3313, -0.3103, -0.9728, -0.5174, -0.5927,  0.7433, -0.7900,\n",
       "           -0.2046, -0.3791, -0.0469,  0.6083, -0.6481, -0.1216,  0.9375,\n",
       "           -0.5403, -0.0460,  0.9679,  0.2132, -0.2289, -0.3440, -0.5789,\n",
       "            0.8784, -0.0917,  0.5589,  0.2286,  0.7709,  0.9766, -0.1124,\n",
       "            0.7022, -0.7985,  0.9631, -0.5448, -0.5964, -0.5035,  0.9532,\n",
       "            0.1713,  0.3475, -0.9471, -0.6455,  0.4196,  0.9746, -0.9909,\n",
       "           -0.1192,  0.3046, -0.7869,  0.5747,  0.7281,  0.3117,  0.9570,\n",
       "            0.7866,  0.9775, -0.1142, -0.9977, -0.8592,  0.5801,  0.5705,\n",
       "           -0.4419,  0.8465, -0.3549,  0.9297, -0.4438, -0.6861,  0.0644,\n",
       "            0.9931, -0.8521,  0.4190,  0.0822]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.5217,  0.7315,  0.9956,  0.9295,  0.8721, -0.7874,  0.9015,\n",
       "            0.8001, -0.9344, -0.9760, -0.5898, -0.9950,  0.4178, -0.6756,\n",
       "            0.0167, -0.6594,  0.6241, -0.7156,  0.9983,  0.6553,  0.7946,\n",
       "            0.9996, -0.9827, -0.8534, -0.7977, -0.6779, -0.9933,  0.5666,\n",
       "            0.1815,  0.7429,  0.9775, -0.9997, -0.3757, -0.5110, -0.6791,\n",
       "           -0.9984, -0.7313,  0.9739, -0.9582, -1.0000, -0.5391, -0.7566,\n",
       "           -0.9999, -0.8919, -0.8655, -0.2092,  0.8522,  0.1750,  0.2772,\n",
       "            0.9369, -0.7477, -0.8184,  0.4734, -0.8836, -0.9974, -0.8716,\n",
       "           -0.9455,  0.9574,  0.9497,  0.9811, -0.7350, -0.8918, -0.7771,\n",
       "           -0.5555,  0.6968,  0.8822, -0.8474,  0.9709,  0.5003, -0.1457,\n",
       "           -0.1698,  0.9515, -0.7975,  1.0000,  0.5427,  0.8220,  0.9643,\n",
       "            0.5088, -0.9210, -0.9756,  0.9604,  0.5456, -0.6698,  0.9490,\n",
       "            0.6259,  0.6963, -0.7533, -0.9985, -0.9872, -0.7812,  0.8710,\n",
       "           -0.0675, -0.9946,  0.9575, -0.2681, -0.9822,  0.9538, -0.2736,\n",
       "           -0.8650,  0.6692,  0.8689,  0.1598, -0.9986, -0.6426,  0.2998,\n",
       "           -0.0226,  0.1040, -0.4734,  0.9901, -0.7855, -0.3317, -0.6528,\n",
       "            0.9906,  0.9999,  0.4752,  0.0281, -0.1228, -0.6781,  0.8054,\n",
       "           -0.3009, -0.9992,  0.6723,  0.2273,  0.9130,  0.2440, -0.9856,\n",
       "            0.7072,  0.8834,  0.8731, -0.1942, -0.9840,  0.9776, -0.8959,\n",
       "           -0.9943,  0.9256,  0.9165, -0.0853, -0.8656,  0.6382,  0.4670,\n",
       "           -0.7783, -0.9926,  0.9542, -0.7560, -0.8957,  0.2241, -0.9597,\n",
       "            0.9234,  0.9563, -0.9740, -0.9991, -0.4547,  0.6054, -0.3734,\n",
       "            0.9786,  0.9795, -0.9886, -0.7496,  0.9906, -0.7521,  0.9930,\n",
       "           -0.9997, -0.9602, -0.6098,  0.2833, -0.5509, -0.9222,  0.9940,\n",
       "           -0.1241, -0.9610,  0.9975,  0.9751, -0.4914, -0.9679,  0.4352,\n",
       "            0.9991, -0.3300,  0.9123, -0.9186, -0.9708, -0.8819, -0.8935,\n",
       "           -0.0417, -0.2939, -0.7206,  0.0472, -0.9017,  0.9050, -0.5774,\n",
       "           -0.5659, -0.9733, -0.9266, -0.8818,  0.7413, -0.9698,  0.5227,\n",
       "            0.9702, -0.8559, -0.9935,  0.9898, -0.9519, -0.6480,  0.9977,\n",
       "           -0.9912, -0.8263,  1.0000, -0.9982, -0.3076, -0.9682, -0.8351,\n",
       "            0.4483,  0.0822,  0.6102,  0.2295,  0.8630,  0.5061, -0.1701,\n",
       "           -0.4374, -0.3291,  0.9564,  0.9355, -0.7028,  0.6230,  0.9652,\n",
       "            0.9985,  0.5784, -0.2750, -0.9709, -0.9660, -0.4118,  0.6046,\n",
       "           -0.9925,  0.0571, -0.9519,  0.2665,  0.2336, -0.7807,  0.9888,\n",
       "            0.2417,  0.9610,  0.6979,  0.9272, -0.8597, -0.4633, -0.5798,\n",
       "           -0.4343,  0.9970, -0.3038,  0.9879, -0.5688,  0.4101, -0.7227,\n",
       "            0.1471, -0.8529,  0.9838,  0.1460]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.7728,  0.7471,  0.9999,  0.9296, -0.8838, -0.8183,  0.6802,\n",
       "            0.8006, -0.9998, -0.9307, -0.5872, -0.9951, -0.8337, -0.5628,\n",
       "            0.3407, -0.6680,  0.6247, -0.9687, -0.9972,  0.6553,  0.9795,\n",
       "            0.9995, -0.9271, -0.8526,  1.0000,  0.9150, -0.9992, -0.9943,\n",
       "            0.9970, -0.9833,  0.4033, -0.9999, -0.9888, -0.5698,  0.9924,\n",
       "            0.9998, -0.7830,  0.9741, -0.9564, -0.5634,  0.9760, -0.7712,\n",
       "            0.9927, -0.4033, -0.8572, -0.0480,  0.8742, -0.0138,  0.9992,\n",
       "            0.1593,  0.6011, -0.8188, -0.7086, -0.8723,  0.9999,  0.9941,\n",
       "           -0.9428,  0.8812,  0.9999,  0.9963, -0.4072,  0.5637, -0.7880,\n",
       "           -0.5570,  0.7851,  0.7456, -0.8531,  0.9702,  0.4965, -0.1436,\n",
       "            0.4223,  0.9517, -0.9051,  1.0000,  0.5184,  0.3026,  0.9735,\n",
       "            0.9960, -0.9063, -0.8797,  0.9575, -0.1491, -0.5115, -0.9932,\n",
       "            0.6142,  0.6554, -0.7280, -0.9986, -0.9697, -0.6220,  0.8843,\n",
       "            0.3342,  0.9999,  0.5148, -0.9989, -0.9827, -0.8767, -0.2548,\n",
       "           -0.8603, -0.9998,  0.9041,  0.0847, -0.9968, -0.6722, -0.0826,\n",
       "           -0.3370,  0.4687, -0.6428,  0.6709,  0.7369, -0.3361, -0.6579,\n",
       "            0.9974, -0.9593,  0.2167,  0.9895,  0.9987, -0.3585,  0.8953,\n",
       "           -0.3076, -0.9997, -0.9887,  0.2230,  0.9127, -0.9155,  0.8256,\n",
       "            0.7596,  0.9972,  0.5923, -0.2292,  0.0054,  0.8517, -0.9964,\n",
       "            0.5251, -0.4100,  0.9961, -0.1155, -0.8880, -0.9789,  0.4420,\n",
       "            0.9639, -0.9922, -0.9807, -0.7540, -0.8376,  0.2247,  0.9996,\n",
       "            0.9896,  0.9923,  0.0566, -0.9793, -0.4551,  0.5929, -0.3220,\n",
       "            0.9783, -0.1334, -0.9967, -0.7521, -0.9183, -0.9994,  0.9739,\n",
       "           -0.9989, -0.9785, -0.6221,  0.2795, -0.4837,  0.6776,  0.5086,\n",
       "           -0.3642,  0.9973, -0.9906,  0.6135, -0.7395, -0.9453,  0.5524,\n",
       "            0.9351, -0.3289,  0.9734, -0.9244,  0.9379, -0.8970, -0.8925,\n",
       "           -0.1442, -0.2973, -0.7309, -0.0626, -0.9898, -0.2250, -0.5463,\n",
       "           -0.9072,  0.9916, -0.6331, -0.8975,  0.0697,  0.2427, -0.3249,\n",
       "            0.9793, -0.9971, -0.9969, -0.6275, -0.9531, -0.7568,  0.9981,\n",
       "            0.7828, -0.8431,  0.9999, -0.3852, -0.3064, -1.0000, -0.8388,\n",
       "            0.2673,  0.4008,  0.8002,  0.2290,  0.8637,  0.5055, -0.1554,\n",
       "           -0.9981,  0.1101,  0.9579,  0.9839, -0.8838, -0.9998,  0.9674,\n",
       "           -0.7802,  0.5795, -0.2204, -0.9708, -0.7560,  0.5027,  1.0000,\n",
       "            0.8167,  0.0309, -0.8469,  0.2199, -0.4631, -0.3064,  0.9753,\n",
       "            0.3598,  0.9652,  0.7459,  0.9870, -0.8595,  0.2492, -0.9076,\n",
       "           -0.3958, -0.0831, -0.3080,  1.0000, -0.0714, -0.5216, -0.7242,\n",
       "           -0.9861, -0.9742, -0.7102, -0.4792]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6017,  0.9814,  1.0000,  0.9320,  0.8750, -0.7323,  0.5372,\n",
       "            0.7989, -0.9995, -0.9985, -0.5206, -0.8792, -0.4027, -0.3352,\n",
       "            0.5152, -0.9927,  0.6314, -0.9764, -0.9383,  0.6576,  0.8972,\n",
       "            0.9764, -0.9381, -0.8496,  0.9996, -0.6588, -0.9859,  0.3325,\n",
       "           -0.3079,  0.3064,  0.9978, -0.9974, -0.9889, -0.6972,  0.2735,\n",
       "            0.9999, -0.0952,  0.9115, -0.8160, -0.9642,  0.9964, -0.9158,\n",
       "           -0.7283, -0.5746, -0.4838,  0.7911,  0.9797,  0.9347,  0.9925,\n",
       "            0.7901,  0.9807, -0.8254, -0.2887, -0.8751,  0.7373, -0.3830,\n",
       "           -0.5673,  0.8043,  0.9932,  0.3620,  0.3278,  0.5896, -0.7617,\n",
       "           -0.5428,  0.7545,  0.7265, -0.8970, -0.9186,  0.4861, -0.1023,\n",
       "            0.4933,  0.9498, -0.7333,  0.9965, -0.5522, -0.8525,  0.9879,\n",
       "            0.9976, -0.9161,  0.1328,  0.8890, -0.0569, -0.1749, -0.2751,\n",
       "            0.5054,  0.7828, -0.9481, -0.9981,  0.0122, -0.9420, -0.9374,\n",
       "           -0.9355, -0.4584, -0.2928,  0.9720, -0.9418, -0.2547,  0.0161,\n",
       "           -0.3923, -0.9989,  0.9051, -0.1248,  0.7279, -0.9533,  0.8360,\n",
       "           -0.4108, -0.9366, -0.7666,  0.6860, -0.4473, -0.3939, -0.7815,\n",
       "            0.9981,  0.3703, -0.4501,  0.9897,  0.9333,  0.6315, -0.1660,\n",
       "           -0.4655,  0.6046,  0.9599,  0.2532,  0.8991, -0.9987, -0.5989,\n",
       "            0.4608,  0.9984,  0.8125, -0.4527, -0.6759,  0.4304, -0.5186,\n",
       "           -0.5909,  0.7883,  0.0905, -0.0255, -0.9406,  0.5892,  0.4433,\n",
       "            0.6085,  0.9962,  0.7910, -0.6682, -0.5853,  0.2190, -0.8634,\n",
       "            0.7847,  0.9861, -0.8960, -0.2111, -0.4574,  0.4715, -0.0590,\n",
       "           -0.2879,  0.2411, -0.9967,  0.2821, -0.3258,  0.8787,  0.9999,\n",
       "            0.2256, -0.9020, -0.3490,  0.2627, -0.9839, -0.9159,  0.5627,\n",
       "            0.8433, -0.9809,  0.7054,  0.8856, -0.9321, -0.9672,  0.7207,\n",
       "            0.9269, -0.3174,  0.9868, -0.9245,  0.4429, -0.9057, -0.9538,\n",
       "            0.7533, -0.2379, -0.8178, -0.3632, -0.9839,  0.2285, -0.8801,\n",
       "           -0.9923, -0.8098, -0.7451, -0.9691, -0.2592,  0.5402, -0.0596,\n",
       "            0.9945, -0.9321, -0.9970, -0.6766, -0.7628, -0.7635,  0.7004,\n",
       "           -0.0929, -0.8429, -0.4037,  0.5384, -0.3045, -0.8983, -0.7350,\n",
       "           -0.0190,  0.3427,  0.8009, -0.5429,  0.7700,  0.5014,  0.2976,\n",
       "           -0.9973,  0.5207,  0.9653, -0.8994, -0.7616,  0.1620,  0.9721,\n",
       "           -0.7889,  0.5953, -0.4360, -0.7506, -0.3159,  0.6262, -0.9903,\n",
       "            0.0756, -0.1742, -0.2224,  0.6528, -0.7609,  0.7702,  0.9945,\n",
       "            0.7059,  0.9901,  0.9015, -0.9769, -0.8604,  0.5516,  0.9440,\n",
       "           -0.3994, -0.4306, -0.3203,  0.9406,  0.4818, -0.8489, -0.5811,\n",
       "            0.9617, -0.3004, -0.5510, -0.7266]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.2856,  0.9956,  1.0000,  0.9333,  0.9154, -0.7664,  0.5012,\n",
       "            0.8741, -0.9950, -0.9997, -0.5204, -0.9021, -0.4880, -0.1293,\n",
       "            0.4952, -0.9929,  0.6308,  0.1919,  0.9991,  0.6760,  0.8693,\n",
       "            0.9808, -0.9664, -0.8925,  0.4303,  0.0380, -0.9969,  0.6025,\n",
       "           -0.0014,  0.2017,  0.8632, -0.9999, -0.5032, -0.7014,  0.4137,\n",
       "            0.0100, -0.8948,  0.9257, -0.9071, -0.9901, -0.8004, -0.9924,\n",
       "           -1.0000, -0.5744, -0.2156,  0.7020,  0.9822,  0.5737,  0.9968,\n",
       "            0.9997,  0.3171, -0.9795,  0.2147, -0.8804, -0.9625, -0.5299,\n",
       "           -0.9355,  0.9733,  0.9992,  0.5416,  0.6461,  0.7787, -0.8978,\n",
       "           -0.5723,  0.9019,  0.6790, -0.9767,  0.9874,  0.4667, -0.0738,\n",
       "            0.4929,  0.9869, -0.9760,  0.9979, -0.7061, -0.9049,  0.9957,\n",
       "            0.7417, -0.9175, -0.2591,  0.9074,  0.6011, -0.1683,  0.9808,\n",
       "            0.4609,  0.8927, -0.3780, -0.9958, -0.0728, -0.9322,  0.8116,\n",
       "           -0.7413, -0.7990,  0.6045, -0.9817, -0.9920,  0.9047,  0.3203,\n",
       "           -0.4414, -0.9950,  0.9443, -0.4601, -0.9919,  0.5787,  0.9100,\n",
       "           -0.3544, -0.1286,  0.1751,  0.7904, -0.8216, -0.4039, -0.7055,\n",
       "            0.9999,  0.6943, -0.5524,  0.9948, -0.7913,  0.6260,  0.0742,\n",
       "           -0.5938, -0.9824,  0.8990,  0.2356,  0.8694, -0.5271, -0.3129,\n",
       "            0.8636,  0.9984,  0.7050, -0.6104, -0.9815,  0.9553, -0.9141,\n",
       "           -0.9982,  0.7928,  0.8032, -0.0309, -0.8660,  0.5484,  0.3807,\n",
       "            0.3105, -0.9777,  0.9997, -0.7272, -0.5859,  0.1470, -0.9503,\n",
       "            0.6384,  0.0597, -0.8976, -0.9994, -0.4941, -0.3796, -0.3004,\n",
       "            0.9685,  0.9989, -0.9989, -0.6255, -0.2763,  0.3515,  0.9703,\n",
       "           -0.9911, -0.9706, -0.3539,  0.4520,  0.4295, -0.9600,  0.6981,\n",
       "            0.9458, -0.9959,  0.9986,  0.7659, -0.9470, -0.9880, -0.5264,\n",
       "            0.9460, -0.1906,  0.9891, -0.9290, -0.7495, -0.9059, -0.9538,\n",
       "            0.8463, -0.7879, -0.9862, -0.6435, -0.9880,  0.4272, -0.9939,\n",
       "           -0.9797, -0.7222, -0.6006, -0.9710, -0.4526, -0.6012,  0.6594,\n",
       "            0.9948, -0.9814, -0.9997,  0.9936, -0.9823, -0.7497,  0.9175,\n",
       "           -0.8295, -0.9284,  0.9993, -0.9773, -0.3045, -0.9922, -0.8480,\n",
       "           -0.4235, -0.0306,  0.7901,  0.1680,  0.7723,  0.4662,  0.1758,\n",
       "           -0.8703,  0.9332,  0.9627,  0.9956, -0.6910,  0.0331,  0.9793,\n",
       "           -0.2053,  0.7795, -0.3407, -0.8668, -0.9680,  0.1465, -0.2362,\n",
       "           -0.9977, -0.4581, -0.6254, -0.0643, -0.8282,  0.7957,  0.9981,\n",
       "            0.4095,  0.9446,  0.9775, -0.4088, -0.9502,  0.2125,  0.9696,\n",
       "           -0.3629,  0.5007, -0.3114,  0.9936, -0.4640, -0.1767, -0.5385,\n",
       "            0.5998, -0.3088,  0.9895, -0.4031]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.5814,  0.9970,  1.0000,  0.9335, -0.9889, -0.6917, -0.1623,\n",
       "            0.8748, -1.0000, -0.9974, -0.4893, -0.9064, -0.8511, -0.0318,\n",
       "            0.6185, -0.9930,  0.6322, -0.9895, -0.9771,  0.6732,  0.9993,\n",
       "            0.9788, -0.9731, -0.8920,  0.9997,  0.6877, -0.9981, -0.9708,\n",
       "            0.9646, -0.7705,  0.7809, -0.9984, -0.9688, -0.7074,  0.9973,\n",
       "            0.9998, -0.9146,  0.9269, -0.8965, -0.9695,  0.9910, -0.9477,\n",
       "            0.9606,  0.1438, -0.3589,  0.7122,  0.9825, -0.2525,  1.0000,\n",
       "            0.3118, -0.0723, -0.9755, -0.6070, -0.8682,  0.9988,  0.7765,\n",
       "           -0.9295,  0.9110,  0.9999,  0.9139,  0.9399,  0.9883, -0.8836,\n",
       "           -0.5728,  0.9546,  0.3527, -0.9777,  0.9853,  0.4685, -0.0688,\n",
       "            0.3522,  0.9847, -0.9288,  0.9980, -0.7198, -0.9305,  0.9824,\n",
       "            0.9980, -0.9143, -0.5520,  0.9104,  0.6366, -0.0342, -0.8690,\n",
       "            0.4496,  0.9237, -0.1964, -0.9969, -0.3908, -0.8834,  0.8444,\n",
       "           -0.6495,  0.9990,  0.4391, -1.0000, -0.9926, -0.6570,  0.2276,\n",
       "           -0.4049, -1.0000,  0.9550, -0.4176, -0.9811,  0.4838,  0.4625,\n",
       "           -0.1227,  0.2617,  0.3170, -0.4983,  0.9731, -0.4208, -0.7079,\n",
       "            0.9997, -0.9558, -0.7837,  0.9948,  0.9922,  0.6640,  0.5889,\n",
       "           -0.6025, -0.9988, -0.9269,  0.1761,  0.8690,  0.3902,  0.9813,\n",
       "            0.8404,  0.9997,  0.6298, -0.6358, -0.2593, -0.7303, -0.9970,\n",
       "            0.7232,  0.1305,  0.9935, -0.0418, -0.8660, -0.9733,  0.3098,\n",
       "            0.9742, -0.9800, -0.4615, -0.7291, -0.5219,  0.1480,  0.9966,\n",
       "            0.4189,  0.7885, -0.7811, -0.9808, -0.4944, -0.3851, -0.3057,\n",
       "            0.9831,  0.3471, -0.9999, -0.7005, -0.9777, -0.9984,  0.9856,\n",
       "           -0.9960, -0.8766, -0.3808,  0.4472,  0.5045,  0.8244,  0.5752,\n",
       "            0.9829,  0.9853, -0.9875,  0.3367, -0.9661, -0.9375, -0.8045,\n",
       "            0.9013, -0.1888,  0.9985, -0.9294,  0.9950, -0.9127, -0.9524,\n",
       "           -0.7523, -0.7758, -0.9912, -0.8521, -0.9964,  0.3861, -0.9098,\n",
       "           -0.7210,  0.9874,  0.5667, -0.9832, -0.5841,  0.2735, -0.0082,\n",
       "            0.9948, -0.9999, -0.9999, -0.9215, -0.9808, -0.9066,  0.9439,\n",
       "            0.8470, -0.8990,  0.9996, -0.9247, -0.3022, -0.9999, -0.8624,\n",
       "           -0.6039,  0.1888,  0.9064,  0.1330,  0.7734,  0.4658,  0.1836,\n",
       "           -0.9890,  0.9869,  0.9735,  0.9976, -0.8359, -0.9947,  0.9241,\n",
       "           -0.8747,  0.7832, -0.2839, -0.8681, -0.8056,  0.1916,  0.9992,\n",
       "            0.8309, -0.5034, -0.5117, -0.1900, -0.9590,  0.5786,  0.9971,\n",
       "            0.5902,  0.9280,  0.9880,  0.6667, -0.9499,  0.1473, -0.5563,\n",
       "           -0.3232,  0.2793, -0.3082,  1.0000,  0.2111, -0.6870, -0.5091,\n",
       "           -0.7449, -0.9185, -0.8860, -0.8546]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4070,  0.9986,  1.0000,  0.9341,  0.9776, -0.3129, -0.0379,\n",
       "            0.8729, -1.0000, -0.9996, -0.4421, -0.8442,  0.0716, -0.0930,\n",
       "            0.6317, -0.9833,  0.6385, -0.6365, -0.7150,  0.6774,  0.8446,\n",
       "            0.9585, -0.9713, -0.8920,  0.9998,  0.0054, -0.9851, -0.8076,\n",
       "           -0.8112,  0.6157,  0.9986, -0.9986, -0.9697, -0.5995, -0.8007,\n",
       "            1.0000, -0.8132,  0.9146, -0.6397, -0.9944,  0.9318, -0.9593,\n",
       "           -0.9830, -0.1165, -0.1040,  0.6327,  0.9833,  0.9658,  0.9952,\n",
       "            0.8976,  0.9295, -0.9755, -0.7144, -0.8678,  0.8249, -0.7808,\n",
       "           -0.8900,  0.9404,  0.9984,  0.1363,  0.7674,  0.9901, -0.8456,\n",
       "           -0.5724,  0.7683,  0.4171, -0.9785, -0.9885,  0.4683, -0.0523,\n",
       "            0.2167,  0.9814, -0.7113,  0.9960, -0.8089, -0.9382,  0.9875,\n",
       "            0.9934, -0.9182, -0.3515,  0.9049,  0.7829, -0.1927, -0.3116,\n",
       "            0.3568,  0.9535, -0.9375, -0.9965,  0.4927, -0.4657, -0.9944,\n",
       "           -0.9849,  0.2976, -0.0620,  0.9979, -0.0979, -0.2713,  0.2868,\n",
       "            0.2509, -0.9972,  0.9357, -0.4414,  0.9468, -0.7167,  0.9123,\n",
       "           -0.2569, -0.7588, -0.1516, -0.5748, -0.7959, -0.4357, -0.6998,\n",
       "            0.9985, -0.0893, -0.3890,  0.9948,  0.9663,  0.6083, -0.4516,\n",
       "           -0.6109,  0.6508,  0.9212,  0.1866,  0.8673, -0.9732,  0.2612,\n",
       "            0.6744,  0.9964,  0.7594, -0.6534, -0.6380, -0.7855, -0.6199,\n",
       "           -0.8966,  0.3073, -0.1647,  0.0402, -0.7672,  0.8544,  0.3044,\n",
       "            0.7912,  0.9997,  0.6849, -0.7283, -0.3882,  0.1477, -0.7995,\n",
       "           -0.7165,  0.9793, -0.9883,  0.1914, -0.4945, -0.4170, -0.3365,\n",
       "            0.2299, -0.2858, -0.9999,  0.7423, -0.5172,  0.9637,  0.9749,\n",
       "           -0.3007, -0.6362, -0.3239,  0.4462, -0.9328, -0.9726,  0.5838,\n",
       "            0.9952, -0.9787,  0.7115,  0.6508, -0.9745, -0.9588, -0.8823,\n",
       "            0.9089, -0.1872,  0.9980, -0.9292, -0.0205, -0.9163, -0.8897,\n",
       "            0.6942, -0.7372, -0.9928, -0.3734, -0.9583,  0.5278, -0.8919,\n",
       "           -0.9529,  0.5935, -0.8287, -0.9777, -0.5466,  0.5110,  0.0781,\n",
       "            0.9949,  0.6889, -0.9999, -0.6295, -0.8921, -0.9098,  0.5613,\n",
       "            0.8527, -0.8946, -0.8952, -0.3941, -0.3002, -0.4651, -0.8532,\n",
       "           -0.4879,  0.2600,  0.9066, -0.8901,  0.7647,  0.4617,  0.2403,\n",
       "           -0.9892,  0.8298,  0.6933, -0.8675, -0.6696, -0.2947,  0.8979,\n",
       "           -0.8775,  0.7546, -0.3673, -0.8217,  0.4367, -0.3607, -0.9971,\n",
       "           -0.6991, -0.5345, -0.3747,  0.6182, -0.5140,  0.9236,  0.9995,\n",
       "            0.8760,  0.9555,  0.9665, -0.9433, -0.9312,  0.2066,  0.9152,\n",
       "           -0.3251, -0.7411, -0.3112,  0.6656,  0.7531, -0.6697, -0.4621,\n",
       "            0.9955,  0.5664, -0.7208, -0.7364]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.7779,  0.9998,  1.0000,  0.9357,  0.9931, -0.6336, -0.2588,\n",
       "            0.9332, -0.9954, -0.9999, -0.4419, -0.8709, -0.2185,  0.3077,\n",
       "            0.6385, -0.9849,  0.6434,  0.8547,  0.9997,  0.7410,  0.7846,\n",
       "            0.9683, -0.9824, -0.9714, -0.4596, -0.4565, -0.9488, -0.0171,\n",
       "           -0.8718,  0.4147,  0.9905, -0.9985, -0.7242, -0.6107, -0.5570,\n",
       "           -0.9554, -0.8366,  0.9418, -0.6004, -0.9965, -0.8229, -0.9952,\n",
       "           -1.0000, -0.1172,  0.0244,  0.4725,  0.9853,  0.9451,  0.9595,\n",
       "            0.9990, -0.3106, -0.9965,  0.0297, -0.8994, -0.9937, -0.7873,\n",
       "           -0.9930,  0.9916,  0.9989,  0.2013,  0.3028,  0.9938, -0.8773,\n",
       "           -0.5924,  0.9058,  0.1698, -0.9939,  0.9846,  0.3718, -0.0405,\n",
       "            0.6303,  0.9936, -0.9933,  0.9979, -0.9284, -0.9451,  0.9921,\n",
       "            0.5572, -0.9199, -0.4368,  0.9108,  0.9043, -0.1830,  0.9587,\n",
       "            0.2568,  0.9757, -0.5960, -0.9839,  0.2937, -0.4482,  0.8935,\n",
       "           -0.7083, -0.9936,  0.3043, -0.9197, -0.9770,  0.8698,  0.0336,\n",
       "           -0.2389, -0.9896,  0.9550, -0.0024, -0.9959,  0.3749,  0.8757,\n",
       "           -0.2148, -0.6297,  0.7426, -0.3153, -0.8957, -0.4391, -0.4966,\n",
       "            0.9999,  0.9860, -0.2724,  0.9966, -0.9934,  0.5974, -0.1251,\n",
       "           -0.6287, -0.8064,  0.9168,  0.1776,  0.8139, -0.7177,  0.2765,\n",
       "            0.9145,  0.9965,  0.8403, -0.7904, -0.9832,  0.7863, -0.9733,\n",
       "           -0.9993,  0.9468,  0.5914,  0.0054, -0.2679,  0.8329,  0.2923,\n",
       "            0.7489, -0.9873,  0.9992, -0.7699, -0.3899,  0.2617, -0.8773,\n",
       "            0.0384,  0.6647, -0.9883, -0.9979, -0.5221, -0.6263, -0.6582,\n",
       "            0.9386,  0.9971, -0.9996,  0.2016, -0.5132,  0.1940,  0.7419,\n",
       "           -0.9976, -0.7421, -0.2710,  0.5183,  0.7006, -0.9819,  0.6494,\n",
       "            0.9924, -0.9975,  0.9997,  0.8152, -0.9731, -0.9835, -0.8780,\n",
       "            0.9554,  0.1128,  0.9991, -0.9465, -0.9239, -0.9164, -0.8897,\n",
       "            0.8822, -0.7995, -0.9995, -0.6151, -0.9632,  0.5585, -0.9642,\n",
       "           -0.9943, -0.6031, -0.9041, -0.9398, -0.5687, -0.7553,  0.4956,\n",
       "            0.9935, -0.5783, -0.9999,  0.9975, -0.9877, -0.1833,  0.7650,\n",
       "           -0.0894, -0.9121,  0.9970, -0.9958, -0.3003, -0.9841, -0.8899,\n",
       "            0.9111,  0.2035,  0.7815, -0.3349,  0.7626,  0.3740, -0.1001,\n",
       "           -0.5003,  0.9296,  0.6985,  0.9950, -0.5142, -0.0044,  0.9180,\n",
       "            0.6473,  0.9568, -0.3587, -0.8904, -0.9933,  0.5173, -0.7832,\n",
       "           -0.9993, -0.6487, -0.8593,  0.5388, -0.9148,  0.7643,  0.9992,\n",
       "            0.7363,  0.9132,  0.9967, -0.7668, -0.9800, -0.2588,  0.9536,\n",
       "           -0.3199,  0.2502, -0.2957,  0.9634, -0.6306,  0.0570, -0.3261,\n",
       "            0.9528,  0.5631,  0.9856, -0.7637]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.5302,  0.9994,  0.9999,  0.9359, -0.9758, -0.5976,  0.0330,\n",
       "            0.9337, -1.0000, -0.9915, -0.4240, -0.8761, -0.8085,  0.3648,\n",
       "            0.5210, -0.9849,  0.6499, -0.9824, -0.9461,  0.7354,  0.9969,\n",
       "            0.9675, -0.9860, -0.9707,  0.9997,  0.4263, -0.9616, -0.9796,\n",
       "            0.9372, -0.9102,  0.9585, -0.9993, -0.9070, -0.6432,  0.9902,\n",
       "            0.9997, -0.8485,  0.9437, -0.6059, -0.9820,  0.9396, -0.8247,\n",
       "            0.9699,  0.4278, -0.2269,  0.4602,  0.9853, -0.4547,  0.9993,\n",
       "            0.4769, -0.7434, -0.9955, -0.7466, -0.8634,  0.9977,  0.9236,\n",
       "           -0.9886,  0.8570,  1.0000,  0.7620,  0.8082,  0.9939, -0.8782,\n",
       "           -0.5930,  0.9225, -0.4498, -0.9923,  0.9994,  0.3764, -0.0339,\n",
       "            0.5017,  0.9918, -0.9353,  0.9980, -0.8569, -0.9450,  0.9821,\n",
       "            0.9976, -0.9178, -0.5054,  0.9012,  0.5701, -0.1134, -0.8672,\n",
       "            0.1787,  0.9542, -0.0471, -0.9891, -0.0512, -0.4708,  0.9056,\n",
       "           -0.6773,  0.9970,  0.7233, -1.0000, -0.9785, -0.5723,  0.0084,\n",
       "           -0.2368, -1.0000,  0.9607, -0.0163, -0.9971,  0.2686,  0.0529,\n",
       "           -0.2368, -0.3754,  0.7167, -0.2237,  0.9259, -0.4454, -0.5098,\n",
       "            0.9998, -0.9317, -0.5943,  0.8925,  0.9948,  0.5783,  0.1209,\n",
       "           -0.6334, -0.9999, -0.8982,  0.0818,  0.8136,  0.3965,  0.9910,\n",
       "            0.9115,  0.9987,  0.7883, -0.8057, -0.5787, -0.5162, -0.9990,\n",
       "           -0.0186,  0.4659,  0.9897, -0.0079, -0.2893, -0.9902,  0.0228,\n",
       "            0.9291, -0.9894,  0.4140, -0.7720, -0.3770,  0.2623,  0.9976,\n",
       "            0.4854,  0.7372, -0.5497, -0.9919, -0.5227, -0.6297, -0.6808,\n",
       "            0.9552,  0.4969, -1.0000,  0.0025, -0.8943, -0.9996,  0.9755,\n",
       "           -0.9994, -0.7898, -0.2889,  0.5151,  0.7331,  0.8556,  0.5294,\n",
       "            0.8739,  0.9918, -0.9958,  0.4031, -0.9769, -0.9656, -0.9206,\n",
       "            0.9745,  0.1155,  0.9992, -0.9471,  0.9979, -0.9196, -0.8885,\n",
       "           -0.8161, -0.7935, -0.9996, -0.8771, -0.9834,  0.5107, -0.9255,\n",
       "           -0.7855,  0.9910,  0.3188, -0.9496, -0.7062, -0.2383,  0.1065,\n",
       "            0.9936, -0.9983, -1.0000, -0.9302, -0.9879, -0.7282,  0.8301,\n",
       "            0.8372, -0.9185,  0.9992, -0.7854, -0.2993, -0.9998, -0.9052,\n",
       "           -0.1418,  0.6339,  0.9068, -0.3420,  0.7646,  0.3730, -0.0901,\n",
       "           -0.9690,  0.9821,  0.8443,  0.9998, -0.6862, -0.9924,  0.9129,\n",
       "           -0.7180,  0.9560, -0.3315, -0.8967, -0.9465,  0.8312,  0.9963,\n",
       "            0.8448, -0.6422, -0.7412,  0.4600, -0.9931,  0.3418,  0.9987,\n",
       "            0.1365,  0.9302,  0.9980,  0.1537, -0.9800, -0.1357, -0.8413,\n",
       "           -0.2660,  0.2105, -0.2975,  1.0000,  0.2058, -0.2958, -0.3300,\n",
       "           -0.9713, -0.3318, -0.5790, -0.8391]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0178,  0.9995,  1.0000,  0.9362,  0.8251, -0.5025,  0.3354,\n",
       "            0.9286, -0.9999, -0.9997, -0.3182, -0.8634, -0.0364,  0.3803,\n",
       "            0.5409, -0.9935,  0.6447, -0.8331, -0.3727,  0.7393,  0.8819,\n",
       "            0.9623, -0.9851, -0.9703,  0.9998, -0.1014, -0.9754, -0.6248,\n",
       "           -0.1401,  0.3638,  0.9949, -0.9995, -0.9098, -0.5677, -0.4599,\n",
       "            1.0000, -0.7445,  0.9191, -0.5521, -0.9950,  0.9750, -0.8813,\n",
       "           -0.8894,  0.0127, -0.7343,  0.4667,  0.9860,  0.9695,  0.9958,\n",
       "            0.9628,  0.7462, -0.9952, -0.8525, -0.8623,  0.8558, -0.7564,\n",
       "           -0.9406,  0.8877,  0.9995,  0.6185,  0.8114,  0.9951, -0.8543,\n",
       "           -0.5926,  0.8211, -0.2207, -0.9923, -0.9696,  0.3763, -0.0116,\n",
       "            0.2704,  0.9876, -0.8207,  0.9973, -0.8987, -0.9235,  0.9846,\n",
       "            0.9924, -0.9197, -0.3463,  0.8936,  0.7623, -0.3219, -0.0337,\n",
       "            0.1420,  0.9005, -0.9464, -0.9889,  0.5761, -0.8401, -0.9953,\n",
       "           -0.8896,  0.0908,  0.0335,  0.9827, -0.4360, -0.2738,  0.0559,\n",
       "            0.0987, -0.9990,  0.9459, -0.0457,  0.9600, -0.5527,  0.6990,\n",
       "           -0.1544, -0.6964,  0.3080, -0.3190, -0.5107, -0.4549, -0.5224,\n",
       "            0.9991, -0.4177, -0.6068,  0.8952,  0.9889,  0.5794,  0.1953,\n",
       "           -0.6376,  0.3716,  0.9321,  0.0783,  0.8129, -0.9736,  0.7600,\n",
       "            0.7843,  0.9970,  0.6770, -0.8157, -0.7122, -0.6839, -0.7395,\n",
       "           -0.8080,  0.3088,  0.6882,  0.1023, -0.4348,  0.8203,  0.0222,\n",
       "            0.7886,  0.9994,  0.6231, -0.7699, -0.3346,  0.2618, -0.9419,\n",
       "           -0.1791,  0.9585, -0.9467, -0.0892, -0.5229, -0.6358, -0.6538,\n",
       "            0.7498, -0.0496, -1.0000,  0.8064, -0.6009,  0.9878,  0.9790,\n",
       "           -0.6134, -0.6541, -0.2542,  0.5133, -0.9170, -0.9714,  0.5418,\n",
       "            0.9837, -0.9665,  0.8541,  0.6289, -0.9800, -0.9748, -0.9393,\n",
       "            0.9577,  0.1163,  0.9975, -0.9470,  0.5343, -0.9204, -0.8831,\n",
       "            0.3753, -0.7696, -0.9996, -0.6791, -0.9827,  0.5791, -0.9042,\n",
       "           -0.9155,  0.6171, -0.7826, -0.9765, -0.5769, -0.0839,  0.2348,\n",
       "            0.9951, -0.4392, -1.0000, -0.8653, -0.9330, -0.7401,  0.5991,\n",
       "            0.8344, -0.9183, -0.6334, -0.2384, -0.2987, -0.4121, -0.8873,\n",
       "           -0.0460,  0.2763,  0.9073, -0.9061,  0.7460,  0.3635, -0.0290,\n",
       "           -0.9710,  0.8672,  0.5580, -0.6551, -0.5585,  0.1420,  0.7439,\n",
       "           -0.7234,  0.9154, -0.3641, -0.8446,  0.2056, -0.5430, -0.9951,\n",
       "           -0.6672, -0.6815, -0.6582,  0.7577, -0.6681,  0.7482,  0.9997,\n",
       "            0.6923,  0.9631,  0.9890, -0.8627, -0.9689, -0.0527,  0.8579,\n",
       "           -0.2685, -0.1472, -0.2996,  0.9263,  0.6983, -0.4653, -0.3143,\n",
       "            0.9899,  0.6321, -0.3711, -0.7314]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4258,  0.9998,  1.0000,  0.9368,  0.9617, -0.7291,  0.1935,\n",
       "            0.9693, -0.9987, -1.0000, -0.3181, -0.8864, -0.7568,  0.6547,\n",
       "            0.5449, -0.9935,  0.6470,  0.5967,  0.9991,  0.7702,  0.9540,\n",
       "            0.9648, -0.9897, -0.9924,  0.4638, -0.4869, -0.9455, -0.1136,\n",
       "           -0.8128,  0.2289,  0.9873, -0.9997, -0.8909, -0.5826, -0.3176,\n",
       "           -0.8532, -0.7979,  0.9400, -0.4247, -0.9970, -0.7615, -0.9939,\n",
       "           -1.0000,  0.0113, -0.5438,  0.4515,  0.9885,  0.9341,  0.9784,\n",
       "            0.9998, -0.2598, -0.9990, -0.0923, -0.8769, -0.9682, -0.7726,\n",
       "           -0.9902,  0.9897,  0.9996,  0.8033,  0.4688,  0.9967, -0.8985,\n",
       "           -0.6168,  0.9498, -0.3281, -0.9974,  0.9923,  0.3005,  0.0025,\n",
       "            0.6269,  0.9961, -0.9952,  0.9980, -0.9434, -0.9331,  0.9881,\n",
       "            0.7309, -0.9200, -0.3772,  0.8963,  0.9458, -0.2894,  0.9764,\n",
       "            0.1217,  0.9497, -0.4733, -0.9821, -0.0607, -0.8083,  0.9033,\n",
       "           -0.4469, -0.9888,  0.4062, -0.9225, -0.9747,  0.8747, -0.1679,\n",
       "           -0.1043, -0.9885,  0.9592,  0.4613, -0.9967,  0.5576,  0.8552,\n",
       "           -0.1306, -0.7993,  0.8302, -0.0250, -0.8672, -0.4576, -0.4464,\n",
       "            0.9999,  0.9280, -0.1748,  0.9152, -0.9231,  0.6988,  0.2759,\n",
       "           -0.6458, -0.7992,  0.8976,  0.0745,  0.7697,  0.4140,  0.4714,\n",
       "            0.9468,  0.9972,  0.8194, -0.8874, -0.9802,  0.7712, -0.9676,\n",
       "           -0.9988,  0.8974,  0.8872,  0.0668, -0.1708,  0.7696,  0.1084,\n",
       "            0.8968, -0.9873,  0.9990, -0.8071, -0.3376,  0.3242, -0.7338,\n",
       "            0.2620,  0.5765, -0.9467, -0.9995, -0.5584, -0.7838, -0.7117,\n",
       "            0.9763,  0.9996, -0.9999,  0.1697, -0.5731, -0.5845,  0.6418,\n",
       "           -0.9943, -0.7755, -0.2642,  0.4699,  0.7603, -0.9852,  0.6271,\n",
       "            0.9902, -0.9991,  0.9989,  0.8943, -0.9523, -0.9896, -0.9535,\n",
       "            0.9728,  0.1752,  0.9993, -0.9523, -0.8502, -0.9206, -0.8831,\n",
       "            0.8946, -0.8348, -1.0000, -0.7850, -0.9833,  0.6009, -0.9314,\n",
       "           -0.9968, -0.4112, -0.9383, -0.9616, -0.6379, -0.5736,  0.5948,\n",
       "            0.9940, -0.9468, -0.9999,  0.9952, -0.9866, -0.3727,  0.7519,\n",
       "           -0.2196, -0.9372,  0.9974, -0.9933, -0.2987, -0.9896, -0.9236,\n",
       "            0.9331,  0.5493,  0.8553, -0.1998,  0.7417,  0.2677, -0.2857,\n",
       "           -0.9347,  0.9485,  0.6138,  0.9986, -0.5436,  0.0028,  0.8213,\n",
       "            0.1447,  0.9726, -0.3547, -0.8921, -0.9935,  0.3540, -0.6695,\n",
       "           -0.9982, -0.8224, -0.8782,  0.5599, -0.9347,  0.6711,  0.9994,\n",
       "            0.4631,  0.8870,  0.9970, -0.4657, -0.9892, -0.4408,  0.9112,\n",
       "           -0.2626,  0.3567, -0.2954,  0.9858, -0.7732,  0.5155, -0.2700,\n",
       "            0.7896,  0.6250,  0.9461, -0.7489]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.5310,  0.9995,  0.9999,  0.9369, -0.9847, -0.6548,  0.1878,\n",
       "            0.9695, -1.0000, -0.9917, -0.3081, -0.8890, -0.6973,  0.6785,\n",
       "            0.4703, -0.9935,  0.6512, -0.9801, -0.9296,  0.7663,  0.9995,\n",
       "            0.9638, -0.9908, -0.9917,  0.9998,  0.4084, -0.9691, -0.9735,\n",
       "            0.9389, -0.8830,  0.9461, -0.9990, -0.9303, -0.6077,  0.9924,\n",
       "            0.9998, -0.8194,  0.9406, -0.4399, -0.9796,  0.9214, -0.8571,\n",
       "            0.9638,  0.7042, -0.6743,  0.4305,  0.9886, -0.3807,  0.9992,\n",
       "            0.4417, -0.6412, -0.9981, -0.7508, -0.8625,  0.9986,  0.8756,\n",
       "           -0.9873,  0.8703,  1.0000,  0.8957,  0.8558,  0.9973, -0.8994,\n",
       "           -0.6172,  0.9562, -0.5998, -0.9967,  0.9990,  0.3023,  0.0074,\n",
       "            0.4553,  0.9940, -0.9317,  0.9980, -0.8280, -0.9333,  0.9800,\n",
       "            0.9973, -0.9189, -0.4779,  0.8573,  0.7681, -0.2187, -0.9151,\n",
       "            0.0840,  0.9083,  0.1102, -0.9845, -0.3965, -0.7763,  0.9247,\n",
       "           -0.4178,  0.9976,  0.7225, -1.0000, -0.9755, -0.5736, -0.1958,\n",
       "           -0.0987, -1.0000,  0.9665,  0.4203, -0.9861,  0.4736, -0.1349,\n",
       "           -0.1723, -0.5194,  0.7506,  0.0585,  0.9672, -0.4609, -0.4515,\n",
       "            0.9996, -0.9182, -0.7104,  0.9105,  0.9947,  0.7004,  0.4310,\n",
       "           -0.6488, -0.9999, -0.9182,  0.0246,  0.7696,  0.2594,  0.9943,\n",
       "            0.9386,  0.9993,  0.7298, -0.8939, -0.8042, -0.5993, -0.9984,\n",
       "            0.2318,  0.5637,  0.9947,  0.0568, -0.1882, -0.9883,  0.0357,\n",
       "            0.9726, -0.9915,  0.1642, -0.8079, -0.3246,  0.3245,  0.9977,\n",
       "            0.6139,  0.7032, -0.5314, -0.9798, -0.5589, -0.7798, -0.7024,\n",
       "            0.9760,  0.4132, -1.0000,  0.0092, -0.9465, -0.9999,  0.9785,\n",
       "           -0.9974, -0.7407, -0.2837,  0.4670,  0.7939,  0.8768,  0.5482,\n",
       "            0.8936,  0.9927, -0.9937,  0.0887, -0.9593, -0.9787, -0.9677,\n",
       "            0.9800,  0.1757,  0.9993, -0.9524,  0.9984, -0.9227, -0.8818,\n",
       "           -0.9352, -0.8285, -1.0000, -0.9369, -0.9919,  0.5722, -0.9013,\n",
       "           -0.8548,  0.9850,  0.0696, -0.9674, -0.7017, -0.3334,  0.2750,\n",
       "            0.9941, -0.9999, -1.0000, -0.9746, -0.9866, -0.7818,  0.8033,\n",
       "            0.8041, -0.9382,  0.9989, -0.7669, -0.2976, -0.9999, -0.9297,\n",
       "           -0.0031,  0.7492,  0.9152, -0.2144,  0.7438,  0.2671, -0.2701,\n",
       "           -0.9772,  0.9911,  0.8300,  0.9998, -0.7525, -0.9949,  0.8823,\n",
       "           -0.6012,  0.9721, -0.3390, -0.8959, -0.9522,  0.7193,  0.9985,\n",
       "            0.9033, -0.8077, -0.8159,  0.3902, -0.9941,  0.4024,  0.9990,\n",
       "            0.1674,  0.9000,  0.9980,  0.5319, -0.9892, -0.3798, -0.7638,\n",
       "           -0.2296,  0.3145, -0.2964,  1.0000, -0.0150, -0.2622, -0.2698,\n",
       "           -0.9377, -0.2996, -0.8035, -0.8524]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0670,  0.9995,  1.0000,  0.9371,  0.7840, -0.5655,  0.3858,\n",
       "            0.9627, -0.9999, -0.9997, -0.2503, -0.8701,  0.0098,  0.6555,\n",
       "            0.4861, -0.9964,  0.6455, -0.9037, -0.8089,  0.7701,  0.9566,\n",
       "            0.9594, -0.9900, -0.9909,  0.9999, -0.1014, -0.9771, -0.6891,\n",
       "           -0.0468,  0.3325,  0.9974, -0.9992, -0.9312, -0.4604, -0.4758,\n",
       "            1.0000, -0.6846,  0.9213, -0.3807, -0.9954,  0.9757, -0.8401,\n",
       "           -0.8756,  0.1028, -0.8331,  0.4314,  0.9894,  0.9634,  0.9974,\n",
       "            0.9598,  0.7439, -0.9978, -0.8611, -0.8619,  0.9031, -0.8172,\n",
       "           -0.9544,  0.9017,  0.9995,  0.7844,  0.8829,  0.9978, -0.8790,\n",
       "           -0.6168,  0.7323, -0.3793, -0.9966, -0.9767,  0.3022,  0.0300,\n",
       "            0.1731,  0.9899, -0.8710,  0.9972, -0.8942, -0.9434,  0.9782,\n",
       "            0.9953, -0.9201, -0.3621,  0.8402,  0.8754, -0.1622, -0.1155,\n",
       "            0.0767,  0.8182, -0.9464, -0.9845,  0.5107, -0.8684, -0.9967,\n",
       "           -0.7665,  0.0970,  0.0685,  0.9865, -0.5539, -0.3050, -0.1546,\n",
       "            0.1642, -0.9988,  0.9454,  0.3770,  0.9678, -0.4587,  0.6410,\n",
       "           -0.1817, -0.7049,  0.4176, -0.0118, -0.3599, -0.4673, -0.4627,\n",
       "            0.9991, -0.7224, -0.7898,  0.9116,  0.9744,  0.6458,  0.0944,\n",
       "           -0.6537,  0.2567,  0.8933,  0.0234,  0.7687, -0.9920,  0.7512,\n",
       "            0.7818,  0.9966,  0.3745, -0.8943, -0.7292, -0.7527, -0.8733,\n",
       "           -0.8019,  0.2392,  0.8216,  0.1904, -0.3421,  0.7974,  0.0350,\n",
       "            0.7658,  0.9995,  0.3991, -0.8055, -0.2665,  0.3241, -0.9349,\n",
       "           -0.0180,  0.9791, -0.9381, -0.1779, -0.5592, -0.7738, -0.6583,\n",
       "            0.3721, -0.1009, -1.0000,  0.6543, -0.1742,  0.9898,  0.9803,\n",
       "           -0.7159, -0.5531, -0.2451,  0.4651, -0.9135, -0.9737,  0.5612,\n",
       "            0.9805, -0.9860,  0.6378,  0.6022, -0.9663, -0.9828, -0.9738,\n",
       "            0.9643,  0.1762,  0.9989, -0.9524,  0.7375, -0.9241, -0.8722,\n",
       "            0.4927, -0.7930, -0.9999, -0.4072, -0.9827,  0.6363, -0.8464,\n",
       "           -0.9437,  0.6064, -0.8657, -0.9783, -0.7146, -0.2552,  0.3894,\n",
       "            0.9953, -0.5841, -1.0000, -0.9469, -0.9269, -0.7926,  0.5355,\n",
       "            0.8706, -0.9380, -0.7416, -0.3114, -0.2970, -0.2471, -0.9134,\n",
       "           -0.0506,  0.3021,  0.9156, -0.9099,  0.7076,  0.2669, -0.1693,\n",
       "           -0.9782,  0.8984,  0.4547, -0.6957, -0.6386,  0.1696,  0.8302,\n",
       "           -0.6081,  0.9422, -0.3508, -0.8229,  0.1479, -0.2031, -0.9950,\n",
       "           -0.4138, -0.8323, -0.7058,  0.7524, -0.8260,  0.6608,  0.9997,\n",
       "            0.7436,  0.9451,  0.9847, -0.8524, -0.9726, -0.3115,  0.9077,\n",
       "           -0.2326, -0.4566, -0.2979,  0.9144,  0.7297, -0.7583, -0.2595,\n",
       "            0.9750,  0.6620, -0.5257, -0.6996]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.3747,  0.9997,  1.0000,  0.9376,  0.9790, -0.8486,  0.2884,\n",
       "            0.9866, -0.9978, -1.0000, -0.2502, -0.8878, -0.6980,  0.7845,\n",
       "            0.4925, -0.9959,  0.6467,  0.5777,  0.9989,  0.8119,  0.9473,\n",
       "            0.9624, -0.9929, -0.9971,  0.0224, -0.4426, -0.9167, -0.2847,\n",
       "           -0.8693,  0.2751,  0.9922, -0.9999, -0.9188, -0.4845, -0.6339,\n",
       "           -0.9346, -0.8332,  0.9452, -0.2681, -0.9983, -0.4675, -0.9943,\n",
       "           -1.0000,  0.1019, -0.6879,  0.4765,  0.9913,  0.9341,  0.9697,\n",
       "            0.9998, -0.0752, -0.9994, -0.1714, -0.8735, -0.9652, -0.8270,\n",
       "           -0.9921,  0.9898,  0.9997,  0.7124,  0.6108,  0.9982, -0.8975,\n",
       "           -0.6421,  0.9210, -0.4195, -0.9985,  0.9910,  0.2087,  0.0483,\n",
       "            0.6369,  0.9975, -0.9966,  0.9978, -0.9401, -0.9520,  0.9827,\n",
       "            0.7238, -0.9204, -0.3738,  0.8428,  0.9707, -0.1371,  0.9646,\n",
       "            0.0892,  0.9117, -0.5297, -0.9852, -0.1587, -0.8257,  0.9097,\n",
       "           -0.1450, -0.9946,  0.3026, -0.8946, -0.9777,  0.8172, -0.4172,\n",
       "           -0.0361, -0.9675,  0.9591,  0.4701, -0.9964,  0.4314,  0.8814,\n",
       "           -0.1625, -0.8899,  0.8077, -0.1046, -0.9127, -0.4695, -0.3616,\n",
       "            0.9999,  0.9779, -0.2230,  0.9227, -0.9489,  0.5689,  0.1482,\n",
       "           -0.6584, -0.6855,  0.8718,  0.0221,  0.7385,  0.4434,  0.3466,\n",
       "            0.9386,  0.9967,  0.7608, -0.9137, -0.9915,  0.8137, -0.9781,\n",
       "           -0.9992,  0.9112,  0.9157,  0.1567, -0.1476,  0.7643,  0.3148,\n",
       "            0.8397, -0.9839,  0.9990, -0.8386, -0.2690,  0.3747, -0.8993,\n",
       "            0.2387,  0.6617, -0.9382, -0.9996, -0.5933, -0.8252, -0.7015,\n",
       "            0.9522,  0.9995, -0.9999,  0.1145, -0.1642, -0.2802,  0.6612,\n",
       "           -0.9965, -0.7580, -0.2628,  0.4500,  0.7776, -0.9873,  0.6509,\n",
       "            0.9905, -0.9997,  0.9998,  0.8944, -0.9534, -0.9847, -0.9513,\n",
       "            0.9721,  0.2207,  0.9994, -0.9559, -0.9341, -0.9242, -0.8722,\n",
       "            0.9326, -0.8651, -1.0000, -0.5885, -0.9813,  0.6496, -0.9212,\n",
       "           -0.9956, -0.2019, -0.9454, -0.9334, -0.7468, -0.5763,  0.7203,\n",
       "            0.9939, -0.9395, -0.9999,  0.9956, -0.9854, -0.4149,  0.6450,\n",
       "           -0.1282, -0.9492,  0.9955, -0.9931, -0.2969, -0.9830, -0.9421,\n",
       "            0.9552,  0.4836,  0.8499,  0.0608,  0.7024,  0.1274, -0.3036,\n",
       "           -0.9639,  0.9550,  0.5233,  0.9983, -0.5795,  0.3186,  0.8931,\n",
       "            0.5918,  0.9797, -0.3433, -0.8644, -0.9931,  0.1882, -0.8585,\n",
       "           -0.9986, -0.9085, -0.8774,  0.5852, -0.9594,  0.4563,  0.9995,\n",
       "            0.6408,  0.8783,  0.9949, -0.5407, -0.9916, -0.5324,  0.9241,\n",
       "           -0.2280,  0.3846, -0.2958,  0.9484, -0.8316,  0.5280, -0.2313,\n",
       "            0.8058,  0.6580,  0.9458, -0.6309]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-5.2167e-01,  9.9934e-01,  9.9993e-01,  9.3766e-01, -9.7240e-01,\n",
       "           -8.1000e-01,  2.4935e-01,  9.8667e-01, -1.0000e+00, -9.9160e-01,\n",
       "           -2.4249e-01, -8.9046e-01, -6.6514e-01,  7.9876e-01,  4.2216e-01,\n",
       "           -9.9588e-01,  6.4964e-01, -9.8331e-01, -9.2649e-01,  8.0647e-01,\n",
       "            9.9943e-01,  9.6191e-01, -9.9353e-01, -9.9661e-01,  9.9968e-01,\n",
       "            2.8393e-01, -9.4813e-01, -9.7469e-01,  9.2523e-01, -8.7892e-01,\n",
       "            9.4951e-01, -9.9933e-01, -8.8796e-01, -5.1955e-01,  9.8990e-01,\n",
       "            9.9982e-01, -8.5003e-01,  9.4587e-01, -2.9327e-01, -9.7710e-01,\n",
       "            8.8472e-01, -8.8943e-01,  9.4708e-01,  6.2087e-01, -7.6185e-01,\n",
       "            4.6116e-01,  9.9134e-01, -4.1572e-01,  9.9838e-01,  5.0257e-01,\n",
       "           -6.0235e-01, -9.9845e-01, -7.4275e-01, -8.5956e-01,  9.9872e-01,\n",
       "            8.4712e-01, -9.8927e-01,  9.0995e-01,  1.0000e+00,  8.2772e-01,\n",
       "            9.0119e-01,  9.9756e-01, -8.9887e-01, -6.4245e-01,  9.3047e-01,\n",
       "           -6.4745e-01, -9.9817e-01,  9.9970e-01,  2.1101e-01,  5.3247e-02,\n",
       "            4.9477e-01,  9.9600e-01, -9.5549e-01,  9.9786e-01, -7.7710e-01,\n",
       "           -9.5261e-01,  9.7909e-01,  9.9578e-01, -9.1992e-01, -4.6204e-01,\n",
       "            8.1729e-01,  7.3150e-01, -9.3205e-02, -9.0161e-01,  5.8137e-02,\n",
       "            8.7516e-01,  6.1918e-02, -9.8767e-01, -4.1429e-01, -7.9572e-01,\n",
       "            9.3927e-01, -1.2222e-01,  9.9680e-01,  7.8648e-01, -9.9999e-01,\n",
       "           -9.7834e-01, -5.3866e-01, -4.3040e-01, -3.5233e-02, -9.9999e-01,\n",
       "            9.6646e-01,  4.2608e-01, -9.9024e-01,  3.9078e-01, -1.9244e-01,\n",
       "           -2.0320e-01, -5.7767e-01,  7.6723e-01, -2.1503e-02,  9.4956e-01,\n",
       "           -4.7189e-01, -3.6695e-01,  9.9967e-01, -9.3511e-01, -6.8178e-01,\n",
       "            7.2462e-01,  9.9193e-01,  5.7467e-01,  2.4310e-01, -6.6146e-01,\n",
       "           -9.9992e-01, -8.9961e-01, -1.7692e-02,  7.3837e-01,  4.3533e-01,\n",
       "            9.9354e-01,  9.3259e-01,  9.9897e-01,  7.2634e-01, -9.1885e-01,\n",
       "           -7.7468e-01, -4.1550e-01, -9.9903e-01, -2.4088e-01,  5.1781e-01,\n",
       "            9.9544e-01,  1.4258e-01, -1.6353e-01, -9.8457e-01,  2.3308e-01,\n",
       "            9.5206e-01, -9.8738e-01,  3.4956e-01, -8.3965e-01, -2.5725e-01,\n",
       "            3.7476e-01,  9.9782e-01,  5.0822e-01,  7.3873e-01, -5.3810e-01,\n",
       "           -9.9247e-01, -5.9384e-01, -8.1716e-01, -7.0510e-01,  9.5926e-01,\n",
       "            5.6878e-01, -9.9999e-01,  8.8845e-03, -9.3735e-01, -9.9987e-01,\n",
       "            9.6396e-01, -9.9794e-01, -7.4359e-01, -2.7855e-01,  4.4662e-01,\n",
       "            8.0779e-01,  8.1611e-01,  5.9556e-01,  8.8710e-01,  9.9076e-01,\n",
       "           -9.9417e-01,  1.1463e-01, -9.6062e-01, -9.7763e-01, -9.6766e-01,\n",
       "            9.7759e-01,  2.2132e-01,  9.9945e-01, -9.5600e-01,  9.9773e-01,\n",
       "           -9.2577e-01, -8.7173e-01, -9.3130e-01, -8.5849e-01, -9.9997e-01,\n",
       "           -8.3061e-01, -9.8927e-01,  6.2618e-01, -8.8826e-01, -8.8622e-01,\n",
       "            9.8632e-01, -8.9859e-04, -9.4363e-01, -7.8383e-01, -4.2372e-01,\n",
       "            3.1050e-01,  9.9393e-01, -9.9980e-01, -9.9995e-01, -9.8137e-01,\n",
       "           -9.8559e-01, -6.9875e-01,  7.0619e-01,  7.9672e-01, -9.4988e-01,\n",
       "            9.9931e-01, -8.3175e-01, -2.9625e-01, -9.9992e-01, -9.4569e-01,\n",
       "           -1.0382e-01,  7.5405e-01,  9.1122e-01,  3.9776e-02,  7.0438e-01,\n",
       "            1.2686e-01, -2.8655e-01, -9.8044e-01,  9.9182e-01,  8.1914e-01,\n",
       "            9.9982e-01, -7.0177e-01, -9.9655e-01,  9.2472e-01, -6.4618e-01,\n",
       "            9.7965e-01, -3.3317e-01, -8.6883e-01, -9.6861e-01,  6.2694e-01,\n",
       "            9.9630e-01,  8.9758e-01, -8.9642e-01, -8.0613e-01,  4.1783e-01,\n",
       "           -9.9673e-01,  2.4359e-01,  9.9891e-01,  2.5078e-01,  8.9745e-01,\n",
       "            9.9641e-01,  3.5954e-01, -9.9155e-01, -4.5567e-01, -7.7609e-01,\n",
       "           -1.8493e-01,  3.5517e-01, -2.9690e-01,  9.9993e-01, -1.0822e-01,\n",
       "           -1.9457e-01, -2.3357e-01, -9.5512e-01, -5.1427e-02, -7.2609e-01,\n",
       "           -8.1505e-01]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.1085,  0.9994,  1.0000,  0.9379,  0.6478, -0.7408,  0.3543,\n",
       "            0.9814, -0.9999, -0.9998, -0.1831, -0.8788,  0.1693,  0.7776,\n",
       "            0.4372, -0.9960,  0.6438, -0.9236, -0.8151,  0.8103,  0.9683,\n",
       "            0.9581, -0.9931, -0.9959,  0.9998, -0.0861, -0.9722, -0.7572,\n",
       "            0.1382,  0.0491,  0.9960, -0.9995, -0.8902, -0.4039, -0.3616,\n",
       "            1.0000, -0.6982,  0.9298, -0.2441, -0.9940,  0.9729, -0.7529,\n",
       "           -0.6830,  0.0099, -0.8733,  0.4563,  0.9920,  0.9686,  0.9968,\n",
       "            0.9643,  0.6687, -0.9982, -0.8497, -0.8585,  0.8902, -0.7997,\n",
       "           -0.9576,  0.9322,  0.9998,  0.7954,  0.9076,  0.9979, -0.8810,\n",
       "           -0.6419,  0.7131, -0.3906, -0.9980, -0.9551,  0.2109,  0.0730,\n",
       "            0.1080,  0.9900, -0.9223,  0.9971, -0.8731, -0.9524,  0.9793,\n",
       "            0.9954, -0.9206, -0.4216,  0.7994,  0.8395, -0.2289,  0.0417,\n",
       "            0.0509,  0.8128, -0.9373, -0.9876,  0.6306, -0.8617, -0.9978,\n",
       "           -0.7060,  0.2174,  0.1022,  0.9726, -0.6274, -0.2834, -0.3737,\n",
       "            0.2009, -0.9984,  0.9458,  0.3843,  0.9727, -0.3596,  0.5543,\n",
       "           -0.1744, -0.5802,  0.5768, -0.0708, -0.1915, -0.4765, -0.3791,\n",
       "            0.9990, -0.7996, -0.6892,  0.7302,  0.9852,  0.5507,  0.3384,\n",
       "           -0.6651, -0.1285,  0.8923, -0.0194,  0.7376, -0.9854,  0.8304,\n",
       "            0.7521,  0.9975,  0.3142, -0.9164, -0.6409, -0.6889, -0.9258,\n",
       "           -0.7579, -0.0224,  0.8737,  0.2321, -0.2897,  0.7698,  0.2323,\n",
       "            0.7936,  0.9990,  0.1626, -0.8369, -0.2207,  0.3743, -0.9482,\n",
       "            0.1490,  0.9806, -0.8503, -0.3058, -0.5941, -0.8045, -0.6813,\n",
       "            0.4172,  0.1142, -1.0000,  0.5541, -0.0661,  0.9939,  0.9691,\n",
       "           -0.7554, -0.5706, -0.2503,  0.4444, -0.8850, -0.9595,  0.6037,\n",
       "            0.9797, -0.9842,  0.6435,  0.6973, -0.9666, -0.9816, -0.9725,\n",
       "            0.9686,  0.2217,  0.9992, -0.9560,  0.6649, -0.9269, -0.8480,\n",
       "            0.2849, -0.8184, -0.9999, -0.4489, -0.9862,  0.6656, -0.8099,\n",
       "           -0.9089,  0.5972, -0.8022, -0.9730, -0.7664, -0.3613,  0.4336,\n",
       "            0.9949, -0.7932, -0.9999, -0.9435, -0.9389, -0.7185,  0.4772,\n",
       "            0.8750, -0.9499, -0.7086, -0.3048, -0.2958, -0.0692, -0.9304,\n",
       "           -0.1140,  0.4237,  0.9119, -0.9288,  0.6806,  0.1262, -0.1965,\n",
       "           -0.9813,  0.9338,  0.2519, -0.5451, -0.6205,  0.1241,  0.6813,\n",
       "           -0.6541,  0.9510, -0.3466, -0.8173,  0.0157, -0.3529, -0.9943,\n",
       "           -0.4034, -0.9084, -0.7035,  0.7613, -0.8737,  0.6287,  0.9997,\n",
       "            0.7236,  0.9394,  0.9851, -0.8100, -0.9686, -0.3837,  0.9058,\n",
       "           -0.1883, -0.4690, -0.2982,  0.9143,  0.7302, -0.8151, -0.2236,\n",
       "            0.9573,  0.7117, -0.4361, -0.6176]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4075,  0.9995,  1.0000,  0.9383,  0.9812, -0.8597,  0.1372,\n",
       "            0.9926, -0.9979, -1.0000, -0.1830, -0.8709, -0.8169,  0.8396,\n",
       "            0.4513, -0.9947,  0.6430,  0.5388,  0.9980,  0.8384,  0.9514,\n",
       "            0.9565, -0.9946, -0.9983, -0.1242, -0.4016, -0.8778, -0.4175,\n",
       "           -0.9102, -0.0060,  0.9947, -0.9999, -0.8831, -0.4393, -0.7279,\n",
       "           -0.9585, -0.8016,  0.9484, -0.1001, -0.9980, -0.5235, -0.9904,\n",
       "           -0.9999,  0.0091, -0.7366,  0.5084,  0.9927,  0.9506,  0.9282,\n",
       "            0.9997,  0.0225, -0.9993, -0.0127, -0.8672, -0.9680, -0.8129,\n",
       "           -0.9863,  0.9917,  0.9996,  0.7008,  0.5205,  0.9982, -0.8772,\n",
       "           -0.6605,  0.9284, -0.3636, -0.9990,  0.9862,  0.1235,  0.0897,\n",
       "            0.5629,  0.9976, -0.9974,  0.9976, -0.9491, -0.9609,  0.9826,\n",
       "            0.7779, -0.9208, -0.4235,  0.8009,  0.9621, -0.1605,  0.9508,\n",
       "            0.0642,  0.8965, -0.6300, -0.9781, -0.2101, -0.7897,  0.8714,\n",
       "            0.2368, -0.9954,  0.2754, -0.7454, -0.9773,  0.7561, -0.4845,\n",
       "            0.0251, -0.9482,  0.9444,  0.4854, -0.9956,  0.4671,  0.8868,\n",
       "           -0.1516, -0.8389,  0.8487, -0.3071, -0.9430, -0.4789, -0.2941,\n",
       "            0.9997,  0.9852, -0.1617,  0.7596, -0.9396,  0.5585,  0.3711,\n",
       "           -0.6628, -0.4777,  0.8749, -0.0191,  0.7099,  0.6309,  0.4250,\n",
       "            0.9228,  0.9976,  0.7275, -0.9059, -0.9896,  0.7117, -0.9783,\n",
       "           -0.9988,  0.8875,  0.9352,  0.1920, -0.0441,  0.7144,  0.3303,\n",
       "            0.8434, -0.9696,  0.9983, -0.8596, -0.2234,  0.4208, -0.8896,\n",
       "            0.3952,  0.7570, -0.8510, -0.9996, -0.6222, -0.8009, -0.7065,\n",
       "            0.9460,  0.9994, -0.9999,  0.1418, -0.0516, -0.2583,  0.6294,\n",
       "           -0.9942, -0.6761, -0.2670,  0.4249,  0.7215, -0.9828,  0.6814,\n",
       "            0.9922, -0.9998,  0.9998,  0.8947, -0.9383, -0.9870, -0.9467,\n",
       "            0.9522,  0.2531,  0.9994, -0.9598, -0.9464, -0.9271, -0.8480,\n",
       "            0.9373, -0.8605, -1.0000, -0.6228, -0.9848,  0.6753, -0.8983,\n",
       "           -0.9943, -0.0736, -0.9350, -0.9447, -0.7894, -0.5993,  0.7130,\n",
       "            0.9928, -0.9587, -0.9998,  0.9924, -0.9862, -0.4517,  0.5507,\n",
       "           -0.0943, -0.9583,  0.9912, -0.9908, -0.2958, -0.9643, -0.9528,\n",
       "            0.9555,  0.5074,  0.8564,  0.0102,  0.6740, -0.0816, -0.3179,\n",
       "           -0.9834,  0.9580,  0.3278,  0.9985, -0.5288,  0.4071,  0.7854,\n",
       "            0.7167,  0.9802, -0.3401, -0.8424, -0.9949, -0.0274, -0.9394,\n",
       "           -0.9978, -0.9509, -0.8668,  0.5446, -0.9629,  0.4525,  0.9989,\n",
       "            0.6384,  0.7949,  0.9922, -0.5196, -0.9885, -0.6128,  0.9342,\n",
       "           -0.1846,  0.5122, -0.2960,  0.8858, -0.8668,  0.7197, -0.1868,\n",
       "            0.6935,  0.7074,  0.8907, -0.5961]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.4501,  0.9993,  0.9999,  0.9383, -0.9631, -0.7949,  0.1016,\n",
       "            0.9926, -1.0000, -0.9875, -0.1788, -0.8736, -0.6194,  0.8509,\n",
       "            0.3707, -0.9947,  0.6460, -0.9849, -0.9229,  0.8317,  0.9992,\n",
       "            0.9561, -0.9949, -0.9981,  0.9996,  0.3270, -0.9254, -0.9771,\n",
       "            0.8734, -0.9077,  0.9538, -0.9991, -0.8511, -0.4863,  0.9918,\n",
       "            0.9998, -0.8238,  0.9490, -0.1324, -0.9777,  0.6952, -0.9095,\n",
       "            0.9281,  0.6388, -0.8017,  0.4723,  0.9927, -0.3769,  0.9945,\n",
       "            0.3020, -0.4742, -0.9987, -0.6658, -0.8530,  0.9987,  0.8513,\n",
       "           -0.9834,  0.9443,  1.0000,  0.8119,  0.8681,  0.9982, -0.8800,\n",
       "           -0.6609,  0.9373, -0.6597, -0.9987,  0.9997,  0.1267,  0.0941,\n",
       "            0.4530,  0.9966, -0.9584,  0.9976, -0.7699, -0.9595,  0.9807,\n",
       "            0.9953, -0.9205, -0.5225,  0.7726,  0.7114, -0.1223, -0.9220,\n",
       "            0.0180,  0.8679, -0.0367, -0.9813, -0.4786, -0.7403,  0.9175,\n",
       "            0.2598,  0.9963,  0.7359, -1.0000, -0.9781, -0.5597, -0.4899,\n",
       "            0.0244, -1.0000,  0.9556,  0.4328, -0.9927,  0.4312, -0.2517,\n",
       "           -0.1941, -0.6141,  0.7982, -0.1422,  0.9368, -0.4808, -0.2996,\n",
       "            0.9996, -0.9300, -0.6725,  0.4306,  0.9838,  0.5683,  0.4200,\n",
       "           -0.6656, -0.9999, -0.9148, -0.0651,  0.7098,  0.1813,  0.9933,\n",
       "            0.9167,  0.9982,  0.7261, -0.9118, -0.8232, -0.1457, -0.9987,\n",
       "           -0.4898,  0.5918,  0.9959,  0.1650, -0.0611, -0.9865,  0.2281,\n",
       "            0.9386, -0.9764,  0.2839, -0.8608, -0.2119,  0.4208,  0.9988,\n",
       "            0.6343,  0.8072, -0.4962, -0.9926, -0.6228, -0.7945, -0.7123,\n",
       "            0.9513,  0.5715, -1.0000,  0.0686, -0.9077, -0.9999,  0.9045,\n",
       "           -0.9975, -0.7049, -0.2849,  0.4219,  0.7579,  0.6694,  0.6297,\n",
       "            0.8876,  0.9878, -0.9956, -0.0112, -0.9506, -0.9831, -0.9700,\n",
       "            0.9807,  0.2537,  0.9994, -0.9599,  0.9981, -0.9282, -0.8476,\n",
       "           -0.9390, -0.8606, -1.0000, -0.8136, -0.9896,  0.6578, -0.8652,\n",
       "           -0.9348,  0.9841, -0.2949, -0.9525, -0.8157, -0.4843,  0.4046,\n",
       "            0.9928, -0.9998, -0.9999, -0.9819, -0.9865, -0.5758,  0.6310,\n",
       "            0.7783, -0.9589,  0.9992, -0.8441, -0.2951, -0.9999, -0.9558,\n",
       "           -0.1858,  0.7895,  0.9013, -0.0028,  0.6762, -0.0822, -0.3001,\n",
       "           -0.9780,  0.9932,  0.7228,  0.9997, -0.6655, -0.9962,  0.8711,\n",
       "           -0.6135,  0.9802, -0.3303, -0.8468, -0.9709,  0.4362,  0.9953,\n",
       "            0.9031, -0.9217, -0.8047,  0.3657, -0.9955,  0.2904,  0.9982,\n",
       "            0.1758,  0.8351,  0.9948,  0.3932, -0.9886, -0.5376, -0.8040,\n",
       "           -0.1505,  0.4886, -0.2974,  0.9998, -0.2587, -0.1240, -0.1926,\n",
       "           -0.9642,  0.1749, -0.7174, -0.7966]]], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([\"za\"])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retry\n",
    "uz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bz = model.enc_lstm.wz_bias.data.numpy() # GOOD\n",
    "\n",
    "ur = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "br = model.enc_lstm.wr_bias.data.numpy()\n",
    "\n",
    "ux = model.enc_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wx = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bx = model.enc_lstm.wx_bias.data.numpy() + model.enc_lstm.wrh_bias.data.numpy() # GOOD\n",
    "\n",
    "\n",
    "full_w = flatten(np.concatenate([wz.transpose(), wr.transpose(), wx.transpose()], axis=1).tolist())\n",
    "full_x = flatten(np.concatenate([uz, ur, ux], axis=0).transpose().tolist()) # CORRECT\n",
    "full_b = flatten(np.concatenate([np.expand_dims(bz,1),np.expand_dims(br,1),np.expand_dims(bx,1)], axis=0).tolist()) # CORRECT\n",
    "\n",
    "# No internal transpose:\n",
    "# 0, no transpose: wrong\n",
    "# 1, no transpose: right first thing, but not rest\n",
    "# 0, transpose: wrong\n",
    "# 1, transpose: wrong\n",
    "\n",
    "# Internal transpose:\n",
    "# 0, no transpose: wrong\n",
    "# 1, no transpose: wrong\n",
    "# 0, transpose: right first thing, but not rest\n",
    "# 1, transpose: wrong\n",
    "\n",
    "\n",
    "# Retry\n",
    "uzd = model.dec_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wzd = model.dec_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bzd = model.dec_lstm.wz_bias.data.numpy() # GOOD\n",
    "\n",
    "urd = model.dec_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wrd = model.dec_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "brd = model.dec_lstm.wr_bias.data.numpy()\n",
    "\n",
    "uxd = model.dec_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wxd = model.dec_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bxd = model.dec_lstm.wx_bias.data.numpy() + model.dec_lstm.wrh_bias.data.numpy() # GOOD\n",
    "\n",
    "\n",
    "full_wd = flatten(np.concatenate([wzd.transpose(), wrd.transpose(), wxd.transpose()], axis=1).tolist())\n",
    "full_xd = flatten(np.concatenate([uzd, urd, uxd], axis=0).transpose().tolist()) # CORRECT\n",
    "full_bd = flatten(np.concatenate([np.expand_dims(bzd,1),np.expand_dims(brd,1),np.expand_dims(bxd,1)], axis=0).tolist()) # CORRECT\n",
    "\n",
    "\n",
    "\n",
    "tf_weights = open(\"tf_weights.js\", \"w\")\n",
    "tf_weights.write(\"emb_wg = \" + stringify_lst(flatten(model.embedding.weights.data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"full_x = \" + stringify_lst(full_x) + \";\\n\")\n",
    "tf_weights.write(\"full_w = \" + stringify_lst(full_w) + \";\\n\")\n",
    "tf_weights.write(\"full_b = \" + stringify_lst(full_b) + \";\\n\")\n",
    "tf_weights.write(\"full_xd = \" + stringify_lst(full_xd) + \";\\n\")\n",
    "tf_weights.write(\"full_wd = \" + stringify_lst(full_wd) + \";\\n\")\n",
    "tf_weights.write(\"full_bd = \" + stringify_lst(full_bd) + \";\\n\")\n",
    "tf_weights.write(\"out_wg = \" + stringify_lst(flatten(model.dec_output.weights.transpose(0,1).data.numpy().tolist())) + \";\\n\")\n",
    "tf_weights.write(\"out_wb = \" + stringify_lst(model.dec_output.bias.data.numpy().tolist()) + \";\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-3.1345e+01, -3.1722e+01, -1.0695e+01, -1.1993e+01, -1.9148e+01,\n",
       "           -1.8033e+01, -1.8337e+01, -2.0455e+01, -1.7708e+01, -1.9106e+01,\n",
       "           -1.7292e+01, -2.0588e+01, -2.1188e+01, -2.1301e+01, -1.7761e+01,\n",
       "           -1.7994e+01, -2.0617e+01, -2.2182e+01, -2.0595e+01, -1.8950e+01,\n",
       "           -2.4255e+01, -2.0938e+01, -1.6749e+01, -2.1373e+01, -1.9037e+01,\n",
       "           -1.9733e+01, -2.0761e+01, -1.6086e+01, -1.9191e+01, -1.6779e+01,\n",
       "           -2.0447e+01, -2.0326e+01, -8.7156e+00, -1.9322e-04]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.5725e+01, -2.5668e+01, -1.1383e+01, -9.6359e+00, -1.6666e+01,\n",
       "           -1.5934e+01, -1.6515e+01, -1.7916e+01, -1.5906e+01, -1.6670e+01,\n",
       "           -1.5977e+01, -1.8716e+01, -1.9436e+01, -1.3169e+01, -9.7857e+00,\n",
       "           -9.6532e+00, -1.2284e+01, -1.4586e+01, -1.2750e+01, -1.0180e+01,\n",
       "           -1.6266e+01, -1.3031e+01, -9.9095e+00, -1.2121e+01, -1.0848e+01,\n",
       "           -1.1269e+01, -1.2470e+01, -7.2635e+00, -1.1487e+01, -8.2810e+00,\n",
       "           -1.1844e+01, -1.3061e+01, -1.3135e-03, -1.5480e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-2.7539e+01, -2.7359e+01, -1.3171e+01, -1.2450e-03, -1.0007e+01,\n",
       "           -7.3815e+00, -1.1806e+01, -1.0662e+01, -7.9450e+00, -1.0501e+01,\n",
       "           -8.7805e+00, -1.2097e+01, -1.4013e+01, -2.1597e+01, -1.8079e+01,\n",
       "           -1.9968e+01, -1.7478e+01, -2.0429e+01, -1.7876e+01, -1.7750e+01,\n",
       "           -2.0317e+01, -1.9816e+01, -1.9489e+01, -2.0251e+01, -1.9857e+01,\n",
       "           -2.2711e+01, -2.1191e+01, -1.8970e+01, -1.9247e+01, -1.7932e+01,\n",
       "           -1.9929e+01, -2.0714e+01, -1.4020e+01, -1.4188e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.6930e+01, -3.7169e+01, -1.1793e+01, -1.2579e+01, -1.9547e+01,\n",
       "           -1.7417e+01, -1.9473e+01, -1.7523e+01, -1.6554e+01, -1.8687e+01,\n",
       "           -1.9719e+01, -1.8384e+01, -2.1955e+01, -2.1350e+01, -1.8604e+01,\n",
       "           -2.0297e+01, -1.7707e+01, -1.9250e+01, -1.6592e+01, -1.7618e+01,\n",
       "           -2.1312e+01, -1.8926e+01, -1.8808e+01, -2.1165e+01, -2.0917e+01,\n",
       "           -1.9879e+01, -2.1552e+01, -1.9716e+01, -1.8407e+01, -1.8395e+01,\n",
       "           -1.9266e+01, -1.9455e+01, -1.4802e+01, -1.1563e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.5782e+01, -3.5732e+01, -1.2994e-05, -1.1853e+01, -1.8670e+01,\n",
       "           -1.6327e+01, -1.8053e+01, -1.6042e+01, -1.5185e+01, -1.8454e+01,\n",
       "           -1.8694e+01, -1.7307e+01, -2.1504e+01, -1.9561e+01, -1.8438e+01,\n",
       "           -1.9011e+01, -1.7082e+01, -1.6859e+01, -1.4549e+01, -1.6109e+01,\n",
       "           -1.9427e+01, -1.8110e+01, -1.6620e+01, -1.8749e+01, -1.8155e+01,\n",
       "           -1.8270e+01, -2.0249e+01, -1.7553e+01, -1.5800e+01, -1.6875e+01,\n",
       "           -1.6410e+01, -1.7252e+01, -1.2786e+01, -1.3331e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-24.3661, -23.9930,  -1.4579,  -0.6249,  -4.1032,  -5.0865,  -5.8230,\n",
       "            -3.0418,  -2.8759,  -3.6333,  -3.9914,  -3.7754,  -7.4964, -16.3347,\n",
       "           -15.8858, -16.4654, -13.8869, -12.7507, -11.3317, -12.0589, -16.2618,\n",
       "           -11.1733, -12.5542, -15.8165, -16.8890, -16.9239, -19.3127, -14.0017,\n",
       "            -9.7758, -12.8463, -12.4979, -11.3867, -12.0453,  -3.3905]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.4134e+01, -3.3882e+01, -8.2424e+00, -1.4594e+01, -1.7959e+01,\n",
       "           -1.6240e+01, -1.5598e+01, -1.3560e+01, -1.5279e+01, -1.5501e+01,\n",
       "           -1.6300e+01, -1.6242e+01, -1.8020e+01, -1.9077e+01, -1.8157e+01,\n",
       "           -2.0395e+01, -1.4706e+01, -1.5307e+01, -1.7804e+01, -1.8199e+01,\n",
       "           -1.9154e+01, -1.3984e+01, -1.6321e+01, -1.8892e+01, -2.1487e+01,\n",
       "           -1.9721e+01, -2.0400e+01, -1.8125e+01, -1.5668e+01, -1.6707e+01,\n",
       "           -1.5201e+01, -1.7834e+01, -1.6558e+01, -2.6807e-04]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-4.0146e+01, -3.9918e+01, -3.2186e-06, -1.8445e+01, -2.1928e+01,\n",
       "           -1.9258e+01, -1.7150e+01, -1.5325e+01, -1.9926e+01, -1.8770e+01,\n",
       "           -1.9762e+01, -1.9232e+01, -2.1285e+01, -2.0067e+01, -1.9772e+01,\n",
       "           -2.1334e+01, -1.5898e+01, -1.4581e+01, -1.8309e+01, -1.9338e+01,\n",
       "           -1.8896e+01, -1.5356e+01, -1.6927e+01, -1.9273e+01, -2.0917e+01,\n",
       "           -2.0229e+01, -2.0418e+01, -1.9207e+01, -1.6783e+01, -1.6870e+01,\n",
       "           -1.6603e+01, -1.8403e+01, -1.6014e+01, -1.3195e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-25.7588, -25.3770,  -3.2394,  -4.5511,  -5.8946,  -5.8282,  -2.6741,\n",
       "            -0.2792,  -4.3331,  -2.6797,  -4.6914,  -3.8995,  -5.4631, -14.8063,\n",
       "           -17.9804, -16.1081, -13.6498, -12.5189, -12.8905, -12.9881, -15.4813,\n",
       "           -11.5525, -13.7918, -14.4346, -17.8851, -16.2256, -17.6373, -14.1918,\n",
       "           -13.2143, -12.8291, -11.8239, -11.7625, -14.2304,  -5.5825]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.5698e+01, -3.5472e+01, -1.0887e+01, -1.6516e+01, -1.9780e+01,\n",
       "           -1.7074e+01, -1.5452e+01, -1.3535e+01, -1.6344e+01, -1.6324e+01,\n",
       "           -1.6663e+01, -1.6968e+01, -1.8488e+01, -1.8574e+01, -2.1287e+01,\n",
       "           -2.1598e+01, -1.6370e+01, -1.6351e+01, -1.9060e+01, -1.9894e+01,\n",
       "           -2.0394e+01, -1.4764e+01, -1.8128e+01, -1.9520e+01, -2.3130e+01,\n",
       "           -2.0149e+01, -2.1135e+01, -2.0451e+01, -1.8564e+01, -1.7994e+01,\n",
       "           -1.5257e+01, -1.8511e+01, -2.0355e+01, -2.1338e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.9022e+01, -3.8811e+01, -1.7166e-05, -1.7837e+01, -2.1481e+01,\n",
       "           -1.8242e+01, -1.6001e+01, -1.4412e+01, -1.8680e+01, -1.7512e+01,\n",
       "           -1.8894e+01, -1.7728e+01, -2.0759e+01, -1.9036e+01, -1.9900e+01,\n",
       "           -2.1008e+01, -1.6600e+01, -1.4524e+01, -1.7888e+01, -1.9668e+01,\n",
       "           -1.8503e+01, -1.5610e+01, -1.7166e+01, -1.8443e+01, -2.0223e+01,\n",
       "           -1.9897e+01, -1.9869e+01, -1.9502e+01, -1.6960e+01, -1.7418e+01,\n",
       "           -1.5942e+01, -1.7195e+01, -1.7200e+01, -1.1079e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-24.3236, -24.1299,  -1.7928,  -5.2477,  -6.6392,  -5.4703,  -2.9471,\n",
       "            -0.4436,  -4.9014,  -2.9353,  -4.3820,  -3.4449,  -5.8112, -13.0575,\n",
       "           -15.0847, -14.7030, -12.1746, -10.0769, -10.6523, -11.4951, -13.4235,\n",
       "            -9.4721, -11.5878, -13.4197, -14.7009, -14.5753, -15.3534, -12.7920,\n",
       "           -10.8379, -11.8817, -10.8086, -10.2622, -12.6873,  -3.8967]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.6323e+01, -3.6131e+01, -9.8250e+00, -1.8067e+01, -2.1717e+01,\n",
       "           -1.8143e+01, -1.6682e+01, -1.4902e+01, -1.8018e+01, -1.7114e+01,\n",
       "           -1.7264e+01, -1.7906e+01, -2.0010e+01, -1.9181e+01, -2.0327e+01,\n",
       "           -2.2275e+01, -1.7018e+01, -1.6091e+01, -1.9427e+01, -2.0532e+01,\n",
       "           -2.0530e+01, -1.4432e+01, -1.7964e+01, -2.0908e+01, -2.2692e+01,\n",
       "           -2.0746e+01, -2.1376e+01, -2.0804e+01, -1.8005e+01, -1.8861e+01,\n",
       "           -1.6846e+01, -1.8981e+01, -2.0605e+01, -5.5312e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.8055e+01, -3.7892e+01, -1.4424e-05, -1.8481e+01, -2.2188e+01,\n",
       "           -1.8687e+01, -1.6514e+01, -1.5038e+01, -1.9043e+01, -1.7957e+01,\n",
       "           -1.8596e+01, -1.8275e+01, -2.1488e+01, -1.9476e+01, -1.9056e+01,\n",
       "           -2.1363e+01, -1.6532e+01, -1.3914e+01, -1.7929e+01, -1.9797e+01,\n",
       "           -1.8293e+01, -1.4961e+01, -1.6697e+01, -1.9311e+01, -1.9484e+01,\n",
       "           -2.0236e+01, -1.9681e+01, -1.9491e+01, -1.6795e+01, -1.7889e+01,\n",
       "           -1.6632e+01, -1.7314e+01, -1.7301e+01, -1.1302e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-24.1390, -23.8396,  -1.6549,  -5.2734,  -6.5983,  -5.5330,  -2.7816,\n",
       "            -0.5531,  -4.5634,  -2.5817,  -3.5425,  -3.3009,  -6.1358, -13.5785,\n",
       "           -14.9882, -14.9893, -12.9701, -10.2612, -10.8443, -11.9153, -13.9145,\n",
       "            -9.7761, -11.7856, -14.0405, -14.3617, -15.3076, -15.6700, -13.1910,\n",
       "           -10.9935, -12.8214, -11.5995, -10.2976, -13.0771,  -4.9437]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.5821e+01, -3.5660e+01, -1.0381e+01, -1.8229e+01, -2.1758e+01,\n",
       "           -1.8498e+01, -1.6885e+01, -1.4704e+01, -1.7950e+01, -1.6914e+01,\n",
       "           -1.7195e+01, -1.8053e+01, -2.0336e+01, -1.9508e+01, -2.0482e+01,\n",
       "           -2.2491e+01, -1.7675e+01, -1.6095e+01, -1.9494e+01, -2.0731e+01,\n",
       "           -2.0808e+01, -1.4692e+01, -1.8248e+01, -2.1067e+01, -2.2743e+01,\n",
       "           -2.0842e+01, -2.1622e+01, -2.0976e+01, -1.7932e+01, -1.9565e+01,\n",
       "           -1.6930e+01, -1.8912e+01, -2.1359e+01, -3.2305e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.7278e+01, -3.7117e+01, -2.5868e-05, -1.8051e+01, -2.1908e+01,\n",
       "           -1.8271e+01, -1.6138e+01, -1.4583e+01, -1.8471e+01, -1.7386e+01,\n",
       "           -1.8214e+01, -1.7997e+01, -2.1271e+01, -1.9537e+01, -1.8960e+01,\n",
       "           -2.1307e+01, -1.6971e+01, -1.3898e+01, -1.7988e+01, -1.9767e+01,\n",
       "           -1.8341e+01, -1.5111e+01, -1.6618e+01, -1.9212e+01, -1.9236e+01,\n",
       "           -2.0067e+01, -1.9638e+01, -1.9325e+01, -1.6625e+01, -1.8208e+01,\n",
       "           -1.6662e+01, -1.7005e+01, -1.7555e+01, -1.0653e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-23.5209, -23.2520,  -1.5674,  -5.1891,  -6.8530,  -5.6592,  -3.2306,\n",
       "            -0.4923,  -4.3097,  -2.7764,  -3.6843,  -3.8025,  -6.7673, -12.6473,\n",
       "           -14.2376, -14.2896, -12.4408,  -9.3699, -10.2297, -11.1932, -13.1407,\n",
       "            -8.9915, -11.1478, -13.1992, -13.4301, -14.1808, -14.6562, -12.6317,\n",
       "           -10.2993, -12.3520, -10.9276,  -9.5356, -12.5300,  -5.1485]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.4864e+01, -3.4771e+01, -1.0556e+01, -1.8161e+01, -2.1678e+01,\n",
       "           -1.8273e+01, -1.6870e+01, -1.4366e+01, -1.7939e+01, -1.6986e+01,\n",
       "           -1.7126e+01, -1.7959e+01, -2.0305e+01, -1.8896e+01, -2.0030e+01,\n",
       "           -2.2323e+01, -1.7252e+01, -1.5459e+01, -1.9141e+01, -1.9893e+01,\n",
       "           -2.0317e+01, -1.3857e+01, -1.8134e+01, -2.0652e+01, -2.2451e+01,\n",
       "           -2.0064e+01, -2.1091e+01, -2.0927e+01, -1.7766e+01, -1.9104e+01,\n",
       "           -1.6429e+01, -1.8720e+01, -2.1409e+01, -2.8014e-05]]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[-3.6011e+01, -3.5885e+01, -3.8861e-05, -1.7606e+01, -2.1458e+01,\n",
       "           -1.7576e+01, -1.5684e+01, -1.3984e+01, -1.7938e+01, -1.7088e+01,\n",
       "           -1.7856e+01, -1.7453e+01, -2.0782e+01, -1.9134e+01, -1.8564e+01,\n",
       "           -2.1171e+01, -1.6642e+01, -1.3368e+01, -1.7662e+01, -1.9068e+01,\n",
       "           -1.7863e+01, -1.4555e+01, -1.6515e+01, -1.8917e+01, -1.8916e+01,\n",
       "           -1.9525e+01, -1.9183e+01, -1.9216e+01, -1.6646e+01, -1.7895e+01,\n",
       "           -1.6372e+01, -1.6811e+01, -1.7588e+01, -1.0254e+01]]],\n",
       "        grad_fn=<LogSoftmaxBackward>)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([\"za\"])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL': 0,\n",
       " 'SOS': 1,\n",
       " 'EOS': 2,\n",
       " 'a': 3,\n",
       " 'e': 4,\n",
       " 'i': 5,\n",
       " 'o': 6,\n",
       " 'u': 7,\n",
       " 'A': 8,\n",
       " 'E': 9,\n",
       " 'I': 10,\n",
       " 'O': 11,\n",
       " 'U': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'f': 16,\n",
       " 'g': 17,\n",
       " 'h': 18,\n",
       " 'j': 19,\n",
       " 'k': 20,\n",
       " 'l': 21,\n",
       " 'm': 22,\n",
       " 'n': 23,\n",
       " 'p': 24,\n",
       " 'q': 25,\n",
       " 'r': 26,\n",
       " 's': 27,\n",
       " 't': 28,\n",
       " 'v': 29,\n",
       " 'w': 30,\n",
       " 'x': 31,\n",
       " 'z': 32,\n",
       " '.': 33}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.char2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding(torch.LongTensor([model.char2ind[\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1)\n",
      "(10, 1)\n",
      "z (256, 1)\n",
      "[[ 2.09263693e-01]\n",
      " [-3.17104201e-01]\n",
      " [-3.46960212e-01]\n",
      " [-3.82099066e-01]\n",
      " [-2.71665826e-01]\n",
      " [ 5.18073654e-01]\n",
      " [ 2.30343703e-01]\n",
      " [ 1.36834229e-01]\n",
      " [-2.99621327e-01]\n",
      " [ 5.35334617e-02]\n",
      " [-8.24420005e-02]\n",
      " [ 1.70503532e-01]\n",
      " [-6.46259067e-01]\n",
      " [-5.18761615e-02]\n",
      " [-1.73623358e-01]\n",
      " [-1.36645874e-01]\n",
      " [-2.27169164e-01]\n",
      " [-8.66621523e-01]\n",
      " [-1.16869672e-01]\n",
      " [ 1.18509523e-01]\n",
      " [ 3.23339634e-01]\n",
      " [ 6.70590519e-01]\n",
      " [-2.80037580e-01]\n",
      " [ 2.53936094e-01]\n",
      " [-1.55476413e-01]\n",
      " [ 9.44273448e-02]\n",
      " [-1.61042798e-01]\n",
      " [ 7.55784983e-01]\n",
      " [ 3.73489295e-01]\n",
      " [ 2.23931687e-01]\n",
      " [-8.35582285e-02]\n",
      " [-2.44428162e-01]\n",
      " [-5.50272047e-01]\n",
      " [-6.31977549e-02]\n",
      " [ 4.13851373e-01]\n",
      " [-2.67462097e-01]\n",
      " [ 1.12094530e-01]\n",
      " [ 5.21987963e-02]\n",
      " [ 1.48834927e-02]\n",
      " [-1.85149505e-01]\n",
      " [ 1.09912258e-01]\n",
      " [-2.77185246e-01]\n",
      " [-1.34415487e-02]\n",
      " [-1.29794342e-01]\n",
      " [ 3.65792176e-01]\n",
      " [-1.51237959e-01]\n",
      " [ 2.71078636e-02]\n",
      " [-6.74984507e-02]\n",
      " [-5.26669585e-02]\n",
      " [-5.15164537e-01]\n",
      " [-3.33322784e-01]\n",
      " [-2.33061285e-02]\n",
      " [ 1.44860534e-01]\n",
      " [ 9.03136662e-02]\n",
      " [ 1.23385908e-03]\n",
      " [-3.79023554e-01]\n",
      " [-2.48144307e-02]\n",
      " [ 6.03304652e-01]\n",
      " [ 9.33481006e-02]\n",
      " [ 2.45898073e-01]\n",
      " [-2.65469909e-01]\n",
      " [-8.69670909e-03]\n",
      " [ 3.84690241e-02]\n",
      " [-1.47481378e-02]\n",
      " [-5.85761510e-02]\n",
      " [-3.21213783e-01]\n",
      " [ 5.56965322e-02]\n",
      " [-1.98984318e-01]\n",
      " [ 1.31892577e-01]\n",
      " [ 1.97228725e-01]\n",
      " [-3.14411936e-01]\n",
      " [-1.25072765e-01]\n",
      " [-1.15442141e-01]\n",
      " [-4.72785767e-01]\n",
      " [ 2.50959612e-01]\n",
      " [ 9.27104786e-01]\n",
      " [ 4.80653436e-01]\n",
      " [-7.14226894e-02]\n",
      " [-2.95752484e-02]\n",
      " [ 2.15063846e-01]\n",
      " [ 3.46496903e-01]\n",
      " [-2.28312078e-01]\n",
      " [ 6.24040221e-02]\n",
      " [-1.41672899e-01]\n",
      " [-5.74230325e-02]\n",
      " [-7.94397714e-02]\n",
      " [-2.69712853e-01]\n",
      " [-1.00580728e-01]\n",
      " [-5.23012615e-02]\n",
      " [-1.60560110e-01]\n",
      " [-2.63880170e-02]\n",
      " [-3.55733999e-01]\n",
      " [-7.37821958e-02]\n",
      " [ 2.76547504e-01]\n",
      " [-1.82888893e-01]\n",
      " [ 1.95526594e-01]\n",
      " [-2.27087500e-01]\n",
      " [-1.91387652e-01]\n",
      " [-8.67196032e-02]\n",
      " [ 2.47359513e-01]\n",
      " [ 1.00088924e-01]\n",
      " [ 6.34749578e-02]\n",
      " [ 1.90869210e-01]\n",
      " [-1.87807568e-01]\n",
      " [ 1.84998741e-01]\n",
      " [ 3.76979433e-02]\n",
      " [ 2.93302160e-01]\n",
      " [ 6.61915475e-02]\n",
      " [-3.35180948e-01]\n",
      " [-2.77563038e-01]\n",
      " [-7.51942089e-02]\n",
      " [-1.22071471e-01]\n",
      " [-5.29219702e-02]\n",
      " [ 1.90229591e-01]\n",
      " [-2.47113173e-01]\n",
      " [ 1.67768608e-01]\n",
      " [-4.42760912e-01]\n",
      " [ 4.66365098e-01]\n",
      " [-2.20264040e-01]\n",
      " [ 9.02489900e-01]\n",
      " [-1.30922019e-01]\n",
      " [ 3.06895110e-01]\n",
      " [ 4.00375268e-02]\n",
      " [ 2.40952152e-01]\n",
      " [-3.67086345e-02]\n",
      " [ 8.33854901e-03]\n",
      " [-7.11920618e-01]\n",
      " [ 1.66858088e-01]\n",
      " [-1.93162197e-01]\n",
      " [-3.69330834e-02]\n",
      " [-3.51505192e-02]\n",
      " [-3.86243892e-01]\n",
      " [ 1.64804962e-01]\n",
      " [ 7.03902657e-02]\n",
      " [-2.08713306e-01]\n",
      " [ 5.12250807e-01]\n",
      " [ 5.17681404e-02]\n",
      " [-1.67835073e-01]\n",
      " [ 2.65583158e-01]\n",
      " [ 1.82494279e-01]\n",
      " [-1.21883510e-01]\n",
      " [ 5.97371867e-02]\n",
      " [ 3.71776717e-02]\n",
      " [-1.35197499e-01]\n",
      " [ 3.11373456e-01]\n",
      " [ 1.19588807e-01]\n",
      " [ 1.49764914e-01]\n",
      " [ 3.67951279e-01]\n",
      " [ 2.94912918e-01]\n",
      " [-1.20449289e-01]\n",
      " [-8.47991517e-02]\n",
      " [-2.92738201e-01]\n",
      " [ 2.20020372e-01]\n",
      " [-3.85045259e-01]\n",
      " [ 2.23883829e-01]\n",
      " [-6.18009492e-01]\n",
      " [ 7.97889567e-02]\n",
      " [-3.68608412e-01]\n",
      " [-2.52731357e-01]\n",
      " [-8.31155248e-02]\n",
      " [-2.03991361e-01]\n",
      " [ 3.73436907e-01]\n",
      " [-1.73136453e-01]\n",
      " [-2.41238939e-01]\n",
      " [-3.33965987e-01]\n",
      " [-1.43424839e-01]\n",
      " [ 1.63857078e-01]\n",
      " [-2.47324970e-02]\n",
      " [ 1.37075587e-02]\n",
      " [ 2.06342700e-01]\n",
      " [-2.97918272e-01]\n",
      " [ 4.43288063e-03]\n",
      " [-1.37366735e-01]\n",
      " [ 1.18139968e-01]\n",
      " [ 1.27610321e-01]\n",
      " [-5.55761239e-02]\n",
      " [ 6.69677366e-02]\n",
      " [ 2.95939153e-01]\n",
      " [-3.66019513e-01]\n",
      " [-4.75694913e-02]\n",
      " [ 3.03658341e-01]\n",
      " [-3.26692263e-01]\n",
      " [ 2.91318747e-01]\n",
      " [ 1.69113713e-01]\n",
      " [-2.45420977e-01]\n",
      " [-5.20656704e-01]\n",
      " [ 8.47713435e-02]\n",
      " [ 2.79220743e-01]\n",
      " [ 4.41278548e-01]\n",
      " [-2.59488567e-01]\n",
      " [-2.20659305e-02]\n",
      " [-7.28187658e-02]\n",
      " [ 4.31874011e-01]\n",
      " [-1.66508648e-01]\n",
      " [ 1.13364453e-01]\n",
      " [-2.41386178e-01]\n",
      " [-7.72922478e-01]\n",
      " [ 3.86465521e-01]\n",
      " [ 1.30577792e-01]\n",
      " [-5.83902061e-02]\n",
      " [ 8.52862973e-01]\n",
      " [-2.92239260e-01]\n",
      " [ 8.15923215e-01]\n",
      " [-6.91459031e-02]\n",
      " [ 2.24891815e-01]\n",
      " [ 3.25780526e-01]\n",
      " [ 9.37625492e-02]\n",
      " [-6.19564719e-02]\n",
      " [-2.28653315e-01]\n",
      " [-1.78628319e-01]\n",
      " [ 3.10927174e-01]\n",
      " [-2.50015681e-01]\n",
      " [-1.85946764e-01]\n",
      " [-9.00507125e-04]\n",
      " [ 2.46623489e-01]\n",
      " [ 1.49002877e-01]\n",
      " [ 4.23229429e-01]\n",
      " [ 1.15494822e-02]\n",
      " [ 1.83288176e-01]\n",
      " [ 9.62587087e-02]\n",
      " [-8.25810740e-02]\n",
      " [ 1.24001867e-01]\n",
      " [ 3.28164183e-02]\n",
      " [-5.10972039e-01]\n",
      " [ 1.07509737e-01]\n",
      " [-1.91146771e-01]\n",
      " [-2.25097190e-01]\n",
      " [ 3.65236139e-01]\n",
      " [-2.53856861e-01]\n",
      " [-1.14152958e-01]\n",
      " [-1.53720840e-01]\n",
      " [ 7.52608894e-02]\n",
      " [-1.71578341e-01]\n",
      " [ 3.19219574e-01]\n",
      " [-2.15871130e-01]\n",
      " [ 1.88845359e-01]\n",
      " [ 1.75756152e-02]\n",
      " [-1.95737122e-01]\n",
      " [ 1.47076675e-01]\n",
      " [-9.39503654e-03]\n",
      " [-3.55825901e-02]\n",
      " [ 1.25339799e-01]\n",
      " [ 4.24451703e-01]\n",
      " [-7.54524809e-02]\n",
      " [-2.93090936e-01]\n",
      " [ 1.01727244e-01]\n",
      " [ 1.06993612e-01]\n",
      " [-3.43947497e-01]\n",
      " [-5.09238811e-01]\n",
      " [ 7.51139824e-01]\n",
      " [ 1.30313462e-01]\n",
      " [ 3.75985829e-01]\n",
      " [ 1.19657333e-01]\n",
      " [-5.54658733e-01]\n",
      " [-2.11217044e-01]\n",
      " [-2.33863934e-01]]\n",
      "new enc\n",
      "[[-0.14522574]\n",
      " [-0.49900668]\n",
      " [-0.45585148]\n",
      " [ 0.68201255]\n",
      " [-0.40116264]\n",
      " [ 0.98002734]\n",
      " [ 0.28217723]\n",
      " [ 0.39785258]\n",
      " [ 0.55776116]\n",
      " [-0.42694242]\n",
      " [ 0.01584135]\n",
      " [ 0.13164868]\n",
      " [-0.99979593]\n",
      " [-0.1006198 ]\n",
      " [-0.29672287]\n",
      " [-0.17351785]\n",
      " [-0.84291363]\n",
      " [-0.79030257]\n",
      " [-0.06692825]\n",
      " [ 0.66387092]\n",
      " [-0.61185422]\n",
      " [ 0.60668538]\n",
      " [-0.37654711]\n",
      " [-0.87911023]\n",
      " [-0.13961561]\n",
      " [-0.60792   ]\n",
      " [-0.00475825]\n",
      " [ 0.59712329]\n",
      " [ 0.38222589]\n",
      " [ 0.29086293]\n",
      " [ 0.83565824]\n",
      " [-0.24709182]\n",
      " [ 0.55392633]\n",
      " [-0.27182726]\n",
      " [ 0.59961771]\n",
      " [-0.57632682]\n",
      " [ 0.55057661]\n",
      " [-0.02465318]\n",
      " [ 0.30416871]\n",
      " [-0.01037252]\n",
      " [ 0.02501664]\n",
      " [-0.27132911]\n",
      " [ 0.85136758]\n",
      " [ 0.99482658]\n",
      " [ 0.23982949]\n",
      " [-0.86300458]\n",
      " [-0.22086253]\n",
      " [-0.06928473]\n",
      " [ 0.303073  ]\n",
      " [-0.46361462]\n",
      " [-0.35932891]\n",
      " [-0.25900634]\n",
      " [ 0.54583956]\n",
      " [ 0.3233186 ]\n",
      " [-0.23027043]\n",
      " [-0.11980026]\n",
      " [-0.32329272]\n",
      " [ 0.54671381]\n",
      " [-0.55247893]\n",
      " [ 0.69365575]\n",
      " [-0.86177111]\n",
      " [-0.04933317]\n",
      " [ 0.71227055]\n",
      " [-0.51713582]\n",
      " [ 0.00625289]\n",
      " [-0.11160461]\n",
      " [ 0.22912919]\n",
      " [-0.25306059]\n",
      " [-0.45737492]\n",
      " [-0.53435127]\n",
      " [-0.91142733]\n",
      " [ 0.90570638]\n",
      " [ 0.51300333]\n",
      " [ 0.81812115]\n",
      " [-0.45036946]\n",
      " [ 0.92075216]\n",
      " [-0.24891681]\n",
      " [ 0.36998725]\n",
      " [ 0.09669186]\n",
      " [-0.45278694]\n",
      " [-0.13586539]\n",
      " [ 0.92816877]\n",
      " [-0.97037827]\n",
      " [-0.27332588]\n",
      " [ 0.79602702]\n",
      " [ 0.31322181]\n",
      " [-0.19716801]\n",
      " [-0.10333103]\n",
      " [-0.14591046]\n",
      " [-0.94806491]\n",
      " [ 0.70786138]\n",
      " [-0.66723698]\n",
      " [-0.06336057]\n",
      " [ 0.12758527]\n",
      " [ 0.94274072]\n",
      " [-0.4009941 ]\n",
      " [-0.21677691]\n",
      " [-0.60084098]\n",
      " [-0.29239745]\n",
      " [ 0.96114319]\n",
      " [ 0.5385555 ]\n",
      " [-0.4244578 ]\n",
      " [ 0.80844443]\n",
      " [-0.81660222]\n",
      " [-0.80475962]\n",
      " [ 0.02097569]\n",
      " [ 0.30333188]\n",
      " [ 0.09334294]\n",
      " [ 0.02207843]\n",
      " [-0.96566063]\n",
      " [-0.53230452]\n",
      " [-0.1377007 ]\n",
      " [ 0.02899562]\n",
      " [-0.24113173]\n",
      " [-0.63605209]\n",
      " [ 0.30584015]\n",
      " [-0.18701186]\n",
      " [ 0.5697868 ]\n",
      " [ 0.03165602]\n",
      " [ 0.8467652 ]\n",
      " [-0.1273723 ]\n",
      " [ 0.30292833]\n",
      " [ 0.89698485]\n",
      " [ 0.90434277]\n",
      " [-0.23486284]\n",
      " [ 0.09326561]\n",
      " [-0.7618957 ]\n",
      " [-0.13392269]\n",
      " [ 0.0034744 ]\n",
      " [ 0.37498496]\n",
      " [-0.73936226]\n",
      " [-0.52851875]\n",
      " [-0.0568043 ]\n",
      " [-0.0342072 ]\n",
      " [-0.14111888]\n",
      " [-0.35684649]\n",
      " [-0.48232259]\n",
      " [ 0.3143079 ]\n",
      " [-0.3907306 ]\n",
      " [ 0.92287147]\n",
      " [-0.32223461]\n",
      " [ 0.96701386]\n",
      " [-0.12994402]\n",
      " [-0.30074507]\n",
      " [ 0.95981706]\n",
      " [-0.20895606]\n",
      " [ 0.01701856]\n",
      " [-0.8300574 ]\n",
      " [ 0.31339009]\n",
      " [-0.91641116]\n",
      " [-0.20861952]\n",
      " [-0.32722972]\n",
      " [ 0.91995308]\n",
      " [ 0.83623338]\n",
      " [-0.7294382 ]\n",
      " [-0.18615319]\n",
      " [ 0.06377785]\n",
      " [-0.50748105]\n",
      " [-0.26405513]\n",
      " [-0.91914169]\n",
      " [-0.17272519]\n",
      " [ 0.11642488]\n",
      " [-0.70139487]\n",
      " [-0.96914328]\n",
      " [ 0.54927382]\n",
      " [ 0.18805908]\n",
      " [ 0.16635569]\n",
      " [ 0.25514195]\n",
      " [-0.96868547]\n",
      " [-0.01948876]\n",
      " [-0.67643126]\n",
      " [-0.78893976]\n",
      " [-0.59338939]\n",
      " [ 0.2362458 ]\n",
      " [ 0.83081633]\n",
      " [-0.5302357 ]\n",
      " [-0.15436578]\n",
      " [ 0.30177641]\n",
      " [-0.01839378]\n",
      " [ 0.03214461]\n",
      " [ 0.30545732]\n",
      " [ 0.91294995]\n",
      " [ 0.9298463 ]\n",
      " [ 0.0310471 ]\n",
      " [-0.25796017]\n",
      " [ 0.69694271]\n",
      " [-0.29359236]\n",
      " [ 0.52203462]\n",
      " [ 0.50874625]\n",
      " [ 0.24154886]\n",
      " [-0.09970921]\n",
      " [ 0.69172611]\n",
      " [ 0.37806428]\n",
      " [-0.23438666]\n",
      " [ 0.20637242]\n",
      " [-0.45471526]\n",
      " [-0.76622279]\n",
      " [ 0.35677379]\n",
      " [ 0.00637017]\n",
      " [-0.24175045]\n",
      " [ 0.85367915]\n",
      " [-0.85071667]\n",
      " [ 0.12450792]\n",
      " [-0.18297007]\n",
      " [ 0.98771346]\n",
      " [-0.11208474]\n",
      " [-0.0537279 ]\n",
      " [ 0.50973313]\n",
      " [ 0.01336043]\n",
      " [-0.65598246]\n",
      " [ 0.9315578 ]\n",
      " [-0.97408833]\n",
      " [ 0.16329214]\n",
      " [ 0.03465015]\n",
      " [-0.60111742]\n",
      " [ 0.16806622]\n",
      " [ 0.0424608 ]\n",
      " [ 0.01508713]\n",
      " [-0.83275435]\n",
      " [ 0.05343455]\n",
      " [-0.08060717]\n",
      " [-0.48419283]\n",
      " [-0.02396302]\n",
      " [-0.60433073]\n",
      " [ 0.0890113 ]\n",
      " [ 0.39508537]\n",
      " [ 0.97241234]\n",
      " [ 0.35143384]\n",
      " [ 0.25319529]\n",
      " [ 0.07512429]\n",
      " [-0.90658522]\n",
      " [ 0.02669325]\n",
      " [ 0.77322391]\n",
      " [ 0.32572924]\n",
      " [-0.13177896]\n",
      " [-0.06695239]\n",
      " [ 0.94947055]\n",
      " [ 0.26149865]\n",
      " [ 0.43849429]\n",
      " [ 0.8292978 ]\n",
      " [-0.03673695]\n",
      " [-0.52251485]\n",
      " [-0.86032412]\n",
      " [-0.74353396]\n",
      " [-0.32600638]\n",
      " [ 0.91409329]\n",
      " [ 0.35328274]\n",
      " [ 0.76785141]\n",
      " [-0.49507987]\n",
      " [ 0.0221958 ]\n",
      " [-0.01517115]\n",
      " [-0.63799478]\n",
      " [ 0.07268998]\n",
      " [-0.97044691]\n",
      " [-0.39454725]\n",
      " [-0.07367579]]\n",
      "in the dec!!!\n",
      "[[-0.38122516]\n",
      " [-0.4204909 ]\n",
      " [-0.8236824 ]\n",
      " [ 0.8546038 ]\n",
      " [ 0.84987246]\n",
      " [-0.22395387]\n",
      " [ 0.27805441]\n",
      " [ 0.52812507]\n",
      " [ 0.55631997]\n",
      " [-0.7545463 ]\n",
      " [-0.33463775]\n",
      " [ 0.24809188]\n",
      " [-0.99497396]\n",
      " [-0.21868817]\n",
      " [-0.84830057]\n",
      " [ 0.25508768]\n",
      " [-0.39820425]\n",
      " [-0.79028725]\n",
      " [ 0.58872827]\n",
      " [ 0.66021741]\n",
      " [-0.61009333]\n",
      " [ 0.61304622]\n",
      " [-0.57556597]\n",
      " [-0.87928503]\n",
      " [ 0.15991799]\n",
      " [-0.16433957]\n",
      " [-0.03418519]\n",
      " [ 0.59343243]\n",
      " [-0.88644427]\n",
      " [-0.41430109]\n",
      " [ 0.96855128]\n",
      " [-0.37598473]\n",
      " [-0.82879979]\n",
      " [-0.28306454]\n",
      " [ 0.5956043 ]\n",
      " [-0.94812264]\n",
      " [ 0.51867669]\n",
      " [ 0.64345867]\n",
      " [ 0.19607154]\n",
      " [-0.99738288]\n",
      " [-0.42299707]\n",
      " [-0.75625295]\n",
      " [-0.98374154]\n",
      " [ 0.96897342]\n",
      " [ 0.12123553]\n",
      " [-0.84247205]\n",
      " [-0.20870399]\n",
      " [-0.01978829]\n",
      " [ 0.35253878]\n",
      " [ 0.90360407]\n",
      " [-0.39974314]\n",
      " [-0.27410264]\n",
      " [ 0.55587693]\n",
      " [ 0.74757891]\n",
      " [-0.81788876]\n",
      " [-0.98066135]\n",
      " [-0.32957981]\n",
      " [ 0.54160301]\n",
      " [-0.63030558]\n",
      " [ 0.68707576]\n",
      " [-0.85961389]\n",
      " [ 0.83383949]\n",
      " [ 0.65903601]\n",
      " [-0.46308328]\n",
      " [ 0.01895303]\n",
      " [-0.12338285]\n",
      " [ 0.18244107]\n",
      " [ 0.96276927]\n",
      " [-0.44908792]\n",
      " [-0.52747604]\n",
      " [ 0.42601557]\n",
      " [ 0.9059791 ]\n",
      " [ 0.38946917]\n",
      " [ 0.92313401]\n",
      " [-0.44542737]\n",
      " [ 0.86451174]\n",
      " [ 0.205659  ]\n",
      " [-0.34003166]\n",
      " [-0.54717251]\n",
      " [-0.74500355]\n",
      " [ 0.03071145]\n",
      " [ 0.87039469]\n",
      " [-0.9603032 ]\n",
      " [ 0.28425466]\n",
      " [ 0.66375068]\n",
      " [-0.21219781]\n",
      " [ 0.22697636]\n",
      " [-0.97392428]\n",
      " [-0.51233897]\n",
      " [-0.90306203]\n",
      " [ 0.96808331]\n",
      " [-0.63762053]\n",
      " [-0.78421012]\n",
      " [ 0.17311724]\n",
      " [-0.40565786]\n",
      " [-0.58378988]\n",
      " [ 0.84116387]\n",
      " [-0.26337562]\n",
      " [-0.61741823]\n",
      " [ 0.74380902]\n",
      " [ 0.63099623]\n",
      " [-0.43403799]\n",
      " [-0.98561216]\n",
      " [-0.78726681]\n",
      " [-0.74071148]\n",
      " [ 0.12768898]\n",
      " [-0.96944882]\n",
      " [ 0.25570051]\n",
      " [ 0.38238112]\n",
      " [-0.95891271]\n",
      " [-0.49863587]\n",
      " [-0.12932374]\n",
      " [ 0.37801128]\n",
      " [ 0.95445912]\n",
      " [ 0.7269755 ]\n",
      " [ 0.23987596]\n",
      " [-0.8993896 ]\n",
      " [ 0.51936586]\n",
      " [-0.41633263]\n",
      " [ 0.82681495]\n",
      " [-0.70765677]\n",
      " [-0.62121842]\n",
      " [ 0.7849127 ]\n",
      " [ 0.90434466]\n",
      " [ 0.93594384]\n",
      " [-0.96060181]\n",
      " [-0.71376556]\n",
      " [ 0.36105265]\n",
      " [ 0.36273956]\n",
      " [ 0.3125305 ]\n",
      " [-0.99542576]\n",
      " [ 0.97926656]\n",
      " [-0.41918033]\n",
      " [-0.91682844]\n",
      " [-0.11142239]\n",
      " [-0.05882289]\n",
      " [-0.56048185]\n",
      " [-0.04061879]\n",
      " [-0.46944666]\n",
      " [ 0.88141639]\n",
      " [-0.28692442]\n",
      " [-0.78184686]\n",
      " [ 0.83030256]\n",
      " [-0.29352161]\n",
      " [-0.42446748]\n",
      " [-0.14847908]\n",
      " [ 0.04020782]\n",
      " [-0.73694652]\n",
      " [ 0.31364357]\n",
      " [-0.97416293]\n",
      " [-0.95619366]\n",
      " [-0.34897444]\n",
      " [ 0.91372082]\n",
      " [ 0.83094754]\n",
      " [-0.48679838]\n",
      " [ 0.93228304]\n",
      " [ 0.3075393 ]\n",
      " [-0.84050709]\n",
      " [-0.26304876]\n",
      " [-0.92437754]\n",
      " [-0.82177327]\n",
      " [ 0.15555452]\n",
      " [-0.76585123]\n",
      " [-0.96851762]\n",
      " [ 0.56070169]\n",
      " [ 0.17682068]\n",
      " [-0.81898773]\n",
      " [ 0.37538663]\n",
      " [-0.95326735]\n",
      " [-0.152777  ]\n",
      " [ 0.92731307]\n",
      " [-0.46731511]\n",
      " [-0.59951653]\n",
      " [ 0.36080834]\n",
      " [ 0.78768139]\n",
      " [ 0.95721135]\n",
      " [-0.19606233]\n",
      " [ 0.37861583]\n",
      " [-0.36277896]\n",
      " [-0.9831923 ]\n",
      " [-0.17838207]\n",
      " [ 0.92454536]\n",
      " [-0.8942072 ]\n",
      " [ 0.02547299]\n",
      " [-0.27958809]\n",
      " [ 0.69567357]\n",
      " [-0.36611723]\n",
      " [ 0.98674878]\n",
      " [ 0.48894055]\n",
      " [ 0.21892866]\n",
      " [-0.12235239]\n",
      " [-0.92393214]\n",
      " [ 0.36757496]\n",
      " [-0.38746044]\n",
      " [-0.17948187]\n",
      " [-0.43712469]\n",
      " [-0.55334739]\n",
      " [-0.34374307]\n",
      " [ 0.18203789]\n",
      " [ 0.63808685]\n",
      " [ 0.81138014]\n",
      " [ 0.9038596 ]\n",
      " [ 0.58077318]\n",
      " [ 0.0584776 ]\n",
      " [ 0.01978765]\n",
      " [-0.02669836]\n",
      " [-0.1703764 ]\n",
      " [ 0.22199844]\n",
      " [-0.96442047]\n",
      " [-0.73449205]\n",
      " [ 0.92411826]\n",
      " [-0.92488225]\n",
      " [ 0.38404209]\n",
      " [ 0.3821445 ]\n",
      " [-0.50230532]\n",
      " [-0.6180162 ]\n",
      " [ 0.0340436 ]\n",
      " [-0.97055701]\n",
      " [-0.8314558 ]\n",
      " [ 0.57073035]\n",
      " [ 0.19248454]\n",
      " [-0.48872368]\n",
      " [ 0.7961439 ]\n",
      " [ 0.25448077]\n",
      " [ 0.81047246]\n",
      " [ 0.39443657]\n",
      " [ 0.87129676]\n",
      " [-0.56954825]\n",
      " [ 0.33922788]\n",
      " [-0.06533537]\n",
      " [ 0.9796656 ]\n",
      " [-0.92713273]\n",
      " [ 0.738329  ]\n",
      " [-0.35564535]\n",
      " [-0.1454001 ]\n",
      " [-0.14011836]\n",
      " [ 0.84552352]\n",
      " [ 0.98741671]\n",
      " [ 0.37805086]\n",
      " [ 0.81799609]\n",
      " [ 0.01729598]\n",
      " [ 0.83767675]\n",
      " [-0.85866246]\n",
      " [-0.90255615]\n",
      " [ 0.6800415 ]\n",
      " [ 0.91672128]\n",
      " [ 0.91541401]\n",
      " [ 0.4166694 ]\n",
      " [-0.34045511]\n",
      " [-0.29750074]\n",
      " [ 0.90234371]\n",
      " [-0.78030948]\n",
      " [ 0.91037573]\n",
      " [-0.97079837]\n",
      " [ 0.22242749]\n",
      " [-0.12789857]]\n"
     ]
    }
   ],
   "source": [
    "# Does the right thing\n",
    "import numpy as np\n",
    "\n",
    "emb_mat = model.embedding.weights.data.numpy()\n",
    "\n",
    "inp = np.matmul(np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]]), emb_mat).transpose() # GOOD\n",
    "h = np.array([[0 for _ in range(256)]]).transpose()\n",
    "\n",
    "print(h.shape)\n",
    "print(inp.shape)\n",
    "\n",
    "uz = model.enc_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wz = model.enc_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bz = np.expand_dims(model.enc_lstm.wz_bias.data.numpy(),axis=1) # GOOD\n",
    "\n",
    "ur = model.enc_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wr = model.enc_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "br = np.expand_dims(model.enc_lstm.wr_bias.data.numpy(),axis=1)\n",
    "\n",
    "ux = model.enc_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wx = model.enc_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bx = np.expand_dims(model.enc_lstm.wx_bias.data.numpy() + model.enc_lstm.wrh_bias.data.numpy(),axis=1) # GOOD\n",
    "\n",
    "\n",
    "uzd = model.dec_lstm.wz_weights.transpose(0,1)[:10].data.numpy().transpose() # GOOD\n",
    "wzd = model.dec_lstm.wz_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "bzd = np.expand_dims(model.dec_lstm.wz_bias.data.numpy(),axis=1) # GOOD\n",
    "\n",
    "urd = model.dec_lstm.wr_weights.transpose(0,1)[:10].data.numpy().transpose()\n",
    "wrd = model.dec_lstm.wr_weights.transpose(0,1)[10:].data.numpy().transpose()\n",
    "brd = np.expand_dims(model.dec_lstm.wr_bias.data.numpy(),axis=1)\n",
    "\n",
    "uxd = model.dec_lstm.wx_weights.transpose(0,1).data.numpy().transpose() # GOOD\n",
    "wxd = model.dec_lstm.wrh_weights.transpose(0,1).data.numpy().transpose()\n",
    "bxd = np.expand_dims(model.dec_lstm.wx_bias.data.numpy() + model.dec_lstm.wrh_bias.data.numpy(),axis=1) # GOOD\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "print(\"z\", bz.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "\n",
    "inp = np.matmul(np.array([[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose()\n",
    "\n",
    "print(h)\n",
    "#print(\"new_enc\", h.shape)\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uz,inp) + np.matmul(wz, h) + bz\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "#print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(ur,inp) + np.matmul(wr, h) + br\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "#print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(ux,inp) + np.matmul(wx, r*h) + bx\n",
    "htilde = np.tanh(htilde_pre)\n",
    "#print(\"htilde\", htilde.shape)\n",
    "\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "\n",
    "#print(h.shape)\n",
    "print(\"new enc\")\n",
    "print(h)\n",
    "\n",
    "inp = np.matmul(np.array([[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]), emb_mat).transpose()\n",
    "#print(\"new_enc\", h.shape)\n",
    "\n",
    "\n",
    "z_pre = np.matmul(uzd,inp) + np.matmul(wzd, h) + bzd\n",
    "z = np.exp(z_pre) / (1 + np.exp(z_pre))\n",
    "#print(\"z\", z.shape)\n",
    "\n",
    "r_pre = np.matmul(urd,inp) + np.matmul(wrd, h) + brd\n",
    "r = np.exp(r_pre) / (1 + np.exp(r_pre))\n",
    "#print(\"r\", r.shape)\n",
    "\n",
    "htilde_pre = np.matmul(uxd,inp) + np.matmul(wxd, r*h) + bxd\n",
    "htilde = np.tanh(htilde_pre)\n",
    "#print(\"htilde\", htilde.shape)\n",
    "\n",
    "\n",
    "h = (1 - z)*htilde + z*h\n",
    "print(\"in the dec!!!\")\n",
    "print(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.7056,  0.1680, -0.8589,  0.9210,  0.6723,  0.3097, -0.2655,\n",
       "            0.6218,  0.7066, -0.8842, -0.5191,  0.4401, -0.9818, -0.4042,\n",
       "           -0.6082, -0.1423,  0.4787, -0.2996,  0.2559,  0.8032, -0.6610,\n",
       "           -0.5030,  0.4970, -0.8645, -0.2783, -0.2295, -0.0696, -0.3059,\n",
       "           -0.7734, -0.2524,  0.9616, -0.2255, -0.9022,  0.9719,  0.5728,\n",
       "           -0.8875,  0.1908,  0.5525,  0.2557, -0.9923, -0.3461, -0.8700,\n",
       "           -0.9734,  0.8422,  0.6291, -0.8584,  0.0915,  0.8212,  0.3918,\n",
       "            0.8232, -0.3085, -0.2670,  0.0442,  0.5186, -0.7909, -0.9605,\n",
       "           -0.0644, -0.5299, -0.7515,  0.4523, -0.9330,  0.7422,  0.7111,\n",
       "            0.2425, -0.7234, -0.4766,  0.0549,  0.9401, -0.6049,  0.1121,\n",
       "            0.5427,  0.8490,  0.5464,  0.9613, -0.7076, -0.3046, -0.0979,\n",
       "           -0.3552, -0.7209, -0.7082,  0.0614,  0.5661,  0.3739,  0.5323,\n",
       "           -0.3221, -0.0046,  0.3147, -0.9543, -0.5502,  0.9482,  0.9584,\n",
       "           -0.4668, -0.5796,  0.1773, -0.5636, -0.8016,  0.7662, -0.2917,\n",
       "           -0.4681,  0.5920,  0.2275, -0.5651, -0.9870,  0.5985, -0.2369,\n",
       "            0.0988, -0.9367,  0.3534, -0.2888, -0.9603, -0.4757,  0.8520,\n",
       "            0.4313,  0.9384,  0.8927,  0.3374, -0.9268, -0.0763, -0.8092,\n",
       "            0.0997, -0.7296, -0.5235,  0.8776,  0.9070,  0.9521, -0.9473,\n",
       "           -0.4330,  0.3364,  0.1783, -0.1612, -0.9968,  0.9779, -0.5567,\n",
       "           -0.9339, -0.1344,  0.3871, -0.5107,  0.9516, -0.1928,  0.8892,\n",
       "           -0.0453, -0.6682,  0.8332,  0.6423, -0.5211, -0.5417,  0.0427,\n",
       "            0.0401,  0.1789, -0.9801, -0.9047,  0.8300,  0.9238,  0.8410,\n",
       "           -0.1936,  0.7580, -0.0358, -0.7622, -0.5855, -0.9519, -0.7193,\n",
       "            0.3083, -0.9066, -0.9560, -0.4926,  0.5083, -0.7349,  0.7393,\n",
       "           -0.9436, -0.2957,  0.7401, -0.4665, -0.3174,  0.5075,  0.5715,\n",
       "            0.9395,  0.1215,  0.3476,  0.0322, -0.9002, -0.3908,  0.8731,\n",
       "           -0.9165, -0.0308, -0.2287,  0.2738,  0.6017,  0.9660,  0.6858,\n",
       "           -0.3229, -0.0679, -0.9460, -0.6745, -0.1985,  0.0930, -0.6268,\n",
       "           -0.0470, -0.6049,  0.1274, -0.1450, -0.2661,  0.8127,  0.8264,\n",
       "            0.4722, -0.0719, -0.3133, -0.2635, -0.2277, -0.9694, -0.7543,\n",
       "            0.9424,  0.9515,  0.5130,  0.1060,  0.7087, -0.5087,  0.5699,\n",
       "           -0.9612, -0.9362,  0.6195, -0.0597, -0.7701,  0.9391,  0.7117,\n",
       "            0.3663,  0.3723,  0.5700, -0.1520,  0.0784, -0.2401,  0.9660,\n",
       "           -0.8841,  0.3298, -0.5531,  0.3632,  0.0916,  0.5360,  0.9867,\n",
       "            0.3434, -0.2615,  0.6820,  0.7587, -0.9077, -0.8959,  0.7271,\n",
       "            0.7506,  0.7476,  0.6429, -0.0198, -0.4904,  0.6632, -0.5882,\n",
       "            0.8865, -0.9359, -0.1471, -0.3614]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.5677,  0.1692, -0.5503,  0.9204, -0.9587,  0.2401, -0.2614,\n",
       "            0.6247,  0.3241, -0.6354,  0.2090,  0.3968,  0.6850, -0.4681,\n",
       "           -0.8513, -0.1464,  0.5927, -0.3071,  0.0551,  0.8031, -0.2248,\n",
       "           -0.5030,  0.4435, -0.8642,  0.9907,  0.3100, -0.1386, -0.4105,\n",
       "            0.9768, -0.9690,  0.5026, -0.5021, -0.6186,  0.9532,  0.5770,\n",
       "            0.9959,  0.1799,  0.5623,  0.2517, -0.7633, -0.9792,  0.9096,\n",
       "            0.9800, -0.9060,  0.5574, -0.8474,  0.1125,  0.8266, -0.3445,\n",
       "           -0.9888,  0.2069, -0.2675, -0.1752,  0.3928, -0.5163,  0.9416,\n",
       "           -0.0653, -0.4531,  0.9994,  0.4623, -0.9303, -0.9972,  0.7037,\n",
       "            0.2386, -0.7235, -0.6579,  0.0529,  0.9984, -0.5834,  0.1148,\n",
       "           -0.2902,  0.8485,  0.5461,  0.9634, -0.7075, -0.2921, -0.0331,\n",
       "            0.0392,  0.1967, -0.7729,  0.1384,  0.6003,  0.3372, -0.9479,\n",
       "           -0.3360,  0.0642,  0.9472, -0.9544,  0.5264,  0.9436, -0.3131,\n",
       "           -0.4664,  0.9851, -0.5826, -0.9994, -0.8038, -0.5580, -0.1940,\n",
       "           -0.4684, -0.5687,  0.1978, -0.5691,  0.9268,  0.5931, -0.2310,\n",
       "            0.0193,  0.7496,  0.3724, -0.0656,  0.7845, -0.4784,  0.8406,\n",
       "            0.5332,  0.5009,  0.2219, -0.9853, -0.9635, -0.0768, -0.8158,\n",
       "            0.0976, -0.9999, -0.9131,  0.8858,  0.9066,  0.6825,  0.9536,\n",
       "           -0.4188,  0.6657,  0.0512, -0.1648, -0.9447, -0.9654, -0.8322,\n",
       "            0.8128, -0.5086, -0.0616, -0.3436,  0.9218, -0.9978,  0.8832,\n",
       "            0.0247, -0.6711, -0.9470,  0.6357, -0.5119, -0.5406,  0.4455,\n",
       "            0.0621,  0.3066,  0.9948,  0.2514,  0.8295,  0.9234,  0.8198,\n",
       "           -0.1601, -0.9567, -0.0644, -0.7916,  0.7575, -0.9949,  0.8311,\n",
       "            0.1045, -0.8275, -0.9559, -0.4930,  0.5074,  0.9980, -0.0046,\n",
       "           -0.9394,  0.6177, -0.9939, -0.4709, -0.3187,  0.3636,  0.4315,\n",
       "            0.5014,  0.1188,  0.3995,  0.0239,  0.9574, -0.6255,  0.8746,\n",
       "           -0.9461, -0.0310, -0.2303, -0.2410,  0.7654, -0.8856,  0.6858,\n",
       "           -0.3224,  0.8669,  0.2702, -0.6728, -0.2517,  0.2125, -0.6655,\n",
       "           -0.0180, -0.9218, -0.1169, -0.7839, -0.2671, -0.3003,  0.8651,\n",
       "            0.6059, -0.0999, -0.1751, -0.3948, -0.1970, -0.6489, -0.7628,\n",
       "            0.9388,  0.9523,  0.6539, -0.5103,  0.7660, -0.5093,  0.5684,\n",
       "            0.8844, -0.9346,  0.9158,  0.9879, -0.5975, -1.0000, -0.5720,\n",
       "            0.2474,  0.3660,  0.5485, -0.1885,  0.2081, -0.3174,  1.0000,\n",
       "           -0.1255,  0.3304, -0.5222,  0.4549,  0.2624,  0.3545,  0.9751,\n",
       "            0.2498, -0.1753,  0.6827,  0.8165, -0.9076, -0.9019,  0.3213,\n",
       "           -0.5535,  0.1355,  0.6025,  0.4443,  0.4073,  0.1344, -0.5881,\n",
       "           -0.9979, -0.3090,  0.1839, -0.0688]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-6.7572e-01,  5.2399e-01,  2.2756e-01,  9.2176e-01,  8.1555e-01,\n",
       "           -3.7361e-01,  7.2368e-01,  3.9081e-01,  6.5697e-01, -9.5299e-01,\n",
       "           -8.8079e-01,  5.0350e-01,  5.0562e-01, -5.9363e-01,  4.6739e-01,\n",
       "            2.2243e-01,  7.2031e-01, -5.7376e-01, -9.3589e-01,  7.5020e-01,\n",
       "           -2.4393e-01,  8.8676e-01,  6.8360e-01, -8.2348e-01,  9.8786e-01,\n",
       "           -8.4595e-01, -7.4826e-01,  7.5655e-01,  9.6189e-01,  8.3861e-01,\n",
       "            9.9735e-01, -7.4685e-01, -6.2202e-01,  5.7947e-01,  7.9921e-01,\n",
       "            9.9977e-01,  2.6073e-01,  8.2702e-01,  5.0881e-03, -9.9642e-01,\n",
       "            3.3476e-01,  3.4957e-01, -1.1763e-01, -9.7662e-01, -2.9324e-01,\n",
       "            7.6376e-01,  9.0149e-01,  5.5622e-01, -4.8922e-01, -9.5989e-01,\n",
       "            9.4725e-01, -2.6761e-01,  6.3813e-01, -7.0945e-01, -8.2844e-01,\n",
       "           -5.9803e-01, -1.6294e-01, -2.0686e-01, -9.8566e-01,  8.1480e-01,\n",
       "           -9.0812e-01, -9.6838e-01,  6.8211e-01,  1.1767e-01, -4.5202e-01,\n",
       "            9.7899e-01, -4.4343e-01, -7.4344e-01, -7.8358e-01,  2.0969e-01,\n",
       "           -2.8744e-01,  8.5219e-01,  5.1020e-01,  9.9216e-01, -7.1087e-01,\n",
       "            9.7611e-01,  5.6744e-01,  7.7657e-01, -3.6807e-01, -9.4858e-01,\n",
       "            2.0880e-01,  5.4380e-01, -9.8278e-01, -9.4468e-01,  7.8414e-03,\n",
       "            1.5991e-02, -8.9422e-01, -9.5429e-01, -9.9343e-01, -6.1545e-01,\n",
       "           -3.9466e-01, -9.1024e-01, -8.0098e-01, -9.8093e-01,  9.4305e-01,\n",
       "           -8.7617e-01,  3.0692e-01, -1.9258e-01, -6.1175e-01,  3.9000e-02,\n",
       "            5.6556e-01, -6.1449e-01,  9.2628e-01, -8.1848e-01, -6.8778e-01,\n",
       "           -2.2914e-01,  7.2548e-01,  2.8538e-01,  5.7334e-01, -2.9024e-01,\n",
       "           -2.3534e-01, -9.4369e-01,  7.0036e-01,  7.5791e-01,  9.0037e-01,\n",
       "           -9.8171e-01, -9.0022e-01, -2.5529e-01, -9.9181e-01, -2.6269e-02,\n",
       "            9.9474e-01,  9.6637e-01,  9.2270e-01,  9.2327e-01, -9.6339e-01,\n",
       "           -6.3891e-01,  3.2935e-01,  9.0590e-01,  9.0228e-01, -1.7081e-01,\n",
       "           -9.0267e-01, -9.5809e-01,  2.6376e-01, -9.5365e-01,  8.6539e-01,\n",
       "           -6.0907e-01, -1.3535e-01, -7.2596e-02,  8.9552e-01,  9.0237e-01,\n",
       "            7.1212e-01,  9.9588e-01, -8.6141e-01,  5.7075e-01, -8.8788e-01,\n",
       "           -4.9710e-01,  4.8008e-01,  7.5836e-01,  9.9815e-01, -9.6243e-01,\n",
       "           -3.7405e-01,  6.5697e-01,  9.1606e-01,  2.4327e-01, -3.8057e-02,\n",
       "           -8.7238e-01, -6.5000e-02, -3.3631e-01,  9.5049e-01, -6.3699e-01,\n",
       "            9.7667e-01,  2.1316e-01, -9.7227e-01, -7.4712e-01, -5.0174e-01,\n",
       "           -9.2367e-01, -8.0615e-01,  9.5188e-01, -9.3887e-01, -9.4636e-01,\n",
       "           -9.1395e-01,  8.6863e-01, -6.2751e-01, -5.4698e-01,  8.7963e-01,\n",
       "            5.3653e-01,  4.3394e-01,  9.7909e-01, -5.0103e-01,  3.5047e-02,\n",
       "           -7.7932e-01, -5.9497e-01,  4.4200e-01, -1.8482e-02, -3.9996e-01,\n",
       "           -1.0766e-01,  3.8872e-01,  9.7683e-01,  6.7166e-01, -7.6763e-01,\n",
       "            2.1214e-01, -9.9331e-01,  2.9975e-01, -3.5184e-01,  6.7716e-01,\n",
       "           -6.5833e-01, -8.4913e-02, -1.1993e-01, -1.9318e-01,  5.4989e-01,\n",
       "           -5.0329e-01, -3.0023e-01,  9.3917e-01, -4.0986e-01, -1.3356e-01,\n",
       "            9.6329e-01, -3.6650e-01, -7.4103e-01,  2.8606e-01, -3.7521e-01,\n",
       "            9.3427e-01,  9.1815e-01,  6.5392e-01,  5.6520e-02,  5.9989e-01,\n",
       "            9.4388e-01, -3.3730e-04,  8.7852e-01, -8.6851e-01,  9.4140e-01,\n",
       "           -1.1837e-01, -7.0996e-01, -7.5172e-01,  9.8810e-01,  2.4719e-01,\n",
       "            1.2273e-01, -8.7974e-01, -1.8958e-01,  1.9962e-01,  8.4958e-01,\n",
       "           -9.7169e-01, -3.0626e-01,  3.7813e-01, -7.1414e-01,  6.2574e-01,\n",
       "            9.2723e-01,  5.9536e-01,  9.7859e-01,  7.5510e-01,  9.8087e-01,\n",
       "            6.4266e-01, -9.8272e-01, -9.0807e-01,  3.5421e-02,  5.2616e-01,\n",
       "           -5.2520e-01,  9.5858e-01, -3.5436e-01,  8.8759e-01,  5.0261e-01,\n",
       "            1.3383e-01, -4.3837e-01,  9.8339e-01, -6.3004e-01,  1.8428e-01,\n",
       "           -7.1404e-02]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.6508,  0.7552,  0.9742,  0.9078,  0.9590, -0.9357,  0.9408,\n",
       "            0.5624, -0.9768, -0.9990, -0.7326, -0.9826,  0.3565, -0.7941,\n",
       "            0.1955,  0.1911,  0.5164, -0.8390,  0.9957,  0.7466,  0.3358,\n",
       "            0.9993, -0.1376, -0.8107, -0.4880, -0.7364, -0.9931,  0.1407,\n",
       "            0.2496,  0.7048,  0.9850, -0.9990, -0.3903, -0.5545, -0.9029,\n",
       "           -0.9907, -0.5812,  0.9623, -0.6575, -0.9996, -0.6302, -0.5246,\n",
       "           -0.9999, -0.9756, -0.8819, -0.3386,  0.8926,  0.3482, -0.0511,\n",
       "            0.8205, -0.8356, -0.5210, -0.2849, -0.9025, -0.9991, -0.7810,\n",
       "           -0.9136,  0.6549,  0.9116,  0.9273, -0.8707, -0.3500, -0.4886,\n",
       "           -0.1593,  0.5744,  0.8937, -0.8095,  0.9834,  0.0498,  0.2355,\n",
       "           -0.2699,  0.8920, -0.7635,  0.9999, -0.0328,  0.8927,  0.6535,\n",
       "            0.6377, -0.7605, -0.9952,  0.9189,  0.5653, -0.9828,  0.8181,\n",
       "            0.6929,  0.6449, -0.8916, -0.9991, -0.9795, -0.4871,  0.4516,\n",
       "           -0.0453, -0.9900,  0.9425, -0.4029, -0.9906,  0.9632, -0.0934,\n",
       "           -0.6742,  0.7989,  0.8678, -0.0303, -0.9985,  0.6460,  0.1067,\n",
       "            0.3461,  0.4581, -0.1693,  0.6204, -0.4014, -0.0567, -0.6864,\n",
       "            0.9802,  0.9997,  0.4671,  0.2528, -0.3990, -0.1277,  0.8080,\n",
       "           -0.3699, -0.9992,  0.4119,  0.4673,  0.9236,  0.5987, -0.9894,\n",
       "            0.9128,  0.9135,  0.8458, -0.4424, -0.9340,  0.9574, -0.8489,\n",
       "           -0.9954,  0.9397,  0.9239, -0.1781, -0.7348,  0.1943, -0.9467,\n",
       "           -0.8650, -0.9924,  0.7511, -0.1793, -0.9298, -0.5612, -0.9501,\n",
       "            0.9445,  0.9735, -0.9617, -0.9996,  0.5241,  0.8431, -0.9174,\n",
       "            0.9861,  0.9928, -0.8660, -0.6441,  0.9039, -0.6424,  0.9953,\n",
       "           -0.9977, -0.6624, -0.4591, -0.5315, -0.7585, -0.9015,  0.9879,\n",
       "            0.2606, -0.9658,  0.9960,  0.9299, -0.6581, -0.9342, -0.8467,\n",
       "            0.9978, -0.3712,  0.2574, -0.9575, -0.9801, -0.8294, -0.5976,\n",
       "           -0.7627, -0.1256, -0.6312, -0.1528, -0.2242,  0.9562,  0.4971,\n",
       "           -0.8823, -0.8831, -0.9479, -0.5035,  0.8925, -0.7392, -0.1344,\n",
       "            0.9772, -0.7539, -0.9934,  0.9850, -0.8604, -0.6978,  0.9957,\n",
       "           -0.9926, -0.9531,  1.0000, -0.9994, -0.8387, -0.8995, -0.6479,\n",
       "            0.8219,  0.1632,  0.6820,  0.0567,  0.9123,  0.3626, -0.1118,\n",
       "           -0.2212, -0.5131,  0.9359,  0.9632, -0.7312, -0.1074,  0.9883,\n",
       "            0.9948,  0.5041,  0.4959, -0.7589, -0.9837, -0.6963,  0.9300,\n",
       "           -0.9771,  0.1104, -0.9381,  0.5052,  0.9065, -0.8071,  0.9892,\n",
       "           -0.1356,  0.9224,  0.8640,  0.9764, -0.9051, -0.9374, -0.1679,\n",
       "           -0.5047,  0.9983, -0.4963,  0.9828, -0.3055,  0.5889, -0.8024,\n",
       "            0.0242, -0.6303,  0.9849, -0.0348]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.7952,  0.7584,  0.9999,  0.9079, -0.3639, -0.7906,  0.5149,\n",
       "            0.5635, -1.0000, -0.9485, -0.7282, -0.9830, -0.8265, -0.6028,\n",
       "            0.2701,  0.1817,  0.5229, -0.9658, -0.9949,  0.7465,  0.8710,\n",
       "            0.9992, -0.2952, -0.8090,  1.0000,  0.9156, -0.9993, -0.9953,\n",
       "            0.9988, -0.9899,  0.5030, -0.9998, -0.9873, -0.7844,  0.9923,\n",
       "            0.9977, -0.6410,  0.9632, -0.6645, -0.8937,  0.6772, -0.2128,\n",
       "            0.9982,  0.1109, -0.6478, -0.0416,  0.9032,  0.2818,  0.9984,\n",
       "           -0.2125,  0.8271, -0.5256, -0.6269, -0.8950,  0.9998,  0.9958,\n",
       "           -0.9130,  0.8056,  0.9999,  0.9809, -0.8171, -0.5257, -0.5077,\n",
       "           -0.1623,  0.6828,  0.3992, -0.8143,  0.8987,  0.0280,  0.2365,\n",
       "            0.1298,  0.8926, -0.9377,  0.9999, -0.0358,  0.6609,  0.7035,\n",
       "            0.9927, -0.7533, -0.9679,  0.9217, -0.4265, -0.4885, -0.9944,\n",
       "            0.6407,  0.5709, -0.8682, -0.9991, -0.9469, -0.4239,  0.4884,\n",
       "            0.5924,  0.9999,  0.0361, -0.9991, -0.9908, -0.8274, -0.0559,\n",
       "           -0.6716, -0.9998,  0.8811, -0.0554, -0.9921, -0.5496, -0.5252,\n",
       "           -0.0638,  0.5084, -0.4924,  0.5185,  0.6956, -0.0592, -0.6971,\n",
       "            0.9942, -0.5755,  0.1844,  0.9895,  0.9949,  0.0342,  0.9413,\n",
       "           -0.3785, -0.9996, -0.9408,  0.4847,  0.9235, -0.9073,  0.6344,\n",
       "            0.9156,  0.9990,  0.6667, -0.4676, -0.7470,  0.5466, -0.9934,\n",
       "            0.6381,  0.0051,  0.9783, -0.2370, -0.7802, -0.9899, -0.8185,\n",
       "            0.9655, -0.9918, -0.9969, -0.1963, -0.8667, -0.5594,  0.9998,\n",
       "            0.9964,  0.9970,  0.8448, -0.9857,  0.5237,  0.8402, -0.8419,\n",
       "            0.9773, -0.6467, -0.9460, -0.6462, -0.9474, -0.9992,  0.9624,\n",
       "           -0.9980, -0.8573, -0.4732, -0.5334, -0.7051,  0.8957,  0.3327,\n",
       "           -0.4209,  0.9966, -0.9886,  0.7632, -0.8396, -0.9215,  0.8819,\n",
       "            0.9334, -0.3693,  0.7248, -0.9612,  0.9389, -0.9093, -0.5941,\n",
       "           -0.4896, -0.1259, -0.6359, -0.1621, -0.9219, -0.3317,  0.4964,\n",
       "           -0.9491,  0.9954, -0.6341, -0.5931,  0.2045,  0.8361, -0.3775,\n",
       "            0.9862, -0.9936, -0.9965, -0.6304, -0.8685, -0.7696,  0.9962,\n",
       "            0.4614, -0.9541,  0.9997, -0.3881, -0.8328, -0.9999, -0.6555,\n",
       "            0.6254,  0.5798,  0.7246,  0.0565,  0.9169,  0.3617, -0.0832,\n",
       "           -0.9941, -0.3555,  0.9369,  0.9957, -0.9877, -1.0000,  0.9211,\n",
       "           -0.3663,  0.5065,  0.5392, -0.7595, -0.9775,  0.4874,  1.0000,\n",
       "            0.5182,  0.0792, -0.8288,  0.4128, -0.1356,  0.3185,  0.9738,\n",
       "            0.1149,  0.9427,  0.8929,  0.9832, -0.9050,  0.4430, -0.8836,\n",
       "           -0.4668,  0.0582, -0.5038,  1.0000,  0.1097, -0.3992, -0.8032,\n",
       "           -0.9905, -0.9755, -0.5701, -0.5721]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4622,  0.9674,  1.0000,  0.9105,  0.9180, -0.8498,  0.5305,\n",
       "            0.5695, -0.9915, -0.9896, -0.7138, -0.8619, -0.5408, -0.5513,\n",
       "            0.5262, -0.9906,  0.5352, -0.9705, -0.9810,  0.7491,  0.9889,\n",
       "            0.9892, -0.6285, -0.8026,  0.9999, -0.8098, -0.8223, -0.0381,\n",
       "           -0.4174,  0.3960,  0.9991, -0.9801, -0.9873, -0.9172,  0.8225,\n",
       "            1.0000, -0.0728,  0.8818, -0.8061, -0.9816,  0.9546, -0.6275,\n",
       "           -0.4389, -0.3804,  0.3277,  0.8722,  0.9719,  0.4737,  0.9903,\n",
       "            0.4386,  0.9944, -0.5494, -0.1235, -0.8964,  0.9281, -0.6142,\n",
       "           -0.2965,  0.4524,  0.8656, -0.0386, -0.6011, -0.2512, -0.3617,\n",
       "           -0.1323,  0.6986,  0.4017, -0.8896, -0.8684, -0.0184,  0.2541,\n",
       "            0.3046,  0.9169, -0.8602,  0.9833, -0.4942, -0.7027,  0.9401,\n",
       "            0.9960, -0.7973, -0.0917,  0.8925, -0.5819, -0.5816, -0.8373,\n",
       "            0.6386,  0.6324, -0.9722, -0.9983, -0.0106, -0.9026, -0.9309,\n",
       "           -0.8534,  0.0186, -0.3261,  0.9941, -0.6449, -0.4108,  0.0886,\n",
       "           -0.7621, -0.9958,  0.7217, -0.2248,  0.5258, -0.9724,  0.5627,\n",
       "           -0.4058, -0.7836, -0.6883,  0.3541, -0.5280, -0.1028, -0.8113,\n",
       "            0.9948,  0.1030, -0.7022,  0.9896,  0.9874,  0.6915,  0.0021,\n",
       "           -0.5879,  0.9799,  0.8444,  0.5314,  0.9216, -0.9990,  0.3304,\n",
       "            0.2060,  0.9679,  0.6357, -0.7266, -0.7414,  0.3127,  0.3321,\n",
       "           -0.7237,  0.8059, -0.1681, -0.1582, -0.8992,  0.5350, -0.8099,\n",
       "            0.5241,  0.9947, -0.0484, -0.2927, -0.7691, -0.5645, -0.4256,\n",
       "            0.8737,  0.9842, -0.9266,  0.6319,  0.5186,  0.8216, -0.6310,\n",
       "           -0.1709, -0.1025, -0.9463, -0.1838, -0.5997,  0.8746,  0.9989,\n",
       "            0.4142, -0.3610, -0.5684, -0.5345, -0.9874, -0.7570,  0.4631,\n",
       "            0.9463, -0.9554,  0.2013,  0.7617, -0.9750, -0.9522,  0.9549,\n",
       "            0.7485, -0.3501,  0.8659, -0.9613,  0.8879, -0.9119, -0.8336,\n",
       "            0.7372, -0.1149, -0.7320,  0.0160, -0.8882,  0.5554, -0.7551,\n",
       "           -0.9973, -0.8606, -0.9355, -0.8805,  0.2722,  0.9291, -0.2052,\n",
       "            0.9809, -0.8650, -0.9967, -0.7304, -0.7912, -0.7725,  0.6715,\n",
       "           -0.4536, -0.9539, -0.4852,  0.1822, -0.8157, -0.9253, -0.5792,\n",
       "            0.5773, -0.1834,  0.7249, -0.5639,  0.5703,  0.3751,  0.3966,\n",
       "           -0.9935,  0.1535,  0.9096, -0.9764, -0.5076, -0.4194,  0.9839,\n",
       "           -0.3862,  0.6366, -0.5085, -0.6151, -0.9116,  0.8201, -0.9940,\n",
       "           -0.0042, -0.3818, -0.4773,  0.8518, -0.6766,  0.8777,  0.9898,\n",
       "            0.7423,  0.9936,  0.9187, -0.9882, -0.9071,  0.4360,  0.8403,\n",
       "           -0.4737, -0.3707, -0.5179,  0.9717, -0.8370, -0.8944, -0.5364,\n",
       "            0.9885,  0.1991, -0.7832, -0.6953]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0971,  0.9953,  1.0000,  0.9135,  0.9548, -0.7919,  0.6210,\n",
       "            0.7923, -0.9826, -0.9993, -0.7137, -0.8785, -0.5568, -0.3515,\n",
       "            0.4766, -0.9903,  0.5431, -0.4029,  0.9989,  0.7715,  0.8207,\n",
       "            0.9910, -0.9021, -0.8700,  0.2888, -0.4596, -0.9859,  0.5305,\n",
       "           -0.6363,  0.3752,  0.8959, -0.9991, -0.9322, -0.9224,  0.3609,\n",
       "            0.2997, -0.8973,  0.8986, -0.9394, -0.9941, -0.7845, -0.9721,\n",
       "           -0.9999, -0.3813,  0.5909,  0.8006,  0.9764,  0.7154,  0.9987,\n",
       "            0.9976,  0.4789, -0.9559,  0.1330, -0.8798, -0.9758, -0.7206,\n",
       "           -0.9281,  0.9407,  0.9993,  0.5056,  0.0316,  0.2444, -0.7903,\n",
       "           -0.2013,  0.7613,  0.3584, -0.9777,  0.9519, -0.0443,  0.2750,\n",
       "            0.3845,  0.9905, -0.9711,  0.9931, -0.3408, -0.8280,  0.9817,\n",
       "            0.9368, -0.7987, -0.4068,  0.8986,  0.2593, -0.5858,  0.9251,\n",
       "            0.6232,  0.6983, -0.7276, -0.9990, -0.3116, -0.8928,  0.8101,\n",
       "           -0.1947, -0.9761,  0.5020, -0.9642, -0.9559,  0.9266,  0.3119,\n",
       "           -0.7791, -0.9835,  0.8239, -0.1917, -0.9975,  0.7831,  0.6905,\n",
       "           -0.3669, -0.8303,  0.1395,  0.9311, -0.7337, -0.1133, -0.7644,\n",
       "            0.9999,  0.8368, -0.7700,  0.9928, -0.9392, -0.5283,  0.3282,\n",
       "           -0.6781, -0.9800,  0.8339,  0.5125,  0.9247, -0.4902, -0.5012,\n",
       "            0.8369,  0.9685,  0.6081, -0.8255, -0.9577,  0.9588, -0.7143,\n",
       "           -0.9992,  0.9628,  0.7670, -0.1599, -0.8186,  0.4837, -0.2243,\n",
       "            0.2549, -0.9822,  0.9988, -0.4905, -0.7702, -0.3909, -0.9651,\n",
       "            0.9392,  0.6799, -0.9276, -0.9995,  0.4963,  0.4793, -0.3097,\n",
       "            0.8268,  0.9913, -0.9997, -0.4373, -0.5834,  0.1922,  0.9536,\n",
       "           -0.9921, -0.8964, -0.5872,  0.0327,  0.6613, -0.8515,  0.7556,\n",
       "            0.9280, -0.9936,  0.9970,  0.5465, -0.9388, -0.9682, -0.0778,\n",
       "            0.9824, -0.2442,  0.9481, -0.9620, -0.8897, -0.9120, -0.8336,\n",
       "            0.6070, -0.5004, -0.9372, -0.3357, -0.9176,  0.7551, -0.9952,\n",
       "           -0.9858, -0.7625, -0.8923, -0.9700,  0.0203, -0.8141,  0.3383,\n",
       "            0.9839, -0.9715, -0.9997,  0.9886, -0.9872, -0.6430,  0.9101,\n",
       "           -0.6936, -0.9741,  0.9987, -0.9894, -0.8149, -0.9876, -0.7968,\n",
       "            0.4745, -0.1160,  0.8052,  0.3912,  0.5796,  0.3173,  0.3194,\n",
       "           -0.9712,  0.8428,  0.9024,  0.9985, -0.5851,  0.5735,  0.9600,\n",
       "            0.6420,  0.8259, -0.2999, -0.8832, -0.9828,  0.6072, -0.3047,\n",
       "           -0.9993, -0.5152, -0.6462, -0.1759, -0.8858,  0.8277,  0.9985,\n",
       "            0.3725,  0.9587,  0.9748,  0.0753, -0.9817,  0.0133,  0.7741,\n",
       "           -0.4581,  0.7743, -0.5004,  0.9951, -0.8475, -0.3627, -0.6604,\n",
       "            0.6717,  0.1955,  0.9925, -0.4977]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.6574,  0.9963,  1.0000,  0.9136, -0.9811, -0.6819,  0.2684,\n",
       "            0.7955, -1.0000, -0.9973, -0.7073, -0.8818, -0.7887, -0.1882,\n",
       "            0.5980, -0.9904,  0.5453, -0.9872, -0.9448,  0.7694,  0.9989,\n",
       "            0.9908, -0.9398, -0.8692,  0.9999,  0.5960, -0.9960, -0.9756,\n",
       "            0.9921, -0.5554,  0.7903, -0.9988, -0.9803, -0.9283,  0.9923,\n",
       "            0.9999, -0.9134,  0.8998, -0.9413, -0.9794,  0.9911, -0.8649,\n",
       "            0.9706,  0.4577,  0.3074,  0.7884,  0.9776,  0.2765,  1.0000,\n",
       "            0.7351, -0.5625, -0.9429, -0.7054, -0.8718,  0.9987,  0.9467,\n",
       "           -0.9261,  0.9480,  1.0000,  0.8658,  0.6792,  0.9572, -0.7866,\n",
       "           -0.2025,  0.8174,  0.1130, -0.9781,  0.9867, -0.0374,  0.2810,\n",
       "            0.4300,  0.9873, -0.8946,  0.9939, -0.3785, -0.8630,  0.9369,\n",
       "            0.9993, -0.7986, -0.6068,  0.8828,  0.5316, -0.5391, -0.9396,\n",
       "            0.6186,  0.7887, -0.2552, -0.9991, -0.6629, -0.8772,  0.8480,\n",
       "           -0.1324,  0.9994,  0.6052, -1.0000, -0.9606, -0.2534,  0.2442,\n",
       "           -0.7566, -1.0000,  0.8558, -0.2157, -0.9902,  0.6473,  0.1434,\n",
       "           -0.2539, -0.3862,  0.2667,  0.1370,  0.9302, -0.1244, -0.7701,\n",
       "            0.9998, -0.9486, -0.8417,  0.9922,  0.9932, -0.3047,  0.5622,\n",
       "           -0.6863, -1.0000, -0.9057,  0.4738,  0.9240,  0.0111,  0.9781,\n",
       "            0.8473,  0.9990,  0.5334, -0.8354, -0.5899, -0.7632, -0.9798,\n",
       "            0.5967,  0.3438,  0.9954, -0.1638, -0.8196, -0.9914, -0.2455,\n",
       "            0.9614, -0.9811, -0.7015, -0.4983, -0.7121, -0.3894,  0.9948,\n",
       "            0.8243,  0.9394, -0.5626, -0.9959,  0.4953,  0.4381, -0.1178,\n",
       "            0.9211,  0.4562, -1.0000, -0.4772, -0.8972, -0.9995,  0.9937,\n",
       "           -0.9973, -0.9389, -0.5959,  0.0299,  0.7033,  0.9215,  0.5784,\n",
       "            0.9267,  0.9944, -0.9928,  0.2633, -0.9717, -0.9328, -0.5554,\n",
       "            0.9279, -0.2436,  0.9957, -0.9621,  0.9966, -0.9142, -0.8331,\n",
       "           -0.8300, -0.5173, -0.9558, -0.6260, -0.9664,  0.6426, -0.9014,\n",
       "           -0.9269,  0.9913,  0.1509, -0.9834, -0.1551, -0.0769, -0.0577,\n",
       "            0.9842, -0.9999, -0.9999, -0.8800, -0.9879, -0.9329,  0.9455,\n",
       "            0.8756, -0.9653,  0.9997, -0.9318, -0.8107, -0.9998, -0.8311,\n",
       "            0.0431,  0.2650,  0.8801,  0.3518,  0.5813,  0.3170,  0.3240,\n",
       "           -0.9930,  0.9678,  0.9332,  0.9994, -0.8044, -0.9991,  0.8994,\n",
       "           -0.8558,  0.8238, -0.2224, -0.8799, -0.8074,  0.6291,  0.9995,\n",
       "            0.9427, -0.5349, -0.5822, -0.2962, -0.9765,  0.6539,  0.9980,\n",
       "            0.8091,  0.9493,  0.9830,  0.6653, -0.9811,  0.1015, -0.8949,\n",
       "           -0.4040,  0.4395, -0.4986,  1.0000,  0.1946, -0.7458, -0.6192,\n",
       "           -0.9585, -0.8669, -0.8105, -0.7993]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0521,  0.9982,  1.0000,  0.9142,  0.9232, -0.5179,  0.5693,\n",
       "            0.7908, -0.9999, -0.9996, -0.6487, -0.8211, -0.1441, -0.1201,\n",
       "            0.6350, -0.9961,  0.5661, -0.9356, -0.7983,  0.7724,  0.9385,\n",
       "            0.9857, -0.9428, -0.8689,  0.9999, -0.2223, -0.9846, -0.8008,\n",
       "           -0.3447,  0.2092,  0.9983, -0.9992, -0.9809, -0.6760, -0.5026,\n",
       "            1.0000, -0.7454,  0.8870, -0.8296, -0.9970,  0.9774, -0.9143,\n",
       "           -0.9746,  0.0766,  0.1126,  0.7261,  0.9822,  0.9818,  0.9969,\n",
       "            0.9678,  0.8710, -0.9434, -0.8322, -0.8710,  0.6424, -0.7618,\n",
       "           -0.8835,  0.9555,  0.9985,  0.7012,  0.8230,  0.9643, -0.7316,\n",
       "           -0.2022,  0.7418,  0.1632, -0.9787, -0.9873, -0.0363,  0.3000,\n",
       "            0.3569,  0.9831, -0.7866,  0.9931, -0.5527, -0.9236,  0.9557,\n",
       "            0.9974, -0.8106, -0.2220,  0.8754,  0.7314, -0.7237, -0.4315,\n",
       "            0.5524,  0.8382, -0.9558, -0.9990,  0.3549, -0.7955, -0.9941,\n",
       "           -0.9530, -0.1671,  0.0766,  0.9949, -0.4309,  0.1860,  0.2939,\n",
       "           -0.2138, -0.9982,  0.8718, -0.2416,  0.9436, -0.5320,  0.8134,\n",
       "           -0.3061, -0.8440, -0.1059, -0.0553, -0.5632, -0.1462, -0.7741,\n",
       "            0.9984, -0.3977, -0.5643,  0.9922,  0.9701, -0.1478,  0.3356,\n",
       "           -0.6978,  0.6223,  0.9024,  0.4649,  0.9215, -0.9852,  0.7396,\n",
       "            0.7116,  0.9968,  0.4651, -0.8424, -0.8343, -0.8509, -0.9036,\n",
       "           -0.9000,  0.3281,  0.8893, -0.1178, -0.7910,  0.8628, -0.2435,\n",
       "            0.5989,  0.9995,  0.2890, -0.5006, -0.5870, -0.3896, -0.9303,\n",
       "           -0.6859,  0.9811, -0.9738, -0.1582,  0.4949,  0.3545, -0.1617,\n",
       "            0.1517, -0.1108, -1.0000,  0.6087, -0.2407,  0.9774,  0.9955,\n",
       "           -0.4851, -0.7966, -0.5602,  0.0292, -0.9223, -0.9682,  0.6004,\n",
       "            0.9927, -0.9915,  0.7544,  0.8171, -0.9837, -0.9498, -0.6843,\n",
       "            0.9508, -0.2427,  0.9973, -0.9621, -0.0514, -0.9182, -0.8869,\n",
       "            0.6051, -0.4623, -0.9638, -0.3993, -0.9731,  0.7304, -0.8505,\n",
       "           -0.9837,  0.4045, -0.8275, -0.9689, -0.3946,  0.2093,  0.0245,\n",
       "            0.9901,  0.1108, -0.9999, -0.8269, -0.9155, -0.9357,  0.3770,\n",
       "            0.7563, -0.9623, -0.7473, -0.3413, -0.8080, -0.5269, -0.8080,\n",
       "            0.0320,  0.1678,  0.8809, -0.8349,  0.5764,  0.3238,  0.3661,\n",
       "           -0.9934,  0.8968,  0.5944, -0.8578, -0.6437, -0.1271,  0.9386,\n",
       "           -0.8627,  0.7812, -0.3274, -0.8070,  0.3846, -0.0596, -0.9969,\n",
       "           -0.3677, -0.5677, -0.4543,  0.5481, -0.8263,  0.8562,  0.9997,\n",
       "            0.9498,  0.9757,  0.9774, -0.9264, -0.9511,  0.1750,  0.9314,\n",
       "           -0.4063, -0.2336, -0.4998,  0.8816,  0.5708, -0.5734, -0.5590,\n",
       "            0.9910,  0.5521, -0.8672, -0.7825]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.3836,  0.9997,  1.0000,  0.9150,  0.9812, -0.7559,  0.4164,\n",
       "            0.9128, -0.9956, -0.9998, -0.6486, -0.8458, -0.6550,  0.3738,\n",
       "            0.6470, -0.9950,  0.5731,  0.4831,  0.9997,  0.7864,  0.8839,\n",
       "            0.9871, -0.9816, -0.9158,  0.3524, -0.3771, -0.9731,  0.0140,\n",
       "           -0.8760,  0.1609,  0.9905, -0.9999, -0.8573, -0.6908, -0.6906,\n",
       "           -0.9072, -0.8901,  0.9093, -0.8237, -0.9986, -0.8683, -0.9965,\n",
       "           -1.0000,  0.0753,  0.3787,  0.6337,  0.9852,  0.9755,  0.9932,\n",
       "            0.9999,  0.0025, -0.9924,  0.1978, -0.8813, -0.9959, -0.7767,\n",
       "           -0.9845,  0.9932,  0.9995,  0.6493,  0.6652,  0.9846, -0.7916,\n",
       "           -0.2355,  0.8661,  0.0016, -0.9911,  0.9931, -0.0729,  0.3089,\n",
       "            0.6509,  0.9948, -0.9925,  0.9953, -0.7806, -0.9310,  0.9659,\n",
       "            0.7447, -0.8114, -0.2620,  0.8772,  0.9424, -0.7037,  0.9592,\n",
       "            0.4994,  0.9096, -0.3379, -0.9934, -0.0055, -0.7460,  0.9218,\n",
       "           -0.5327, -0.9941,  0.2836, -0.9817, -0.9772,  0.9638,  0.1490,\n",
       "           -0.3142, -0.9848,  0.9059,  0.0763, -0.9986,  0.6088,  0.7749,\n",
       "           -0.2788, -0.8042,  0.6491,  0.5964, -0.8665, -0.1498, -0.7275,\n",
       "            0.9999,  0.9520, -0.4405,  0.9940, -0.9934,  0.0507,  0.4002,\n",
       "           -0.7062, -0.8393,  0.8720,  0.4582,  0.9048, -0.1780,  0.0316,\n",
       "            0.9442,  0.9969,  0.7477, -0.8870, -0.9906,  0.8578, -0.9681,\n",
       "           -0.9997,  0.9179,  0.9625, -0.1234, -0.3910,  0.8108,  0.4512,\n",
       "            0.7871, -0.9914,  0.9988, -0.6254, -0.5885, -0.2851, -0.9051,\n",
       "           -0.0059,  0.6432, -0.9738, -0.9997,  0.4477, -0.1342, -0.4838,\n",
       "            0.9433,  0.9995, -0.9996,  0.0755, -0.2246, -0.0918,  0.8924,\n",
       "           -0.9921, -0.8914, -0.5268,  0.1266,  0.6711, -0.9853,  0.7200,\n",
       "            0.9940, -0.9998,  0.9997,  0.9442, -0.9619, -0.9752, -0.7863,\n",
       "            0.9820, -0.1799,  0.9989, -0.9646, -0.9759, -0.9183, -0.8869,\n",
       "            0.7179, -0.6949, -0.9975, -0.5943, -0.9749,  0.7523, -0.9403,\n",
       "           -0.9978, -0.4003, -0.9674, -0.9078, -0.4486, -0.7859,  0.3727,\n",
       "            0.9910, -0.8266, -0.9997,  0.9915, -0.9877, -0.2422,  0.6636,\n",
       "            0.0972, -0.9644,  0.9968, -0.9957, -0.8077, -0.9910, -0.8755,\n",
       "            0.8686,  0.4594,  0.8394, -0.1735,  0.5757,  0.2331, -0.0052,\n",
       "           -0.9100,  0.9739,  0.6307,  0.9980, -0.5053,  0.4597,  0.9525,\n",
       "            0.5812,  0.9118, -0.3063, -0.8580, -0.9768,  0.5444, -0.4153,\n",
       "           -0.9985, -0.6973, -0.7922,  0.3431, -0.9389,  0.7484,  0.9995,\n",
       "            0.8661,  0.7362,  0.9957, -0.6895, -0.9750, -0.1293,  0.8317,\n",
       "           -0.3992,  0.6559, -0.4973,  0.9819, -0.7259,  0.3969, -0.4697,\n",
       "            0.6940,  0.5473,  0.9752, -0.7761]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.4485,  0.9991,  0.9999,  0.9151, -0.9789, -0.7075,  0.2605,\n",
       "            0.9135, -1.0000, -0.9926, -0.6344, -0.8519, -0.6723,  0.4678,\n",
       "            0.6299, -0.9950,  0.5849, -0.9760, -0.9339,  0.7833,  0.9985,\n",
       "            0.9859, -0.9885, -0.9154,  0.9999,  0.5517, -0.9887, -0.9893,\n",
       "            0.9851, -0.8550,  0.9411, -0.9996, -0.9378, -0.7166,  0.9796,\n",
       "            0.9998, -0.9044,  0.9110, -0.8244, -0.9665,  0.9496, -0.7682,\n",
       "            0.9673,  0.6093,  0.0881,  0.6161,  0.9855, -0.1284,  0.9998,\n",
       "            0.8007, -0.6373, -0.9903, -0.7756, -0.8571,  0.9976,  0.9144,\n",
       "           -0.9813,  0.8710,  1.0000,  0.9103,  0.9246,  0.9866, -0.7938,\n",
       "           -0.2363,  0.8781, -0.3777, -0.9900,  0.9992, -0.0613,  0.3127,\n",
       "            0.4788,  0.9918, -0.8948,  0.9956, -0.6921, -0.9392,  0.9383,\n",
       "            0.9976, -0.8104, -0.3406,  0.8591,  0.7391, -0.5879, -0.9203,\n",
       "            0.4549,  0.9041,  0.3286, -0.9942, -0.3002, -0.7141,  0.9314,\n",
       "           -0.5052,  0.9984,  0.5759, -1.0000, -0.9787, -0.4501,  0.1379,\n",
       "           -0.2997, -1.0000,  0.9183,  0.0465, -0.9853,  0.5578,  0.1097,\n",
       "           -0.3089, -0.4201,  0.6305,  0.3105,  0.9537, -0.1559, -0.7290,\n",
       "            0.9998, -0.9319, -0.6595,  0.9485,  0.9946,  0.1174,  0.5083,\n",
       "           -0.7105, -1.0000, -0.9229,  0.3789,  0.9045,  0.3731,  0.9913,\n",
       "            0.9401,  0.9993,  0.7243, -0.8932, -0.5699, -0.7416, -0.9981,\n",
       "            0.3319,  0.3521,  0.9988, -0.1211, -0.4073, -0.9923,  0.3359,\n",
       "            0.9539, -0.9928, -0.1166, -0.6347, -0.5507, -0.2837,  0.9931,\n",
       "            0.4950,  0.7806, -0.4356, -0.9907,  0.4459, -0.1881, -0.4486,\n",
       "            0.9571,  0.7633, -1.0000, -0.0613, -0.8359, -0.9999,  0.9907,\n",
       "           -0.9977, -0.8917, -0.5374,  0.1233,  0.7168,  0.9232,  0.5531,\n",
       "            0.9000,  0.9960, -0.9958,  0.2626, -0.9739, -0.9742, -0.8784,\n",
       "            0.9710, -0.1790,  0.9989, -0.9646,  0.9942, -0.9204, -0.8859,\n",
       "           -0.9329, -0.6885, -0.9983, -0.8432, -0.9899,  0.6879, -0.9063,\n",
       "           -0.8849,  0.9932,  0.4154, -0.9359, -0.5825, -0.3916,  0.1250,\n",
       "            0.9912, -0.9996, -0.9999, -0.9538, -0.9881, -0.8047,  0.7695,\n",
       "            0.8561, -0.9625,  0.9997, -0.9060, -0.8060, -0.9996, -0.8927,\n",
       "            0.1015,  0.6577,  0.9223, -0.1886,  0.5786,  0.2329,  0.0077,\n",
       "           -0.9880,  0.9963,  0.8417,  0.9998, -0.7306, -0.9988,  0.9179,\n",
       "           -0.8504,  0.9093, -0.2607, -0.8639, -0.8744,  0.7912,  0.9982,\n",
       "            0.8820, -0.7135, -0.6986,  0.1770, -0.9924,  0.2802,  0.9990,\n",
       "            0.7867,  0.8086,  0.9971,  0.0329, -0.9749, -0.0319, -0.8787,\n",
       "           -0.3406,  0.5233, -0.4975,  1.0000,  0.2850, -0.3262, -0.4513,\n",
       "           -0.9735, -0.4868, -0.6515, -0.8675]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0246,  0.9994,  1.0000,  0.9155,  0.8860, -0.6451,  0.4944,\n",
       "            0.9066, -0.9999, -0.9998, -0.5132, -0.8236, -0.0610,  0.4360,\n",
       "            0.6524, -0.9944,  0.5619, -0.9347, -0.0882,  0.7864,  0.9195,\n",
       "            0.9825, -0.9871, -0.9150,  0.9999, -0.1365, -0.9894, -0.6936,\n",
       "           -0.3033,  0.2911,  0.9966, -0.9997, -0.9395, -0.5550, -0.6278,\n",
       "            1.0000, -0.7459,  0.9016, -0.7759, -0.9948,  0.9735, -0.9053,\n",
       "           -0.9684,  0.2089, -0.5827,  0.6114,  0.9873,  0.9815,  0.9979,\n",
       "            0.9818,  0.7902, -0.9901, -0.8866, -0.8552,  0.4327, -0.7436,\n",
       "           -0.9489,  0.8954,  0.9990,  0.7096,  0.9243,  0.9884, -0.7667,\n",
       "           -0.2361,  0.7901, -0.1975, -0.9901, -0.9817, -0.0599,  0.3278,\n",
       "            0.3512,  0.9881, -0.7837,  0.9953, -0.8009, -0.9404,  0.9459,\n",
       "            0.9957, -0.8156, -0.2247,  0.8508,  0.8816, -0.6290, -0.3100,\n",
       "            0.4176,  0.8032, -0.9385, -0.9942,  0.4465, -0.8252, -0.9954,\n",
       "           -0.9260, -0.0181, -0.1395,  0.9872, -0.4647,  0.0225,  0.1895,\n",
       "            0.0515, -0.9992,  0.9164,  0.0166,  0.9504, -0.3560,  0.7138,\n",
       "           -0.3122, -0.8356,  0.4063,  0.0942, -0.6593, -0.1715, -0.7324,\n",
       "            0.9991, -0.6137, -0.4723,  0.9494,  0.9829,  0.1796,  0.3672,\n",
       "           -0.7157,  0.5422,  0.9290,  0.3694,  0.9033, -0.9833,  0.6131,\n",
       "            0.7870,  0.9985,  0.6987, -0.8960, -0.8606, -0.8266, -0.7924,\n",
       "           -0.8742,  0.3608,  0.8060, -0.0168, -0.4965,  0.8824,  0.3346,\n",
       "            0.8340,  0.9995,  0.3683, -0.6349, -0.4720, -0.2839, -0.9682,\n",
       "           -0.1975,  0.9710, -0.9817, -0.3821,  0.4452, -0.2327, -0.3973,\n",
       "            0.6366,  0.2668, -1.0000,  0.8064, -0.2856,  0.9837,  0.9903,\n",
       "           -0.6815, -0.7455, -0.4971,  0.1224, -0.9151, -0.9686,  0.5759,\n",
       "            0.9897, -0.9860,  0.7826,  0.7878, -0.9797, -0.9793, -0.9078,\n",
       "            0.9797, -0.1782,  0.9989, -0.9646, -0.4504, -0.9227, -0.9281,\n",
       "            0.3640, -0.6519, -0.9986, -0.5363, -0.9847,  0.7387, -0.8695,\n",
       "           -0.9569,  0.7538, -0.8618, -0.9700, -0.6321, -0.2272,  0.2147,\n",
       "            0.9942, -0.1175, -0.9999, -0.6633, -0.9385, -0.8121,  0.4250,\n",
       "            0.8156, -0.9619, -0.7592, -0.4544, -0.8048, -0.2264, -0.8699,\n",
       "            0.0578,  0.2114,  0.9227, -0.8900,  0.5654,  0.2307,  0.0680,\n",
       "           -0.9886,  0.9005,  0.5170, -0.8018, -0.5866,  0.2061,  0.8583,\n",
       "           -0.8543,  0.8540, -0.3224, -0.8121,  0.4890, -0.4559, -0.9948,\n",
       "           -0.6618, -0.7477, -0.5926,  0.6370, -0.6414,  0.6845,  0.9997,\n",
       "            0.9271,  0.9106,  0.9829, -0.9058, -0.9569,  0.0531,  0.9295,\n",
       "           -0.3430,  0.1450, -0.4988,  0.8839,  0.8012, -0.2844, -0.4307,\n",
       "            0.9914,  0.5296, -0.6492, -0.7981]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 4.9394e-01,  9.9985e-01,  1.0000e+00,  9.1616e-01,  9.6333e-01,\n",
       "           -7.5464e-01,  1.0779e-01,  9.5609e-01, -9.9916e-01, -9.9996e-01,\n",
       "           -5.1308e-01, -8.6828e-01, -7.9639e-01,  7.4919e-01,  6.5596e-01,\n",
       "           -9.9397e-01,  5.7454e-01,  3.3792e-01,  9.9956e-01,  7.8412e-01,\n",
       "            9.7463e-01,  9.8290e-01, -9.8938e-01, -9.6493e-01,  6.6178e-01,\n",
       "           -5.6559e-01, -9.3842e-01,  5.2685e-04, -7.5033e-01,  1.9214e-01,\n",
       "            9.8974e-01, -9.9970e-01, -8.7262e-01, -5.9002e-01, -4.9141e-01,\n",
       "           -8.7848e-01, -8.9505e-01,  9.2248e-01, -7.9520e-01, -9.9645e-01,\n",
       "           -9.2249e-01, -9.9532e-01, -9.9999e-01,  2.0703e-01, -1.9238e-01,\n",
       "            5.3887e-01,  9.9001e-01,  9.7223e-01,  9.9005e-01,  9.9990e-01,\n",
       "           -2.6903e-01, -9.9782e-01,  1.0752e-01, -8.7748e-01, -9.9485e-01,\n",
       "           -7.4840e-01, -9.9081e-01,  9.8558e-01,  9.9977e-01,  8.7683e-01,\n",
       "            6.0809e-01,  9.9372e-01, -8.5971e-01, -2.6117e-01,  9.0973e-01,\n",
       "           -3.8330e-01, -9.9560e-01,  9.9444e-01, -7.7577e-02,  3.3572e-01,\n",
       "            6.5597e-01,  9.9517e-01, -9.9360e-01,  9.9686e-01, -8.6455e-01,\n",
       "           -9.4612e-01,  9.5496e-01,  8.1708e-01, -8.1640e-01, -3.0278e-01,\n",
       "            8.5854e-01,  9.8330e-01, -5.9198e-01,  9.7597e-01,  3.5778e-01,\n",
       "            9.1832e-01, -3.4360e-01, -9.9176e-01, -1.4108e-01, -7.7392e-01,\n",
       "            9.5056e-01, -5.4216e-01, -9.8690e-01,  3.5395e-01, -9.7649e-01,\n",
       "           -9.7591e-01,  9.5055e-01, -8.1536e-03, -7.4317e-02, -9.9317e-01,\n",
       "            9.4246e-01,  4.3359e-01, -9.9796e-01,  6.8031e-01,  7.3555e-01,\n",
       "           -2.6578e-01, -6.0087e-01,  8.1671e-01,  2.2672e-01, -8.4043e-01,\n",
       "           -1.7493e-01, -7.0042e-01,  9.9993e-01,  9.0599e-01, -4.5189e-01,\n",
       "            9.6808e-01, -9.7750e-01,  4.5258e-01,  4.2965e-01, -7.2558e-01,\n",
       "           -9.3681e-01,  8.3709e-01,  3.6198e-01,  8.7519e-01,  4.1369e-01,\n",
       "            6.1946e-01,  9.5080e-01,  9.9860e-01,  7.7966e-01, -9.0264e-01,\n",
       "           -9.8991e-01,  8.0929e-01, -9.5886e-01, -9.9911e-01,  9.1198e-01,\n",
       "            9.3183e-01, -3.8525e-02, -4.4087e-02,  7.9739e-01,  3.8907e-01,\n",
       "            9.2488e-01, -9.8281e-01,  9.9882e-01, -7.1388e-01, -4.7524e-01,\n",
       "           -1.9097e-01, -7.1172e-01,  2.4522e-01,  7.4219e-01, -9.8164e-01,\n",
       "           -9.9977e-01,  3.7050e-01, -5.8538e-01, -5.6599e-01,  9.7197e-01,\n",
       "            9.9986e-01, -9.9976e-01,  8.4249e-02, -2.7748e-01, -5.9350e-01,\n",
       "            8.2423e-01, -9.9571e-01, -8.1677e-01, -4.6994e-01,  1.4467e-01,\n",
       "            7.3755e-01, -9.8335e-01,  7.2428e-01,  9.9111e-01, -9.9965e-01,\n",
       "            9.9857e-01,  9.5232e-01, -9.2870e-01, -9.9149e-01, -8.9096e-01,\n",
       "            9.8287e-01, -9.2157e-02,  9.9946e-01, -9.6787e-01, -9.6518e-01,\n",
       "           -9.2291e-01, -9.2805e-01,  7.7735e-01, -7.5411e-01, -9.9990e-01,\n",
       "           -6.3921e-01, -9.8529e-01,  7.4924e-01, -9.0647e-01, -9.9769e-01,\n",
       "           -4.5610e-01, -9.5721e-01, -9.4954e-01, -6.4712e-01, -7.5904e-01,\n",
       "            5.1676e-01,  9.9433e-01, -9.1373e-01, -9.9982e-01,  9.9102e-01,\n",
       "           -9.8859e-01, -3.3943e-01,  7.0824e-01, -1.9662e-01, -9.6864e-01,\n",
       "            9.9783e-01, -9.9563e-01, -8.0453e-01, -9.9161e-01, -9.1353e-01,\n",
       "            8.5010e-01,  6.7366e-01,  8.3545e-01, -3.5997e-01,  5.6518e-01,\n",
       "            1.5980e-01, -2.6197e-01, -9.2991e-01,  9.7189e-01,  5.6120e-01,\n",
       "            9.9786e-01, -5.6900e-01, -3.4188e-01,  8.7822e-01,  1.3882e-01,\n",
       "            9.4252e-01, -3.0683e-01, -8.7573e-01, -9.9213e-01,  4.4817e-01,\n",
       "           -3.2737e-01, -9.9663e-01, -8.4607e-01, -8.8521e-01,  4.0666e-01,\n",
       "           -8.7899e-01,  6.1876e-01,  9.9898e-01,  7.6128e-01,  7.0341e-01,\n",
       "            9.9591e-01, -4.6306e-01, -9.7006e-01, -4.5883e-01,  6.1967e-01,\n",
       "           -3.3444e-01,  6.0426e-01, -4.9469e-01,  9.8869e-01, -6.9706e-01,\n",
       "            6.3623e-01, -4.1174e-01,  6.3822e-01,  5.2304e-01,  9.3947e-01,\n",
       "           -7.9285e-01]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.3809,  0.9995,  0.9999,  0.9163, -0.9751, -0.6968,  0.0643,\n",
       "            0.9563, -1.0000, -0.9877, -0.4956, -0.8718, -0.6662,  0.7758,\n",
       "            0.5588, -0.9940,  0.5874, -0.9802, -0.9487,  0.7810,  0.9997,\n",
       "            0.9824, -0.9918, -0.9645,  0.9999,  0.5158, -0.9734, -0.9917,\n",
       "            0.9789, -0.8731,  0.9442, -0.9994, -0.9560, -0.6292,  0.9921,\n",
       "            0.9997, -0.9073,  0.9232, -0.7942, -0.9752,  0.9168, -0.7619,\n",
       "            0.9672,  0.7140, -0.4588,  0.5112,  0.9901, -0.3485,  0.9997,\n",
       "            0.4168, -0.5888, -0.9969, -0.7885, -0.8588,  0.9989,  0.9197,\n",
       "           -0.9884,  0.8598,  1.0000,  0.9575,  0.8821,  0.9942, -0.8607,\n",
       "           -0.2616,  0.9195, -0.6414, -0.9947,  0.9981, -0.0687,  0.3389,\n",
       "            0.4108,  0.9934, -0.9203,  0.9970, -0.7873, -0.9472,  0.9512,\n",
       "            0.9987, -0.8156, -0.4586,  0.8490,  0.8302, -0.3898, -0.9362,\n",
       "            0.3070,  0.9098,  0.2111, -0.9923, -0.4235, -0.6434,  0.9502,\n",
       "           -0.4938,  0.9981,  0.4952, -1.0000, -0.9769, -0.7085, -0.0259,\n",
       "           -0.0659, -1.0000,  0.9510,  0.4063, -0.9809,  0.5688, -0.0520,\n",
       "           -0.3265, -0.3645,  0.7735, -0.0183,  0.9668, -0.1798, -0.7020,\n",
       "            0.9998, -0.8774, -0.7826,  0.9655,  0.9956,  0.4780,  0.5919,\n",
       "           -0.7285, -0.9999, -0.9407,  0.2771,  0.8751,  0.0842,  0.9948,\n",
       "            0.9402,  0.9992,  0.7291, -0.9071, -0.7690, -0.6914, -0.9974,\n",
       "            0.4474,  0.5138,  0.9978, -0.0396, -0.0735, -0.9927,  0.2980,\n",
       "            0.9842, -0.9873, -0.2755, -0.7174, -0.4457, -0.1899,  0.9966,\n",
       "            0.6729,  0.8586, -0.4989, -0.9737,  0.3692, -0.5898, -0.5482,\n",
       "            0.9657,  0.3314, -1.0000, -0.0463, -0.9496, -0.9999,  0.9862,\n",
       "           -0.9979, -0.7798, -0.4844,  0.1431,  0.7807,  0.9230,  0.5568,\n",
       "            0.8621,  0.9949, -0.9946,  0.3433, -0.9453, -0.9886, -0.9295,\n",
       "            0.9778, -0.0916,  0.9994, -0.9680,  0.9967, -0.9258, -0.9259,\n",
       "           -0.9333, -0.7494, -0.9999, -0.8809, -0.9948,  0.7082, -0.8844,\n",
       "           -0.9147,  0.9904,  0.2887, -0.9632, -0.7285, -0.3958,  0.2325,\n",
       "            0.9944, -0.9998, -0.9999, -0.9634, -0.9887, -0.7947,  0.7912,\n",
       "            0.8178, -0.9685,  0.9996, -0.8107, -0.8026, -0.9997, -0.9218,\n",
       "            0.1594,  0.8008,  0.9224, -0.3694,  0.5689,  0.1595, -0.2486,\n",
       "           -0.9873,  0.9960,  0.7664,  0.9998, -0.8171, -0.9973,  0.8979,\n",
       "           -0.5771,  0.9420, -0.2759, -0.8800, -0.9659,  0.6916,  0.9993,\n",
       "            0.8882, -0.8269, -0.8347,  0.2078, -0.9846,  0.2077,  0.9986,\n",
       "            0.3324,  0.7507,  0.9972,  0.5471, -0.9700, -0.3133, -0.8101,\n",
       "           -0.3038,  0.4770, -0.4952,  1.0000,  0.0875, -0.3519, -0.4034,\n",
       "           -0.9010, -0.3335, -0.8265, -0.8746]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0611,  0.9997,  1.0000,  0.9167,  0.8982, -0.6196,  0.2541,\n",
       "            0.9522, -0.9999, -0.9997, -0.4183, -0.8454, -0.0284,  0.6580,\n",
       "            0.5793, -0.9969,  0.5735, -0.9250, -0.7056,  0.7847,  0.9722,\n",
       "            0.9795, -0.9910, -0.9640,  0.9999, -0.2421, -0.9772, -0.6181,\n",
       "           -0.1344,  0.3105,  0.9985, -0.9996, -0.9565, -0.4205, -0.6807,\n",
       "            1.0000, -0.7239,  0.9040, -0.7052, -0.9946,  0.9422, -0.8694,\n",
       "           -0.9400,  0.1533, -0.6820,  0.5246,  0.9913,  0.9606,  0.9985,\n",
       "            0.9555,  0.8236, -0.9968, -0.8906, -0.8583,  0.8010, -0.8007,\n",
       "           -0.9558,  0.8898,  0.9990,  0.7719,  0.8935,  0.9949, -0.8415,\n",
       "           -0.2615,  0.8150, -0.4056, -0.9947, -0.9837, -0.0673,  0.3559,\n",
       "            0.2545,  0.9909, -0.8382,  0.9961, -0.8790, -0.9395,  0.9566,\n",
       "            0.9976, -0.8192, -0.2845,  0.8359,  0.9311, -0.2966, -0.3630,\n",
       "            0.2782,  0.7617, -0.9405, -0.9922,  0.1964, -0.8666, -0.9945,\n",
       "           -0.8304, -0.1011, -0.2263,  0.9920, -0.3774, -0.2880,  0.0337,\n",
       "            0.2457, -0.9994,  0.9210,  0.3715,  0.9533, -0.5316,  0.7180,\n",
       "           -0.3877, -0.8000,  0.4282, -0.1234, -0.7019, -0.1911, -0.7079,\n",
       "            0.9995, -0.7116, -0.6788,  0.9659,  0.9643,  0.4684,  0.2171,\n",
       "           -0.7349,  0.3181,  0.9150,  0.2676,  0.8740, -0.9959,  0.6224,\n",
       "            0.7167,  0.9969,  0.5362, -0.9078, -0.8394, -0.7849, -0.7839,\n",
       "           -0.8794,  0.5042,  0.7750,  0.0999, -0.2938,  0.7627,  0.2932,\n",
       "            0.7840,  0.9996,  0.2738, -0.7178, -0.3445, -0.1901, -0.9184,\n",
       "           -0.1157,  0.9860, -0.9780, -0.1208,  0.3685, -0.6007, -0.4898,\n",
       "            0.0100, -0.1142, -1.0000,  0.6565, -0.3284,  0.9882,  0.9855,\n",
       "           -0.7789, -0.4775, -0.4421,  0.1418, -0.9293, -0.9777,  0.5775,\n",
       "            0.9852, -0.9904,  0.3193,  0.7266, -0.9578, -0.9907, -0.9523,\n",
       "            0.9786, -0.0908,  0.9989, -0.9680,  0.4897, -0.9277, -0.8983,\n",
       "            0.5283, -0.7150, -0.9999, -0.4520, -0.9845,  0.7501, -0.8511,\n",
       "           -0.9681,  0.6283, -0.8977, -0.9763, -0.7281, -0.2749,  0.3063,\n",
       "            0.9963, -0.3389, -0.9999, -0.8534, -0.9269, -0.8033,  0.4275,\n",
       "            0.8188, -0.9681, -0.7844, -0.4067, -0.8009, -0.2012, -0.8975,\n",
       "            0.0859,  0.1466,  0.9226, -0.8664,  0.5489,  0.1648, -0.1562,\n",
       "           -0.9878,  0.8969,  0.4099, -0.7714, -0.6717,  0.1236,  0.8972,\n",
       "           -0.5832,  0.9179, -0.3082, -0.7972, -0.0098, -0.0365, -0.9939,\n",
       "           -0.4682, -0.8513, -0.7099,  0.6937, -0.7237,  0.7543,  0.9995,\n",
       "            0.8007,  0.8910,  0.9799, -0.8931, -0.9581, -0.1934,  0.9249,\n",
       "           -0.3065, -0.1028, -0.4966,  0.8804,  0.6758, -0.5665, -0.3861,\n",
       "            0.9843,  0.5709, -0.6330, -0.7767]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4702,  0.9999,  1.0000,  0.9173,  0.9793, -0.8534,  0.2360,\n",
       "            0.9856, -0.9972, -0.9999, -0.4182, -0.8746, -0.6815,  0.8067,\n",
       "            0.5762, -0.9965,  0.5903,  0.4906,  0.9993,  0.8175,  0.9805,\n",
       "            0.9822, -0.9939, -0.9873,  0.2723, -0.5354, -0.9319,  0.0376,\n",
       "           -0.7292,  0.2658,  0.9891, -0.9999, -0.9363, -0.4613, -0.5755,\n",
       "           -0.8274, -0.8935,  0.9255, -0.7088, -0.9978, -0.7758, -0.9953,\n",
       "           -1.0000,  0.1525, -0.3308,  0.5397,  0.9925,  0.9446,  0.9920,\n",
       "            0.9998, -0.0652, -0.9993, -0.2154, -0.8785, -0.9802, -0.8110,\n",
       "           -0.9947,  0.9905,  0.9998,  0.7881,  0.6792,  0.9964, -0.8878,\n",
       "           -0.2978,  0.9078, -0.4990, -0.9981,  0.9906, -0.0850,  0.3674,\n",
       "            0.7002,  0.9978, -0.9954,  0.9974, -0.8708, -0.9466,  0.9659,\n",
       "            0.8548, -0.8200, -0.3126,  0.8413,  0.9809, -0.2819,  0.9693,\n",
       "            0.2623,  0.9143, -0.4324, -0.9899, -0.3032, -0.8010,  0.9310,\n",
       "           -0.2121, -0.9909,  0.2413, -0.9754, -0.9802,  0.8994, -0.2800,\n",
       "            0.0049, -0.9833,  0.9483,  0.5384, -0.9986,  0.5757,  0.8209,\n",
       "           -0.3649, -0.8681,  0.7588,  0.1554, -0.8634, -0.1935, -0.6465,\n",
       "            1.0000,  0.9499, -0.4347,  0.9701, -0.9642,  0.2575,  0.2784,\n",
       "           -0.7486, -0.8903,  0.8863,  0.2621,  0.8491,  0.4563,  0.3756,\n",
       "            0.9373,  0.9970,  0.7651, -0.9221, -0.9918,  0.8675, -0.9677,\n",
       "           -0.9994,  0.9148,  0.8884,  0.0793, -0.1362,  0.7257,  0.5015,\n",
       "            0.8239, -0.9862,  0.9989, -0.7913, -0.3479, -0.0767, -0.9025,\n",
       "            0.1307,  0.7980, -0.9779, -0.9997,  0.2877, -0.7504, -0.6476,\n",
       "            0.9156,  0.9997, -0.9999,  0.1869, -0.3180, -0.3595,  0.6939,\n",
       "           -0.9982, -0.7819, -0.4730,  0.2651,  0.8407, -0.9876,  0.6981,\n",
       "            0.9924, -0.9998,  0.9993,  0.9299, -0.9431, -0.9878, -0.9124,\n",
       "            0.9866, -0.0309,  0.9993, -0.9700, -0.9500, -0.9278, -0.8983,\n",
       "            0.8845, -0.8364, -1.0000, -0.5583, -0.9840,  0.7586, -0.9212,\n",
       "           -0.9963, -0.3699, -0.9754, -0.9138, -0.7547, -0.6594,  0.6304,\n",
       "            0.9955, -0.9241, -0.9999,  0.9954, -0.9906, -0.3579,  0.6855,\n",
       "           -0.1909, -0.9761,  0.9970, -0.9954, -0.8008, -0.9895, -0.9375,\n",
       "            0.9341,  0.5712,  0.8269, -0.1747,  0.5479,  0.0857, -0.3662,\n",
       "           -0.9320,  0.9776,  0.4916,  0.9982, -0.6679,  0.0484,  0.9145,\n",
       "            0.4095,  0.9731, -0.2955, -0.8566, -0.9928,  0.5296, -0.6540,\n",
       "           -0.9986, -0.9163, -0.9067,  0.5129, -0.9330,  0.6062,  0.9995,\n",
       "            0.6757,  0.7784,  0.9960, -0.4714, -0.9809, -0.4642,  0.7728,\n",
       "           -0.3007,  0.4947, -0.4935,  0.9725, -0.7488,  0.5003, -0.3915,\n",
       "            0.7449,  0.5670,  0.9586, -0.6729]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.4070,  0.9995,  0.9999,  0.9174, -0.9718, -0.8240,  0.1895,\n",
       "            0.9856, -1.0000, -0.9907, -0.4077, -0.8773, -0.6400,  0.8224,\n",
       "            0.4905, -0.9965,  0.5967, -0.9835, -0.9376,  0.8132,  0.9998,\n",
       "            0.9820, -0.9946, -0.9867,  0.9999,  0.3743, -0.9637, -0.9869,\n",
       "            0.9769, -0.8163,  0.9544, -0.9995, -0.9361, -0.5100,  0.9911,\n",
       "            0.9998, -0.9079,  0.9260, -0.7144, -0.9733,  0.9432, -0.8038,\n",
       "            0.9618,  0.6377, -0.5101,  0.5311,  0.9926, -0.5135,  0.9997,\n",
       "            0.4611, -0.6392, -0.9978, -0.8036, -0.8626,  0.9984,  0.8983,\n",
       "           -0.9911,  0.8586,  1.0000,  0.9034,  0.9094,  0.9966, -0.8887,\n",
       "           -0.2984,  0.9173, -0.6740, -0.9973,  0.9993, -0.0782,  0.3708,\n",
       "            0.4549,  0.9958, -0.9441,  0.9975, -0.7509, -0.9487,  0.9603,\n",
       "            0.9985, -0.8198, -0.4186,  0.8343,  0.7725, -0.1963, -0.9073,\n",
       "            0.2379,  0.8895,  0.2283, -0.9909, -0.5061, -0.7423,  0.9423,\n",
       "           -0.1777,  0.9978,  0.7259, -1.0000, -0.9810, -0.6020, -0.2978,\n",
       "            0.0071, -1.0000,  0.9568,  0.5061, -0.9858,  0.5180, -0.1916,\n",
       "           -0.3931, -0.6206,  0.7361, -0.0096,  0.9592, -0.1971, -0.6488,\n",
       "            0.9998, -0.9147, -0.7667,  0.9380,  0.9947,  0.2969,  0.4146,\n",
       "           -0.7524, -1.0000, -0.8905,  0.2058,  0.8489,  0.2085,  0.9917,\n",
       "            0.9262,  0.9994,  0.7376, -0.9265, -0.8131, -0.6417, -0.9985,\n",
       "            0.1647,  0.5316,  0.9952,  0.0764, -0.1525, -0.9883,  0.4131,\n",
       "            0.9587, -0.9886,  0.0365, -0.7942, -0.3314, -0.0759,  0.9957,\n",
       "            0.4905,  0.8631, -0.5202, -0.9895,  0.2861, -0.7423, -0.6512,\n",
       "            0.9305,  0.5857, -1.0000,  0.0791, -0.9435, -0.9999,  0.9860,\n",
       "           -0.9987, -0.7673, -0.4826,  0.2627,  0.8702,  0.8880,  0.5957,\n",
       "            0.8769,  0.9931, -0.9940,  0.3000, -0.9545, -0.9838, -0.9374,\n",
       "            0.9781, -0.0303,  0.9994, -0.9701,  0.9976, -0.9293, -0.8976,\n",
       "           -0.9347, -0.8293, -1.0000, -0.7885, -0.9926,  0.7362, -0.8895,\n",
       "           -0.9079,  0.9887,  0.2182, -0.9398, -0.7972, -0.4715,  0.1863,\n",
       "            0.9955, -0.9998, -1.0000, -0.9704, -0.9906, -0.7850,  0.7628,\n",
       "            0.8290, -0.9763,  0.9996, -0.8619, -0.7996, -0.9998, -0.9436,\n",
       "           -0.0048,  0.7602,  0.9196, -0.1897,  0.5510,  0.0854, -0.3487,\n",
       "           -0.9869,  0.9961,  0.7682,  0.9999, -0.8028, -0.9975,  0.9236,\n",
       "           -0.5578,  0.9729, -0.2762, -0.8617, -0.9758,  0.8142,  0.9979,\n",
       "            0.9195, -0.9049, -0.8411,  0.3453, -0.9934,  0.2739,  0.9992,\n",
       "            0.3544,  0.8094,  0.9972,  0.4103, -0.9809, -0.3734, -0.7958,\n",
       "           -0.2582,  0.4304, -0.4940,  1.0000,  0.0232, -0.2796, -0.3874,\n",
       "           -0.9382, -0.2570, -0.7420, -0.8452]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0179,  0.9996,  1.0000,  0.9177,  0.8111, -0.7558,  0.2432,\n",
       "            0.9805, -0.9999, -0.9998, -0.3477, -0.8616,  0.0862,  0.7693,\n",
       "            0.5098, -0.9974,  0.5926, -0.9313, -0.6861,  0.8164,  0.9692,\n",
       "            0.9786, -0.9943, -0.9862,  0.9999, -0.1650, -0.9763, -0.6514,\n",
       "            0.0363,  0.0484,  0.9975, -0.9996, -0.9373, -0.3860, -0.6642,\n",
       "            1.0000, -0.7124,  0.9088, -0.6542, -0.9942,  0.9663, -0.8207,\n",
       "           -0.8769,  0.1667, -0.7295,  0.5383,  0.9934,  0.9684,  0.9986,\n",
       "            0.9622,  0.7445, -0.9976, -0.8890, -0.8617,  0.8251, -0.8240,\n",
       "           -0.9534,  0.8900,  0.9996,  0.7881,  0.9148,  0.9970, -0.8731,\n",
       "           -0.2982,  0.7651, -0.4688, -0.9972, -0.9742, -0.0773,  0.3860,\n",
       "            0.1838,  0.9915, -0.9001,  0.9967, -0.8527, -0.9513,  0.9615,\n",
       "            0.9972, -0.8221, -0.3585,  0.8200,  0.9025, -0.3823, -0.1670,\n",
       "            0.2271,  0.7828, -0.9356, -0.9908,  0.3985, -0.8782, -0.9968,\n",
       "           -0.7629, -0.0881, -0.0717,  0.9830, -0.4668, -0.2517, -0.2363,\n",
       "            0.2315, -0.9992,  0.9328,  0.4686,  0.9644, -0.3703,  0.6109,\n",
       "           -0.3776, -0.7402,  0.5536, -0.0852, -0.5457, -0.2049, -0.6518,\n",
       "            0.9994, -0.7622, -0.5936,  0.9388,  0.9731,  0.3202,  0.4621,\n",
       "           -0.7575,  0.0183,  0.8868,  0.1968,  0.8481, -0.9926,  0.7371,\n",
       "            0.7031,  0.9981,  0.4884, -0.9243, -0.8192, -0.7911, -0.8962,\n",
       "           -0.8299,  0.3064,  0.8410,  0.1676, -0.3153,  0.7318,  0.4099,\n",
       "            0.7574,  0.9993,  0.1565, -0.7921, -0.2732, -0.0762, -0.9502,\n",
       "            0.0117,  0.9830, -0.9491, -0.1994,  0.2855, -0.7387, -0.6118,\n",
       "            0.0843,  0.1939, -1.0000,  0.6222, -0.1088,  0.9914,  0.9821,\n",
       "           -0.8368, -0.5363, -0.4524,  0.2612, -0.9102, -0.9706,  0.6119,\n",
       "            0.9864, -0.9888,  0.5238,  0.7153, -0.9629, -0.9863, -0.9495,\n",
       "            0.9811, -0.0298,  0.9991, -0.9701,  0.4552, -0.9304, -0.8653,\n",
       "            0.3504, -0.7948, -0.9999, -0.4916, -0.9890,  0.7613, -0.8348,\n",
       "           -0.9524,  0.6495, -0.8746, -0.9685, -0.7769, -0.4000,  0.2872,\n",
       "            0.9965, -0.5853, -1.0000, -0.8825, -0.9349, -0.7985,  0.4306,\n",
       "            0.8658, -0.9762, -0.7566, -0.4309, -0.7987, -0.0349, -0.9218,\n",
       "           -0.0458,  0.2094,  0.9200, -0.8906,  0.5358,  0.0870, -0.2579,\n",
       "           -0.9874,  0.9155,  0.1859, -0.6310, -0.6981,  0.1283,  0.8149,\n",
       "           -0.5651,  0.9450, -0.2964, -0.7988,  0.0316, -0.2281, -0.9935,\n",
       "           -0.4693, -0.9170, -0.7270,  0.7307, -0.8426,  0.7303,  0.9997,\n",
       "            0.7889,  0.9018,  0.9852, -0.8584, -0.9630, -0.2918,  0.9158,\n",
       "           -0.2612, -0.1404, -0.4950,  0.8978,  0.7548, -0.6730, -0.3720,\n",
       "            0.9760,  0.6113, -0.5410, -0.7107]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.4409,  0.9998,  1.0000,  0.9181,  0.9780, -0.8768,  0.1426,\n",
       "            0.9928, -0.9979, -0.9999, -0.3476, -0.8757, -0.8178,  0.8584,\n",
       "            0.5214, -0.9966,  0.6013,  0.4647,  0.9989,  0.8375,  0.9767,\n",
       "            0.9796, -0.9959, -0.9942,  0.2622, -0.4476, -0.9179, -0.1179,\n",
       "           -0.8369, -0.0035,  0.9931, -0.9999, -0.9229, -0.4331, -0.7310,\n",
       "           -0.8855, -0.8744,  0.9256, -0.6480, -0.9978, -0.7950, -0.9943,\n",
       "           -1.0000,  0.1656, -0.3955,  0.5581,  0.9937,  0.9516,  0.9850,\n",
       "            0.9998, -0.0227, -0.9993, -0.0964, -0.8732, -0.9809, -0.8298,\n",
       "           -0.9914,  0.9901,  0.9998,  0.7456,  0.6355,  0.9978, -0.8893,\n",
       "           -0.3281,  0.9144, -0.5480, -0.9989,  0.9909, -0.1046,  0.3962,\n",
       "            0.6602,  0.9981, -0.9969,  0.9975, -0.8853, -0.9578,  0.9674,\n",
       "            0.8555, -0.8225, -0.3704,  0.8226,  0.9822, -0.3392,  0.9650,\n",
       "            0.2159,  0.9132, -0.4909, -0.9868, -0.2597, -0.7955,  0.9188,\n",
       "            0.0553, -0.9934,  0.2361, -0.9538, -0.9776,  0.8803, -0.3787,\n",
       "            0.0571, -0.9705,  0.9501,  0.5363, -0.9984,  0.5861,  0.8239,\n",
       "           -0.3543, -0.8567,  0.8028, -0.0590, -0.9148, -0.2074, -0.5872,\n",
       "            0.9999,  0.9615, -0.2721,  0.9453, -0.9644,  0.1994,  0.4890,\n",
       "           -0.7647, -0.8200,  0.8502,  0.1935,  0.8218,  0.6515,  0.4174,\n",
       "            0.9259,  0.9981,  0.7598, -0.9210, -0.9919,  0.8432, -0.9745,\n",
       "           -0.9991,  0.8909,  0.9099,  0.1441, -0.0572,  0.6741,  0.4737,\n",
       "            0.8367, -0.9807,  0.9986, -0.8395, -0.2766,  0.0239, -0.8907,\n",
       "            0.2276,  0.7925, -0.9491, -0.9997,  0.2101, -0.8047, -0.6868,\n",
       "            0.9041,  0.9998, -0.9999,  0.1676, -0.0925, -0.4355,  0.6385,\n",
       "           -0.9974, -0.7372, -0.4722,  0.3057,  0.8040, -0.9868,  0.7203,\n",
       "            0.9943, -0.9998,  0.9996,  0.9234, -0.9395, -0.9874, -0.9199,\n",
       "            0.9775,  0.0150,  0.9993, -0.9719, -0.9622, -0.9305, -0.8653,\n",
       "            0.9092, -0.8633, -1.0000, -0.6036, -0.9882,  0.7684, -0.9042,\n",
       "           -0.9963, -0.2274, -0.9703, -0.9242, -0.8034, -0.6428,  0.6061,\n",
       "            0.9949, -0.9506, -0.9998,  0.9927, -0.9899, -0.3519,  0.5954,\n",
       "           -0.1035, -0.9817,  0.9948, -0.9949, -0.7986, -0.9817, -0.9513,\n",
       "            0.9426,  0.5427,  0.8374, -0.2167,  0.5330, -0.0394, -0.4295,\n",
       "           -0.9708,  0.9759,  0.2822,  0.9987, -0.6521,  0.1811,  0.8468,\n",
       "            0.5783,  0.9791, -0.2873, -0.8316, -0.9930,  0.3769, -0.7459,\n",
       "           -0.9982, -0.9539, -0.9008,  0.5157, -0.9514,  0.6106,  0.9993,\n",
       "            0.6931,  0.7074,  0.9955, -0.4199, -0.9819, -0.5290,  0.7859,\n",
       "           -0.2561,  0.5050, -0.4925,  0.9554, -0.8083,  0.6901, -0.3451,\n",
       "            0.6153,  0.6073,  0.9251, -0.6418]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.3355,  0.9994,  0.9999,  0.9181, -0.9652, -0.8285,  0.0961,\n",
       "            0.9928, -1.0000, -0.9869, -0.3404, -0.8782, -0.5804,  0.8698,\n",
       "            0.4426, -0.9967,  0.6076, -0.9851, -0.9348,  0.8327,  0.9997,\n",
       "            0.9792, -0.9962, -0.9938,  0.9998,  0.3596, -0.9559, -0.9877,\n",
       "            0.9631, -0.8849,  0.9596, -0.9994, -0.9304, -0.4889,  0.9913,\n",
       "            0.9998, -0.8924,  0.9261, -0.6552, -0.9727,  0.8902, -0.8091,\n",
       "            0.9664,  0.6981, -0.5520,  0.5392,  0.9937, -0.5276,  0.9992,\n",
       "            0.2526, -0.5674, -0.9981, -0.7561, -0.8573,  0.9987,  0.9038,\n",
       "           -0.9876,  0.8889,  1.0000,  0.8744,  0.8938,  0.9969, -0.8905,\n",
       "           -0.3287,  0.9227, -0.7390, -0.9983,  0.9994, -0.0967,  0.3995,\n",
       "            0.4278,  0.9965, -0.9473,  0.9976, -0.7590, -0.9589,  0.9637,\n",
       "            0.9982, -0.8223, -0.4723,  0.8097,  0.7483, -0.2577, -0.9234,\n",
       "            0.1852,  0.8826,  0.2045, -0.9879, -0.4870, -0.7192,  0.9355,\n",
       "            0.0874,  0.9976,  0.6680, -1.0000, -0.9785, -0.6812, -0.3853,\n",
       "            0.0590, -1.0000,  0.9594,  0.4981, -0.9875,  0.5430, -0.2275,\n",
       "           -0.3875, -0.6615,  0.7730, -0.1231,  0.9563, -0.2105, -0.5898,\n",
       "            0.9998, -0.9009, -0.6945,  0.8382,  0.9897,  0.2447,  0.5601,\n",
       "           -0.7679, -1.0000, -0.9064,  0.1340,  0.8217,  0.0111,  0.9913,\n",
       "            0.9160,  0.9991,  0.7425, -0.9256, -0.8367, -0.5487, -0.9985,\n",
       "           -0.0412,  0.5946,  0.9955,  0.1330, -0.0748, -0.9903,  0.3969,\n",
       "            0.9492, -0.9845, -0.0259, -0.8420, -0.2599,  0.0244,  0.9976,\n",
       "            0.5394,  0.8557, -0.4398, -0.9849,  0.2084, -0.7916, -0.6897,\n",
       "            0.9157,  0.5573, -1.0000,  0.0951, -0.9207, -0.9999,  0.9747,\n",
       "           -0.9984, -0.7231, -0.4833,  0.3034,  0.8407,  0.8583,  0.6313,\n",
       "            0.8804,  0.9935, -0.9954,  0.1235, -0.9533, -0.9853, -0.9491,\n",
       "            0.9791,  0.0155,  0.9992, -0.9719,  0.9981, -0.9316, -0.8648,\n",
       "           -0.9436, -0.8553, -1.0000, -0.8140, -0.9936,  0.7510, -0.8703,\n",
       "           -0.9414,  0.9871,  0.0073, -0.9434, -0.8333, -0.5155,  0.2501,\n",
       "            0.9949, -0.9999, -0.9999, -0.9748, -0.9901, -0.7194,  0.6867,\n",
       "            0.8040, -0.9819,  0.9996, -0.8647, -0.7975, -0.9998, -0.9558,\n",
       "           -0.1019,  0.7614,  0.9186, -0.2288,  0.5361, -0.0397, -0.4091,\n",
       "           -0.9881,  0.9963,  0.6850,  0.9998, -0.7821, -0.9978,  0.8855,\n",
       "           -0.4421,  0.9788, -0.2700, -0.8364, -0.9778,  0.7595,  0.9972,\n",
       "            0.9282, -0.9272, -0.8320,  0.3309, -0.9938,  0.3354,  0.9989,\n",
       "            0.3113,  0.7558,  0.9969,  0.4409, -0.9819, -0.4397, -0.8460,\n",
       "           -0.2235,  0.4564, -0.4930,  1.0000, -0.0669, -0.2353, -0.3432,\n",
       "           -0.9543, -0.0632, -0.7281, -0.8377]]], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([\"ba\"])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
