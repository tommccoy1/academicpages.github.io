{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        ignore_wi = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wf = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wg = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        ignore_wo = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "        self.wi_weights = V(ignore_wi.weight.data, requires_grad=True)\n",
    "        self.wi_bias = V(ignore_wi.bias.data, requires_grad=True)\n",
    "        self.wf_weights = V(ignore_wf.weight.data, requires_grad=True)\n",
    "        self.wf_bias = V(ignore_wf.bias.data, requires_grad=True)\n",
    "        self.wg_weights = V(ignore_wg.weight.data, requires_grad=True)\n",
    "        self.wg_bias = V(ignore_wg.bias.data, requires_grad=True)\n",
    "        self.wo_weights = V(ignore_wo.weight.data, requires_grad=True)\n",
    "        self.wo_bias = V(ignore_wo.bias.data, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        input_plus_hidden = torch.cat((inp, hx), 2)\n",
    "        \n",
    "        \n",
    "        #myhook = hx.register_hook(print_grad)\n",
    "\n",
    "\n",
    "        i_tpre = F.linear(input_plus_hidden, self.wi_weights, self.wi_bias)\n",
    "        i_t = torch.sigmoid(i_tpre)\n",
    "        f_tpre = F.linear(input_plus_hidden, self.wf_weights, self.wf_bias)\n",
    "        f_t = torch.sigmoid(f_tpre)\n",
    "        g_tpre = F.linear(input_plus_hidden, self.wg_weights, self.wg_bias)\n",
    "        g_t = torch.tanh(g_tpre)\n",
    "        fred = F.linear(input_plus_hidden, self.wo_weights, self.wo_bias)\n",
    "        o_t = torch.sigmoid(fred)\n",
    "        #o_t = torch.sigmoid(F.linear(input_plus_hidden, self.wo_weights, self.wo_bias))\n",
    "        \n",
    "        #myhook = f_tpre.register_hook(print_grad)\n",
    "\n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * torch.tanh(cx)\n",
    "\n",
    "        #if cx.requires_grad:\n",
    "        #    myhook = cx.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), fred, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n",
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "\n",
    "\n",
    "    def forward(self, sequence_list, correct):\n",
    "        computation_graph = {}\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (V(torch.zeros(1, len(sequence_list), self.hidden_size)),\n",
    "                  V(torch.zeros(1, len(sequence_list), self.hidden_size)))\n",
    "        \n",
    "        computation_graph[\"enc_h-1\"] = [\"init\", [(\"ZERO\", hidden[0].detach())]]\n",
    "        computation_graph[\"enc_c-1\"] = [\"init\", [(\"ZERO\", hidden[1].detach())]]\n",
    "        \n",
    "        hidden_prev = hidden\n",
    "\n",
    "        # The input is a list of sequences. Here the sequences are converted\n",
    "        # into integer keys\n",
    "        all_seqs = []\n",
    "        for sequence in sequence_list:\n",
    "            this_seq = []\n",
    "            # Iterate over the sequence\n",
    "            for elt in sequence:\n",
    "                ind = self.char2ind[elt]\n",
    "                this_seq.append(ind)\n",
    "            all_seqs.append(torch.LongTensor(this_seq))\n",
    "        max_length = max([len(x) for x in sequence_list])\n",
    "        \n",
    "        index = 0\n",
    "        if max_length > 0:\n",
    "            # Pad the sequences to allow batching \n",
    "            all_seqs = torch.nn.utils.rnn.pad_sequence(all_seqs)\n",
    "\n",
    "            all_seqs_onehot = (all_seqs > 0).type(torch.FloatTensor)\n",
    "\n",
    "            index = 0\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(all_seqs):\n",
    "                cprev_name = \"enc_c\" + str(index-1)\n",
    "                hprev_name = \"enc_h\" + str(index-1)\n",
    "                \n",
    "                # Embed the character\n",
    "                emb = self.embedding(elt.unsqueeze(0))\n",
    "                \n",
    "                computation_graph[\"enc_input\" + str(index)] = [\"emb\", [(\"onehot\", elt), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "                computation_graph[\"enc_inputhidden\" + str(index)] = [\"concat\", [(\"enc_input\" + str(index), emb), (hprev_name, hidden)]]\n",
    "\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, o_t, iph, i_tpre, f_tpre, g_tpre = self.enc_lstm(emb, hidden)\n",
    "                \n",
    "                \n",
    "                \n",
    "                computation_graph[\"enc_h\" + str(index)] = [\"tanhsigmoideltwisemul\", [(\"enc_c\" + str(index), hidden_new[1].detach()), (\"enc_o\" + str(index), o_t.detach())]]\n",
    "                computation_graph[\"enc_c\" + str(index)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"enc_f\" + str(index), f_tpre.detach()), (\"enc_i\" + str(index), i_tpre.detach()), (\"enc_g\" + str(index), g_tpre.detach())]]\n",
    "                computation_graph[\"enc_o\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wo\", self.enc_lstm.wo_weights),(\"enc_bo\", self.enc_lstm.wo_bias)]]\n",
    "                computation_graph[\"enc_f\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wf\", self.enc_lstm.wf_weights),(\"enc_bf\", self.enc_lstm.wf_bias)]]\n",
    "                computation_graph[\"enc_i\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wi\", self.enc_lstm.wi_weights),(\"enc_bi\", self.enc_lstm.wi_bias)]]\n",
    "                computation_graph[\"enc_g\" + str(index)] = [\"weightbias\", [(\"enc_inputhidden\" + str(index), iph.detach()),(\"enc_wg\", self.enc_lstm.wg_weights),(\"enc_bg\", self.enc_lstm.wg_bias)]]\n",
    "\n",
    "                \n",
    "                \n",
    "                hidden_prev = hidden_new\n",
    "\n",
    "                # Awkward solution to variable length inputs: For each sequence in the batch, use the\n",
    "                # new hidden state if the sequence is still being updated, or retain the old\n",
    "                # hidden state if the sequence is over and we're now in the padding\n",
    "                hx = hidden_prev[0] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)) + hidden_new[0] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[0].shape)\n",
    "                cx = hidden_prev[1] * (1 - all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)) + hidden_new[1] * all_seqs_onehot[index].unsqueeze(0).unsqueeze(2).expand(hidden_prev[1].shape)\n",
    "\n",
    "                hidden = (hx, cx)\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = [\"SOS\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_strings = [\"\" for _ in range(len(sequence_list))]\n",
    "\n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "        \n",
    "        cprev_name = \"enc_c\" + str(index)\n",
    "        hprev_name = \"enc_h\" + str(index)\n",
    "\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            if correct[i-1] == \"EOS\":\n",
    "                #print(correct[:i-1])\n",
    "                break\n",
    "            \n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            prev_outputs = []\n",
    "            for elt in prev_output:\n",
    "                ind = self.char2ind[elt]\n",
    "                prev_outputs.append(ind)\n",
    "\n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding(torch.LongTensor([prev_outputs]))\n",
    "            \n",
    "            computation_graph[\"dec_input\" + str(i)] = [\"emb\", [(\"onehot\", ind), (\"emb_mat\", self.embedding.weights)]]\n",
    "\n",
    "            \n",
    "            computation_graph[\"dec_inputhidden\" + str(i)] = [\"concat\", [(\"dec_input\" + str(i), emb), (hprev_name, hidden)]]\n",
    "\n",
    "            hidden_prev = hidden\n",
    "            \n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output(output)\n",
    "            probs = F.log_softmax(pred, dim=2)\n",
    "            logits.append(probs)\n",
    "\n",
    "            \n",
    "            computation_graph[\"logit\" + str(i)] = [\"logsoftmax\", [(\"pred\" + str(i), pred.detach()), self.char2ind[correct[i]]]]\n",
    "            computation_graph[\"pred\" + str(i)] = [\"weightbias\", [(\"dec_h\" + str(i), output.detach()),(\"output_weights\", self.dec_output.weights),(\"output_bias\", self.dec_output.bias)]]\n",
    "            computation_graph[\"dec_h\" + str(i)] = [\"tanhsigmoideltwisemul\", [(\"dec_c\" + str(i), hidden[1].detach()), (\"dec_o\" + str(i), o_t.detach())]]\n",
    "            computation_graph[\"dec_c\" + str(i)] = [\"newc\", [(cprev_name, hidden_prev[1].detach()), (\"dec_f\" + str(i), f_tpre.detach()), (\"dec_i\" + str(i), i_tpre.detach()), (\"dec_g\" + str(i), g_tpre.detach())]]\n",
    "            computation_graph[\"dec_o\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wo\", self.dec_lstm.wo_weights),(\"dec_bo\", self.dec_lstm.wo_bias)]]\n",
    "            computation_graph[\"dec_f\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wf\", self.dec_lstm.wf_weights),(\"dec_bf\", self.dec_lstm.wf_bias)]]\n",
    "            computation_graph[\"dec_i\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wi\", self.dec_lstm.wi_weights),(\"dec_bi\", self.dec_lstm.wi_bias)]]\n",
    "            computation_graph[\"dec_g\" + str(i)] = [\"weightbias\", [(\"dec_inputhidden\" + str(i), iph.detach()),(\"dec_wg\", self.dec_lstm.wg_weights),(\"dec_bg\", self.dec_lstm.wg_bias)]]\n",
    "\n",
    "            \n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            label = topi[0]\n",
    "\n",
    "            prev_output = []\n",
    "            for index, elt in enumerate(label):\n",
    "                char = self.ind2char[elt.item()]\n",
    "\n",
    "                out_strings[index] += char\n",
    "                prev_output.append(char)\n",
    "                \n",
    "            cprev_name = \"dec_c\" + str(i)\n",
    "            hprev_name = \"dec_h\" + str(i)\n",
    "\n",
    "        return out_strings, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts, computation_graph\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grad(args, result):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_grad(args):\n",
    "    name, pred = args[0]\n",
    "    correct_ind = args[1]\n",
    "    \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][correct_ind] = 1.0\n",
    "\n",
    "    exped = torch.exp(pred)\n",
    "    sm = (exped/torch.sum(exped)).view(-1).unsqueeze(0)\n",
    "\n",
    "    mat_grad = torch.transpose(sm,0,1).expand(34,34) - torch.eye(34)\n",
    "\n",
    "    grad_pred = torch.matmul(onehot, torch.transpose(mat_grad,0,1))\n",
    "\n",
    "    return [(name, grad_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightbias_grad(args, result):\n",
    "    name_inp, inp = args[0]\n",
    "    inp = inp.view(-1).unsqueeze(0)\n",
    "    name_weight, weight = args[1]\n",
    "    name_bias, bias = args[2]\n",
    "    \n",
    "    grad_bias = result\n",
    "    grad_weight = torch.transpose(torch.mm(torch.transpose(inp,0,1),grad_bias),0,1)\n",
    "    grad_inp = torch.mm(grad_bias, weight).view(-1).unsqueeze(0)\n",
    "    \n",
    "    return [(name_weight, grad_weight), (name_bias, grad_bias), (name_inp, grad_inp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_grad(args, result):\n",
    "    name_ind, ind = args[0]\n",
    "    name_weight, weight = args[1]\n",
    "    \n",
    "    onehot = torch.zeros(1,34)\n",
    "    onehot[0][ind] = 1.0\n",
    "    \n",
    "    grad_weight = torch.mm(torch.transpose(onehot,0,1),result)\n",
    "    \n",
    "    return [(name_weight, grad_weight)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanhsigmoideltwisemul_grad(args, result):\n",
    "    name_c, ct = args[0]\n",
    "    name_o, ot = args[1]\n",
    "    \n",
    "    grad_ot = torch.sigmoid(ot) * (1 - torch.sigmoid(ot)) * torch.tanh(ct) * result\n",
    "    grad_ct = torch.sigmoid(ot) * result * (1 - torch.pow(torch.tanh(ct),2))\n",
    "    \n",
    "    grad_ot = grad_ot.view(-1).unsqueeze(0)\n",
    "    grad_ct = grad_ct.view(-1).unsqueeze(0)\n",
    "    \n",
    "    return [(name_c, grad_ct), (name_o, grad_ot)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newc_grad(args, result):\n",
    "    name_cprev, cprev = args[0]\n",
    "    name_f, ft = args[1]\n",
    "    name_i, it = args[2]\n",
    "    name_g, gt = args[3]\n",
    "    \n",
    "    grad_cprev = result * torch.sigmoid(ft) # Might be wrong\n",
    "    grad_ft = cprev * result * torch.sigmoid(ft) * (1 - torch.sigmoid(ft))\n",
    "    grad_it = torch.sigmoid(it) * (1 - torch.sigmoid(it)) * torch.tanh(gt) * result\n",
    "    grad_gt = (1 - torch.pow(torch.tanh(gt),2)) * torch.sigmoid(it) * result\n",
    "    \n",
    "    grad_cprev = grad_cprev.view(-1).unsqueeze(0)\n",
    "    grad_ft = grad_ft.view(-1).unsqueeze(0)\n",
    "    grad_it = grad_it.view(-1).unsqueeze(0)\n",
    "    grad_gt = grad_gt.view(-1).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    return [(name_cprev, grad_cprev), (name_f, grad_ft), (name_i, grad_it), (name_g, grad_gt)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_grad(args, result):\n",
    "    inp_name, inp = args[0]\n",
    "    hprev_name, hprev = args[1]\n",
    "    \n",
    "    grad_inp, grad_hprev = torch.split(result, [10,256], dim=1)\n",
    "    \n",
    "    return [(inp_name, grad_inp), (hprev_name, grad_hprev)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {}\n",
    "def init_grads():\n",
    "    gradients[\"dec_wi\"] = None\n",
    "    gradients[\"dec_wf\"] = None\n",
    "    gradients[\"dec_wg\"] = None\n",
    "    gradients[\"dec_wo\"] = None\n",
    "\n",
    "    gradients[\"dec_bi\"] = None\n",
    "    gradients[\"dec_bf\"] = None\n",
    "    gradients[\"dec_bg\"] = None\n",
    "    gradients[\"dec_bo\"] = None\n",
    "\n",
    "    gradients[\"enc_wi\"] = None\n",
    "    gradients[\"enc_wf\"] = None\n",
    "    gradients[\"enc_wg\"] = None\n",
    "    gradients[\"enc_wo\"] = None\n",
    "\n",
    "    gradients[\"enc_bi\"] = None\n",
    "    gradients[\"enc_bf\"] = None\n",
    "    gradients[\"enc_bg\"] = None\n",
    "    gradients[\"enc_bo\"] = None\n",
    "\n",
    "    gradients[\"output_weights\"] = None\n",
    "    gradients[\"output_bias\"] = None\n",
    "\n",
    "    gradients[\"emb_mat\"] = None\n",
    "    \n",
    "init_grads()\n",
    "\n",
    "function2grad = {}\n",
    "function2grad[\"init\"] = init_grad\n",
    "function2grad[\"logsoftmax\"] = logsoftmax_grad\n",
    "function2grad[\"weightbias\"] = weightbias_grad\n",
    "function2grad[\"emb\"] = emb_grad\n",
    "function2grad[\"tanhsigmoideltwisemul\"] = tanhsigmoideltwisemul_grad\n",
    "function2grad[\"newc\"] = newc_grad\n",
    "function2grad[\"concat\"] = concat_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_gradient(cg, name, gradients):\n",
    "    done = False\n",
    "        \n",
    "    grad_type, args = cg[name]\n",
    "    grad_function = function2grad[grad_type]\n",
    "    \n",
    "    results = {}\n",
    "    for result in grad_function(args):\n",
    "        name, grad = result\n",
    "        if name in results:\n",
    "            results[name] = results[name] + grad\n",
    "        else:\n",
    "            results[name] = grad\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if len(results.keys()) == 0:\n",
    "            done = True\n",
    "            break\n",
    "                    \n",
    "        results_new = {}\n",
    "        for name in results:\n",
    "            grad = results[name]\n",
    "            \n",
    "            if name in gradients:\n",
    "                if gradients[name] is None:\n",
    "                    gradients[name] = grad\n",
    "                else:\n",
    "                    gradients[name] = gradients[name] + grad\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                grad_type, args = cg[name]\n",
    "                grad_function = function2grad[grad_type]\n",
    "\n",
    "                to_add = grad_function(args, grad)\n",
    "                for result in to_add:\n",
    "                    name, grad = result\n",
    "                    if name in results_new:\n",
    "                        results_new[name] = results_new[name] + grad\n",
    "                    else:\n",
    "                        results_new[name] = grad\n",
    "\n",
    "        results = results_new \n",
    "        \n",
    "    results = None\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "model.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "cg = model([\"dogsqmasmad\"], \".....................\")[11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_grads()\n",
    "output = backprop_gradient(cg,\"logit0\",gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dec_wi': None,\n",
       " 'dec_wf': None,\n",
       " 'dec_wg': None,\n",
       " 'dec_wo': None,\n",
       " 'dec_bi': None,\n",
       " 'dec_bf': None,\n",
       " 'dec_bg': None,\n",
       " 'dec_bo': None,\n",
       " 'enc_wi': None,\n",
       " 'enc_wf': None,\n",
       " 'enc_wg': None,\n",
       " 'enc_wo': None,\n",
       " 'enc_bi': None,\n",
       " 'enc_bf': None,\n",
       " 'enc_bg': None,\n",
       " 'enc_bo': None,\n",
       " 'output_weights': None,\n",
       " 'output_bias': None,\n",
       " 'emb_mat': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_grads()\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uuxu', '.u.u.xu.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name_dict = {}\n",
    "param_name_dict[\"embedding.weights\"] = \"emb_mat\"\n",
    "param_name_dict[\"enc_lstm.wi_weights\"] = \"enc_wi\"\n",
    "param_name_dict[\"enc_lstm.wi_bias\"] = \"enc_bi\"\n",
    "param_name_dict[\"enc_lstm.wf_weights\"] = \"enc_wf\"\n",
    "param_name_dict[\"enc_lstm.wf_bias\"] = \"enc_bf\"\n",
    "param_name_dict[\"enc_lstm.wg_weights\"] = \"enc_wg\"\n",
    "param_name_dict[\"enc_lstm.wg_bias\"] = \"enc_bg\"\n",
    "param_name_dict[\"enc_lstm.wo_weights\"] = \"enc_wo\"\n",
    "param_name_dict[\"enc_lstm.wo_bias\"] = \"enc_bo\"\n",
    "param_name_dict[\"dec_lstm.wi_weights\"] = \"dec_wi\"\n",
    "param_name_dict[\"dec_lstm.wi_bias\"] = \"dec_bi\"\n",
    "param_name_dict[\"dec_lstm.wf_weights\"] = \"dec_wf\"\n",
    "param_name_dict[\"dec_lstm.wf_bias\"] = \"dec_bf\"\n",
    "param_name_dict[\"dec_lstm.wg_weights\"] = \"dec_wg\"\n",
    "param_name_dict[\"dec_lstm.wg_bias\"] = \"dec_bg\"\n",
    "param_name_dict[\"dec_lstm.wo_weights\"] = \"dec_wo\"\n",
    "param_name_dict[\"dec_lstm.wo_bias\"] = \"dec_bo\"\n",
    "param_name_dict[\"dec_output.weights\"] = \"output_weights\"\n",
    "param_name_dict[\"dec_output.bias\"] = \"output_bias\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set):\n",
    "    model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))\n",
    "    \n",
    "    for index, elt in enumerate(train_set):\n",
    "        print(index)\n",
    "        \n",
    "        inp, outp = elt\n",
    "        print(inp)\n",
    "        \n",
    "        all_outs = model([inp], list(outp) + [\"EOS\", \"z\", \"z\", \"z\"])\n",
    "        \n",
    "        print(outp)\n",
    "        print(all_outs[0][0])\n",
    "        print(outp == all_outs[0][0][:-3])\n",
    "        print(\"\")\n",
    "        cg = all_outs[11]\n",
    "\n",
    "        init_grads()\n",
    "        gc.collect()\n",
    "        if len(outp) > 0:\n",
    "            for i in range(len(outp) + 1):\n",
    "                backprop_gradient(cg,\"logit\"+str(i),gradients)\n",
    "\n",
    "            for name, param in model.named_params():\n",
    "                 model.set_param(name, param - 0.01 * gradients[param_name_dict[name]])\n",
    "                    \n",
    "        cg = None\n",
    "        all_outs = None\n",
    "            \n",
    "      \n",
    "    init_grads()\n",
    "    gc.collect()\n",
    "    cg = None\n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "IIpEp\n",
      ".I.I.pEp.\n",
      ".pII.IEpEOSEOS\n",
      "False\n",
      "\n",
      "1\n",
      "cI\n",
      ".cI.\n",
      ".cI.EOS\n",
      "True\n",
      "\n",
      "2\n",
      "IIIII\n",
      ".I.I.I.I.I.\n",
      ".I...I.I.I.EOS\n",
      "False\n",
      "\n",
      "3\n",
      "EIEE\n",
      ".E.I.E.E.\n",
      ".E.I.E.E.EOS\n",
      "True\n",
      "\n",
      "4\n",
      "mIIxx\n",
      ".mI.I.xEx.\n",
      ".mI.I.xxIx.\n",
      "False\n",
      "\n",
      "5\n",
      "xpEcp\n",
      ".xE.pE.cEp.\n",
      ".pE.pE.cEp.EOS\n",
      "False\n",
      "\n",
      "6\n",
      "IxEII\n",
      ".I.xE.I.I.\n",
      ".I.xE.I.I.EOS\n",
      "True\n",
      "\n",
      "7\n",
      "IEIcI\n",
      ".I.E.I.cI.\n",
      ".I.E.I.cI.EOS\n",
      "True\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "EOS\n",
      "True\n",
      "\n",
      "9\n",
      "EIEIm\n",
      ".E.I.E.Im.\n",
      ".E.I.E.Im.EOS\n",
      "True\n",
      "\n",
      "10\n",
      "xEEI\n",
      ".xE.E.I.\n",
      ".xE.E.I.EOS\n",
      "True\n",
      "\n",
      "11\n",
      "IIEc\n",
      ".I.I.Ec.\n",
      ".I.I.Ec.EOS\n",
      "True\n",
      "\n",
      "12\n",
      "pE\n",
      ".pE.\n",
      ".pE.EOS\n",
      "True\n",
      "\n",
      "13\n",
      "Emcpx\n",
      ".E.mEc.pEx.\n",
      ".E.mEc.xEx.EOS\n",
      "False\n",
      "\n",
      "14\n",
      "pmx\n",
      ".pE.mEx.\n",
      ".pE.mEx.EOS\n",
      "True\n",
      "\n",
      "15\n",
      "pEccm\n",
      ".pEc.cEm.\n",
      ".pEc.cEm.EOS\n",
      "True\n",
      "\n",
      "16\n",
      "xpcI\n",
      ".xEp.cI.\n",
      ".xEp.cI.EOS\n",
      "True\n",
      "\n",
      "17\n",
      "pExEc\n",
      ".pE.xEc.\n",
      ".pE.xEc.EOS\n",
      "True\n",
      "\n",
      "18\n",
      "EEm\n",
      ".E.Em.\n",
      ".E.Em.EOS\n",
      "True\n",
      "\n",
      "19\n",
      "EIEIx\n",
      ".E.I.E.Ix.\n",
      ".E.I.E.Ix.EOS\n",
      "True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model,test_set[1][0][:20])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.rE.EOSEOS.EOSEOS.EOSEOS.EOSEOS.EOSEOS.EOS']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([\"r\"], \"...........................\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
