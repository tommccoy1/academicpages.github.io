{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of abstract language descriptors\n",
    "def load_languages(language_file):\n",
    "    fi = open(language_file, \"r\")\n",
    "    lang_list = []\n",
    "\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        ranking = [int(x) for x in parts[0].split(\",\")]\n",
    "        vowel_inventory = parts[1].split(\",\")\n",
    "        consonant_inventory = parts[2].split(\",\")\n",
    "\n",
    "        lang = [ranking, vowel_inventory, consonant_inventory]\n",
    "\n",
    "        lang_list.append(lang)\n",
    "\n",
    "    return lang_list\n",
    "\n",
    "# Load the file input/output correspondences\n",
    "def load_io(io_file):\n",
    "    fi = open(io_file, \"r\")\n",
    "\n",
    "    io_correspondences = {}\n",
    "\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        ranking = tuple([int(x) for x in parts[0].split(\",\")])\n",
    "\n",
    "        value = parts[1]\n",
    "        value_groups = value.split(\"&\")\n",
    "\n",
    "        value_list = []\n",
    "\n",
    "        for group in value_groups:\n",
    "            components = group.split(\"#\")\n",
    "            inp = components[0]\n",
    "            outp = components[1]\n",
    "            steps = components[2].split(\",\")\n",
    "\n",
    "            value_list.append([inp, outp, steps])\n",
    "\n",
    "        io_correspondences[ranking] = value_list\n",
    "\n",
    "    return io_correspondences\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        dev_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[2].split()]\n",
    "        vocab = parts[3].split()\n",
    "        key_string = parts[4].split(\",\")\n",
    "\n",
    "        v_list = key_string[0].split()\n",
    "        c_list = key_string[1].split()\n",
    "        ranking = [int(x) for x in key_string[2].split()]\n",
    "\n",
    "        key = [v_list, c_list, ranking]\n",
    "\n",
    "        langs.append([train_set, dev_set, test_set, vocab, key])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset_scramble(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    all_train_sets = []\n",
    "    all_dev_sets = []\n",
    "    all_test_sets = []\n",
    "\n",
    "    n_tasks = 0\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        dev_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[2].split()]\n",
    "        all_train_sets += train_set\n",
    "        all_dev_sets += dev_set\n",
    "        all_test_sets += test_set\n",
    "\n",
    "        vocab = parts[3].split()\n",
    "\n",
    "        n_tasks += 1\n",
    "\n",
    "    shuffle(all_train_sets)\n",
    "    shuffle(all_dev_sets)\n",
    "    shuffle(all_test_sets)\n",
    "\n",
    "    train_len = len(train_set)\n",
    "    dev_len = len(dev_set)\n",
    "    test_len = len(test_set)\n",
    "\n",
    "\n",
    "    for i in range(n_tasks):\n",
    "        train_set = all_train_sets[i*train_len:(i+1)*train_len]\n",
    "        dev_set = all_dev_sets[i*dev_len:(i+1)*dev_len]\n",
    "        test_set = all_test_sets[i*test_len:(i+1)*test_len]\n",
    "\n",
    "        v_list = \"scrambled\"\n",
    "        c_list = \"scrambled\"\n",
    "        ranking = \"scrambled\"\n",
    "\n",
    "        key = [v_list, c_list, ranking]\n",
    "\n",
    "        langs.append([train_set, dev_set, test_set, vocab, key])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a language that is just Cs and Vs\n",
    "def load_dataset_cv(dataset_file):\n",
    "    fi = open(dataset_file, \"r\")\n",
    "\n",
    "    langs = []\n",
    "    for line in fi:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        train_set = [elt.split(\",\") for elt in parts[0].split()]\n",
    "        test_set = [elt.split(\",\") for elt in parts[1].split()]\n",
    "        vocab = parts[2].split()\n",
    "\n",
    "        langs.append([train_set, test_set, vocab])\n",
    "\n",
    "    return langs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(\"yonc.test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break a list into batches of the desired size\n",
    "def batchify_list(lst, batch_size=100):\n",
    "    batches = []\n",
    "    this_batch_in = []\n",
    "    this_batch_out = []\n",
    "\n",
    "    for index, elt in enumerate(lst):\n",
    "        #print(elt)\n",
    "        this_batch_in.append(elt[0])\n",
    "        this_batch_out.append(elt[1])\n",
    "\n",
    "        if (index + 1) % batch_size == 0:\n",
    "            batches.append([this_batch_in, this_batch_out])\n",
    "            this_batch_in = []\n",
    "            this_batch_out = []\n",
    "\n",
    "    if this_batch_in != []:\n",
    "        batches.append([this_batch_in, this_batch_out])\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Trim the excess from the end of an output string\n",
    "def process_output(output):\n",
    "    if \"EOS\" in output:\n",
    "        return output[:output.index(\"EOS\")]\n",
    "    else:\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine a basic PyTorch model to allow\n",
    "# for double gradients and manual modification\n",
    "# of weights\n",
    "class ModifiableModule():\n",
    "    def params(self):\n",
    "        return [p for _, p in self.named_params()]\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return []\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return []\n",
    "\n",
    "    def named_params(self):\n",
    "        subparams = []\n",
    "        for name, mod in self.named_submodules():\n",
    "            for subname, param in mod.named_params():\n",
    "                subparams.append((name + '.' + subname, param))\n",
    "        return self.named_leaves() + subparams\n",
    "\n",
    "    def set_param(self, name, param):\n",
    "        if '.' in name:\n",
    "            n = name.split('.')\n",
    "            module_name = n[0]\n",
    "            rest = '.'.join(n[1:])\n",
    "            for name, mod in self.named_submodules():\n",
    "                if module_name == name:\n",
    "                    mod.set_param(rest, param)\n",
    "                    break\n",
    "        else:\n",
    "            setattr(self, name, param)\n",
    "\n",
    "    def copy(self, other, same_var=False):\n",
    "        for name, param in other.named_params():\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "            self.set_param(name, param)\n",
    "\n",
    "\n",
    "    def load_state_dict(self, sdict, same_var=False):\n",
    "        for name in sdict:\n",
    "            param = sdict[name]\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "\n",
    "            self.set_param(name, param)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return OrderedDict(self.named_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined linear layer\n",
    "class GradLinear(ModifiableModule):\n",
    "    def __init__(self, inp_size, outp_size):\n",
    "        super(GradLinear, self).__init__()\n",
    "        self.weights = np.random.rand(outp_size, inp_size)\n",
    "        self.bias = np.random.rand(outp_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return np.matmul(self.weights,x) + self.bias\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights), ('bias', self.bias)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def logsoftmax(x):\n",
    "    return np.log(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined LSTM\n",
    "class GradLSTM(ModifiableModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GradLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.wi_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wi_bias = np.random.rand(hidden_size)\n",
    "        self.wf_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wf_bias = np.random.rand(hidden_size)\n",
    "        self.wg_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wg_bias = np.random.rand(hidden_size)\n",
    "        self.wo_weights = np.random.rand(hidden_size, hidden_size + input_size)\n",
    "        self.wo_bias = np.random.rand(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        hx, cx = hidden\n",
    "        \n",
    "        input_plus_hidden = np.concatenate([inp.flatten(), hx.flatten()])\n",
    "\n",
    "        i_tpre = np.matmul(self.wi_weights,input_plus_hidden) + self.wi_bias\n",
    "        i_t = sigmoid(i_tpre)\n",
    "        f_tpre = np.matmul(self.wf_weights,input_plus_hidden) + self.wf_bias\n",
    "        f_t = sigmoid(f_tpre)\n",
    "        g_tpre = np.matmul(self.wg_weights,input_plus_hidden) + self.wg_bias\n",
    "        g_t = tanh(g_tpre)\n",
    "        o_tpre = np.matmul(self.wo_weights,input_plus_hidden) + self.wo_bias\n",
    "        o_t = sigmoid(o_tpre)\n",
    "        #print(i_t)\n",
    "        #print(f_t)\n",
    "        #print(g_t)\n",
    "        #print(o_t)\n",
    "\n",
    "        cx = f_t * cx + i_t * g_t\n",
    "        hx = o_t * tanh(cx)\n",
    "\n",
    "        #myhook = input_plus_hidden.register_hook(print_grad)\n",
    "\n",
    "        return hx, (hx, cx), o_tpre, input_plus_hidden, i_tpre, f_tpre, g_tpre\n",
    "\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('wi_weights', self.wi_weights), ('wi_bias', self.wi_bias),\n",
    "                ('wf_weights', self.wf_weights), ('wf_bias', self.wf_bias),\n",
    "                ('wg_weights', self.wg_weights), ('wg_bias', self.wg_bias),\n",
    "                ('wo_weights', self.wo_weights), ('wo_bias', self.wo_bias)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefined embedding layer\n",
    "class GradEmbedding(ModifiableModule):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(GradEmbedding, self).__init__()\n",
    "        self.weights = np.random.rand(emb_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.matmul(self.weights,x)\n",
    "\n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onehot(ind):\n",
    "    oh = np.zeros(34)\n",
    "    oh[ind] = 1.0\n",
    "    \n",
    "    return oh\n",
    "\n",
    "onehot(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/decoder model\n",
    "class EncoderDecoder(ModifiableModule):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = GradEmbedding(vocab_size, input_size)\n",
    "        self.enc_lstm = GradLSTM(input_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = GradLSTM(input_size, hidden_size)\n",
    "        self.dec_output = GradLinear(hidden_size, vocab_size)\n",
    "\n",
    "        self.max_length = 20\n",
    "        \n",
    "        self.set_dicts(\"a e i o u A E I O U b c d f g h j k l m n p q r s t v w x z .\".split())\n",
    "\n",
    "\n",
    "    def forward(self, inp, outp_length=20):\n",
    "        # Initialize the hidden and cell states\n",
    "        hidden = (np.zeros([1,self.hidden_size]), np.zeros([1,self.hidden_size]))\n",
    "\n",
    "        this_seq = []\n",
    "        # Iterate over the sequence\n",
    "        for elt in inp:\n",
    "            ind = self.char2ind[elt]\n",
    "            this_seq.append(ind)\n",
    "        \n",
    "        inp_length = len(inp)\n",
    "        if inp_length > 0:\n",
    "\n",
    "            # Pass the sequences through the encoder, one character at a time\n",
    "            for index, elt in enumerate(this_seq):\n",
    "                # Embed the character\n",
    "                emb = self.embedding.forward(onehot(elt))\n",
    "\n",
    "                # Pass through the LSTM\n",
    "                output, hidden_new, _, _, i_tpre, f_tpre, g_tpre = self.enc_lstm.forward(emb, hidden)\n",
    "                hidden_prev = hidden\n",
    "\n",
    "\n",
    "                hidden = hidden_new\n",
    "\n",
    "        encoding = hidden\n",
    "        # Decoding\n",
    "\n",
    "        # Previous output characters (used as input for the following time step)\n",
    "        prev_output = \"SOS\"\n",
    "\n",
    "        # Accumulates the output sequences\n",
    "        out_string = \"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        # Probabilities at each output position (used for computing the loss)\n",
    "        logits = []\n",
    "        preds = []\n",
    "        hiddens = []\n",
    "        ots = []\n",
    "        iphs = []\n",
    "        hidden_prev = hidden\n",
    "        its = []\n",
    "        fts = []\n",
    "        gts = []\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(min(self.max_length,outp_length)):\n",
    "            # Determine the previous output character for each element\n",
    "            # of the batch; to be used as the input for this time step\n",
    "            \n",
    "            # Embed the previous outputs\n",
    "            emb = self.embedding.forward(onehot(self.char2ind[prev_output]))\n",
    "\n",
    "            # Pass through the decoder\n",
    "            output, hidden, o_t, iph, i_tpre, f_tpre, g_tpre = self.dec_lstm.forward(emb, hidden)\n",
    "            #myhook = o_t.register_hook(print_grad)\n",
    "\n",
    "            # Determine the output probabilities used to make predictions\n",
    "            pred = self.dec_output.forward(output.flatten())\n",
    "            probs = logsoftmax(pred)\n",
    "            logits.append(probs)\n",
    "            preds.append(pred)\n",
    "            hiddens.append(hidden)\n",
    "            ots.append(o_t)\n",
    "            iphs.append(iph)\n",
    "            its.append(i_tpre)\n",
    "            fts.append(f_tpre)\n",
    "            gts.append(g_tpre)\n",
    "\n",
    "            # Discretize the output labels (via argmax) for generating an output character\n",
    "            label = np.argmax(probs)\n",
    "\n",
    "            char = self.ind2char[label]\n",
    "            out_string += char\n",
    "            prev_output = char\n",
    "            \n",
    "\n",
    "        return out_string, logits, encoding, preds, hiddens, ots, iphs, hidden_prev, its, fts, gts\n",
    "\n",
    "    def named_submodules(self):\n",
    "        return [('embedding', self.embedding), ('enc_lstm', self.enc_lstm),\n",
    "                ('dec_lstm', self.dec_lstm), ('dec_output', self.dec_output)]\n",
    "\n",
    "    # Create a copy of the model\n",
    "    def create_copy(self, same_var=False):\n",
    "        new_model = EncoderDecoder(self.vocab_size, self.input_size, self.hidden_size)\n",
    "        new_model.copy(self, same_var=same_var)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "    def set_dicts(self, vocab_list):\n",
    "        vocab_list = [\"NULL\", \"SOS\", \"EOS\"] + vocab_list\n",
    "\n",
    "        index = 0\n",
    "        char2ind = {}\n",
    "        ind2char = {}\n",
    "\n",
    "        for elt in vocab_list:\n",
    "            char2ind[elt] = index\n",
    "            ind2char[index] = elt\n",
    "            index += 1\n",
    "\n",
    "        self.char2ind = char2ind\n",
    "        self.ind2char = ind2char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec = EncoderDecoder(34,10,256)\n",
    "\n",
    "encdec.enc_lstm.wo_weights = np.loadtxt(\"enc_lstm.wo_weights\")\n",
    "encdec.enc_lstm.wi_weights = np.loadtxt(\"enc_lstm.wi_weights\")\n",
    "encdec.enc_lstm.wg_weights = np.loadtxt(\"enc_lstm.wg_weights\")\n",
    "encdec.enc_lstm.wf_weights = np.loadtxt(\"enc_lstm.wf_weights\")\n",
    "encdec.enc_lstm.wo_bias = np.loadtxt(\"enc_lstm.wo_bias\")\n",
    "encdec.enc_lstm.wi_bias = np.loadtxt(\"enc_lstm.wi_bias\")\n",
    "encdec.enc_lstm.wg_bias = np.loadtxt(\"enc_lstm.wg_bias\")\n",
    "encdec.enc_lstm.wf_bias = np.loadtxt(\"enc_lstm.wf_bias\")\n",
    "\n",
    "encdec.dec_lstm.wo_weights = np.loadtxt(\"dec_lstm.wo_weights\")\n",
    "encdec.dec_lstm.wi_weights = np.loadtxt(\"dec_lstm.wi_weights\")\n",
    "encdec.dec_lstm.wg_weights = np.loadtxt(\"dec_lstm.wg_weights\")\n",
    "encdec.dec_lstm.wf_weights = np.loadtxt(\"dec_lstm.wf_weights\")\n",
    "encdec.dec_lstm.wo_bias = np.loadtxt(\"dec_lstm.wo_bias\")\n",
    "encdec.dec_lstm.wi_bias = np.loadtxt(\"dec_lstm.wi_bias\")\n",
    "encdec.dec_lstm.wg_bias = np.loadtxt(\"dec_lstm.wg_bias\")\n",
    "encdec.dec_lstm.wf_bias = np.loadtxt(\"dec_lstm.wf_bias\")\n",
    "\n",
    "encdec.embedding.weights = np.loadtxt(\"embedding.weights\").transpose()\n",
    "encdec.dec_output.weights = np.loadtxt(\"dec_output.weights\")\n",
    "encdec.dec_output.bias = np.loadtxt(\"dec_output.bias\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-3.01279152e+01, -3.00848924e+01, -9.03083511e+00, -1.56202118e+01,\n",
       "        -1.70851091e+01, -1.41661532e+01, -1.22493396e+01, -1.84939267e+01,\n",
       "        -1.81772379e+01, -1.75917400e+01, -1.60071577e+01, -1.67578941e+01,\n",
       "        -1.52173436e+01, -1.89212427e+01, -1.68303586e+01, -1.19849026e+01,\n",
       "        -1.89060033e+01, -2.01457507e+01, -1.97769620e+01, -1.80531533e+01,\n",
       "        -1.76944151e+01, -2.08710772e+01, -1.59148652e+01, -2.03524548e+01,\n",
       "        -1.70581634e+01, -2.12773568e+01, -1.88027619e+01, -1.74220780e+01,\n",
       "        -1.81387134e+01, -1.71793718e+01, -1.68596371e+01, -1.89546851e+01,\n",
       "        -1.66746885e+01, -1.32517483e-04]),\n",
       " array([-3.14575991e+01, -3.15137758e+01, -1.26959036e+01, -2.07247396e+01,\n",
       "        -2.12164366e+01, -1.81320284e+01, -1.24132728e+01, -2.67874971e+01,\n",
       "        -2.33397049e+01, -2.17807042e+01, -1.94993713e+01, -1.93487731e+01,\n",
       "        -2.05535019e+01, -1.43962494e+01, -1.06567485e+01, -1.20069616e-03,\n",
       "        -1.30311754e+01, -1.66945707e+01, -1.49215861e+01, -8.56598238e+00,\n",
       "        -1.06712757e+01, -1.68096925e+01, -9.69655949e+00, -1.29411665e+01,\n",
       "        -8.89847485e+00, -1.73865702e+01, -1.46887029e+01, -1.13443802e+01,\n",
       "        -1.13372489e+01, -9.55783501e+00, -8.11035183e+00, -1.40517565e+01,\n",
       "        -7.95134880e+00, -1.25098437e+01]),\n",
       " array([-3.24653538e+01, -3.23654189e+01, -1.60642582e+01, -1.13562904e+01,\n",
       "        -1.42446994e+01, -8.20122687e+00, -3.61397200e-04, -1.82077842e+01,\n",
       "        -1.72302956e+01, -1.10026124e+01, -1.10521279e+01, -1.01819967e+01,\n",
       "        -1.25235296e+01, -2.22859166e+01, -2.33376259e+01, -1.73578915e+01,\n",
       "        -2.29573968e+01, -2.08910895e+01, -2.26348636e+01, -2.01610332e+01,\n",
       "        -2.31309361e+01, -2.54595819e+01, -2.06032999e+01, -2.49518064e+01,\n",
       "        -2.03233844e+01, -2.57524006e+01, -2.29253548e+01, -1.97993461e+01,\n",
       "        -2.20426512e+01, -2.14110087e+01, -1.97959295e+01, -2.18355197e+01,\n",
       "        -2.31030458e+01, -1.45508377e+01]),\n",
       " array([-3.29866895e+01, -3.29138342e+01, -1.36914171e+01, -2.06708981e+01,\n",
       "        -2.27494253e+01, -1.84367048e+01, -1.30464583e+01, -2.34397573e+01,\n",
       "        -2.30497800e+01, -2.15863632e+01, -1.81826691e+01, -2.00998281e+01,\n",
       "        -1.97609787e+01, -1.91267547e+01, -2.01993829e+01, -1.58391920e+01,\n",
       "        -1.88394682e+01, -1.63594957e+01, -1.80875531e+01, -1.59854551e+01,\n",
       "        -2.09268581e+01, -2.23149632e+01, -1.86916904e+01, -2.26671218e+01,\n",
       "        -1.70691981e+01, -2.30786716e+01, -2.11548697e+01, -1.71637573e+01,\n",
       "        -1.93316271e+01, -1.87235914e+01, -1.86343557e+01, -2.09291397e+01,\n",
       "        -1.94561548e+01, -3.77785145e-06]),\n",
       " array([-3.64148224e+01, -3.63577592e+01, -1.21657995e-05, -2.30227396e+01,\n",
       "        -2.26139024e+01, -2.18115545e+01, -1.55366806e+01, -2.52961080e+01,\n",
       "        -2.24196088e+01, -2.23871202e+01, -2.04605915e+01, -2.10423043e+01,\n",
       "        -2.08351067e+01, -1.71549708e+01, -1.71986814e+01, -1.42500700e+01,\n",
       "        -1.51130544e+01, -1.80887379e+01, -1.51407363e+01, -1.71940180e+01,\n",
       "        -1.71974424e+01, -1.91817509e+01, -1.58904713e+01, -1.94625210e+01,\n",
       "        -1.70882767e+01, -1.99268762e+01, -1.89290171e+01, -1.65612509e+01,\n",
       "        -2.08000623e+01, -1.78561389e+01, -1.69433756e+01, -1.88007957e+01,\n",
       "        -1.65852740e+01, -1.14863678e+01]),\n",
       " array([-27.92881596, -28.06423237,  -2.30617628,  -7.85073213,\n",
       "         -8.62866018,  -6.13287483,  -0.14115509,  -9.70777322,\n",
       "        -10.26585686,  -7.18923936,  -4.52429227,  -5.48915792,\n",
       "         -7.13018514, -14.09842394, -15.1885853 , -14.27141721,\n",
       "        -14.86294836, -12.25716493, -12.82462364, -13.93436084,\n",
       "        -16.82492147, -15.36339244, -15.27346065, -17.07105391,\n",
       "        -13.25761339, -15.28241652, -13.15282444, -14.41085683,\n",
       "        -14.7091347 , -14.23340922, -16.01606668, -12.64513442,\n",
       "        -16.04059685,  -4.37241278]),\n",
       " array([-3.50138852e+01, -3.47369325e+01, -6.81284298e+00, -2.20269819e+01,\n",
       "        -1.99046851e+01, -2.01287182e+01, -1.54855373e+01, -2.07034792e+01,\n",
       "        -1.96083496e+01, -2.22081879e+01, -1.57585109e+01, -1.76181332e+01,\n",
       "        -1.88209227e+01, -2.01215020e+01, -2.08764220e+01, -1.72065511e+01,\n",
       "        -1.88824117e+01, -1.95011261e+01, -1.64352105e+01, -2.02297115e+01,\n",
       "        -2.04679141e+01, -2.19355354e+01, -2.05565199e+01, -2.11204265e+01,\n",
       "        -1.89377359e+01, -2.11230666e+01, -2.31785939e+01, -2.12423055e+01,\n",
       "        -2.25032290e+01, -2.16272300e+01, -2.18473576e+01, -1.93195775e+01,\n",
       "        -1.96979008e+01, -1.10067614e-03]),\n",
       " array([-3.92740809e+01, -3.92871900e+01, -2.99209193e-06, -2.48299415e+01,\n",
       "        -2.35844549e+01, -2.40267612e+01, -1.90305559e+01, -2.44103859e+01,\n",
       "        -2.33562244e+01, -2.47488717e+01, -2.03332332e+01, -2.26999400e+01,\n",
       "        -2.23060963e+01, -1.91770742e+01, -1.88968079e+01, -1.69833456e+01,\n",
       "        -1.66279991e+01, -1.95519638e+01, -1.47864670e+01, -1.94998232e+01,\n",
       "        -1.99978024e+01, -1.99891882e+01, -2.01185813e+01, -1.95797709e+01,\n",
       "        -1.91380862e+01, -2.06960452e+01, -2.20248469e+01, -1.94234475e+01,\n",
       "        -2.21803845e+01, -2.09381995e+01, -2.06378767e+01, -1.70528925e+01,\n",
       "        -1.82602156e+01, -1.29341823e+01]),\n",
       " array([-27.83829351, -28.05069221,  -0.69873871,  -6.3897987 ,\n",
       "         -7.09332903,  -5.45128068,  -1.07793653,  -6.97992124,\n",
       "         -7.76857132,  -6.56096466,  -2.23473481,  -3.6948525 ,\n",
       "         -4.8495875 , -14.15481624, -14.47443844, -14.27653441,\n",
       "        -12.87470201, -12.42431484, -10.92668289, -13.8120923 ,\n",
       "        -14.57823081, -12.38251223, -15.47131832, -14.46366187,\n",
       "        -12.8009295 , -12.67995825, -13.08455843, -13.7006278 ,\n",
       "        -14.0883883 , -14.33221316, -15.65309704,  -9.944152  ,\n",
       "        -15.01740143,  -4.3329843 ]),\n",
       " array([-3.26448321e+01, -3.23666229e+01, -8.16421771e+00, -2.07517592e+01,\n",
       "        -1.88690842e+01, -1.92694470e+01, -1.56940585e+01, -1.85760905e+01,\n",
       "        -1.84380563e+01, -2.11068224e+01, -1.46174637e+01, -1.74627729e+01,\n",
       "        -1.68683125e+01, -2.01185553e+01, -1.81123200e+01, -1.70450966e+01,\n",
       "        -1.73085623e+01, -2.08617799e+01, -1.54662428e+01, -2.05792550e+01,\n",
       "        -2.06769808e+01, -1.98682703e+01, -2.07503420e+01, -1.82430625e+01,\n",
       "        -1.98315815e+01, -1.99519532e+01, -2.22261613e+01, -2.05180425e+01,\n",
       "        -2.22242714e+01, -2.10826858e+01, -2.15122557e+01, -1.75200076e+01,\n",
       "        -1.80275658e+01, -2.85747863e-04]),\n",
       " array([-3.54664735e+01, -3.54570664e+01, -4.78231812e-06, -2.37810019e+01,\n",
       "        -2.28534684e+01, -2.34573657e+01, -1.90375375e+01, -2.29175380e+01,\n",
       "        -2.28908887e+01, -2.46802448e+01, -1.94632964e+01, -2.33017800e+01,\n",
       "        -2.01958412e+01, -1.92468944e+01, -1.72477434e+01, -1.58547942e+01,\n",
       "        -1.42822185e+01, -2.03583405e+01, -1.41847451e+01, -1.81911805e+01,\n",
       "        -1.92664383e+01, -1.82699704e+01, -1.94216289e+01, -1.69522536e+01,\n",
       "        -1.85355869e+01, -1.98058753e+01, -2.11224353e+01, -1.79915082e+01,\n",
       "        -2.08069253e+01, -2.02446989e+01, -1.94460301e+01, -1.52275471e+01,\n",
       "        -1.63778551e+01, -1.27665712e+01]),\n",
       " array([-25.9425287 , -26.19765045,  -0.70238344,  -5.77395713,\n",
       "         -6.07581842,  -4.85563146,  -1.17800331,  -6.8623806 ,\n",
       "         -7.45777458,  -7.38438958,  -2.06531144,  -3.92239595,\n",
       "         -3.47334708, -13.69551181, -13.13010668, -12.87986719,\n",
       "        -11.85190447, -12.22578851, -10.63619426, -12.55302139,\n",
       "        -14.09467341, -11.72957042, -14.87449149, -13.26871523,\n",
       "        -12.14469289, -12.75298354, -12.75005065, -12.57727064,\n",
       "        -12.47573124, -14.03121943, -14.25923323,  -8.85869185,\n",
       "        -13.45881466,  -5.66500492]),\n",
       " array([-2.99402113e+01, -2.97854331e+01, -9.72954482e+00, -1.99487012e+01,\n",
       "        -1.75710482e+01, -1.79584445e+01, -1.46719805e+01, -1.80408245e+01,\n",
       "        -1.75621908e+01, -2.05008887e+01, -1.36580190e+01, -1.69521179e+01,\n",
       "        -1.53950999e+01, -2.05180673e+01, -1.70800182e+01, -1.60689844e+01,\n",
       "        -1.70523186e+01, -2.10085825e+01, -1.63598681e+01, -1.95638190e+01,\n",
       "        -2.06509450e+01, -1.96348480e+01, -2.01810985e+01, -1.85350193e+01,\n",
       "        -1.93841960e+01, -2.04620849e+01, -2.11198075e+01, -1.94326515e+01,\n",
       "        -2.01259049e+01, -1.99472208e+01, -1.99793588e+01, -1.78773627e+01,\n",
       "        -1.70953134e+01, -6.17781801e-05]),\n",
       " array([-3.35567649e+01, -3.35407474e+01, -1.52213510e-05, -2.37350529e+01,\n",
       "        -2.12491186e+01, -2.26633072e+01, -1.82305524e+01, -2.19961168e+01,\n",
       "        -2.14883176e+01, -2.41981870e+01, -1.76407364e+01, -2.30445782e+01,\n",
       "        -1.86018787e+01, -1.88202962e+01, -1.57190979e+01, -1.38393332e+01,\n",
       "        -1.29741586e+01, -1.98748783e+01, -1.40545858e+01, -1.66522026e+01,\n",
       "        -1.81333709e+01, -1.77657759e+01, -1.79187501e+01, -1.60883675e+01,\n",
       "        -1.73593035e+01, -1.91634481e+01, -2.00581908e+01, -1.73504327e+01,\n",
       "        -1.91439989e+01, -1.91619859e+01, -1.76251938e+01, -1.47129088e+01,\n",
       "        -1.42275477e+01, -1.15580551e+01]),\n",
       " array([-20.82600894, -21.17326142,  -0.60533811,  -7.044712  ,\n",
       "         -5.51975534,  -5.39893161,  -1.61348256,  -7.64111941,\n",
       "         -7.31863802,  -8.68720271,  -1.62011726,  -4.76758132,\n",
       "         -3.32454159, -12.27287511, -11.74267408, -11.12636981,\n",
       "        -10.76091894, -11.39183834, -10.08077699, -10.9457052 ,\n",
       "        -13.40318452, -11.17330286, -13.19329461, -12.11664577,\n",
       "        -11.19624391, -12.526025  , -11.98655641, -11.46324231,\n",
       "        -11.12593085, -12.86976048, -12.88547515,  -8.49869042,\n",
       "        -11.38215782,  -6.5511901 ]),\n",
       " array([-2.47982326e+01, -2.47742109e+01, -8.82442669e+00, -1.67353068e+01,\n",
       "        -1.52444969e+01, -1.56256631e+01, -1.24163556e+01, -1.51662711e+01,\n",
       "        -1.45537771e+01, -1.74936048e+01, -1.14698108e+01, -1.40534378e+01,\n",
       "        -1.29319523e+01, -1.78190366e+01, -1.56138249e+01, -1.42805714e+01,\n",
       "        -1.49940739e+01, -1.78226498e+01, -1.48648674e+01, -1.61486351e+01,\n",
       "        -1.88973835e+01, -1.77399041e+01, -1.69271592e+01, -1.79581214e+01,\n",
       "        -1.74902854e+01, -1.97050154e+01, -1.76149146e+01, -1.57064074e+01,\n",
       "        -1.66510816e+01, -1.80343662e+01, -1.68756606e+01, -1.66444736e+01,\n",
       "        -1.57282554e+01, -1.68229271e-04]),\n",
       " array([-3.18127196e+01, -3.16961355e+01, -4.02075408e-05, -2.21225195e+01,\n",
       "        -2.03425128e+01, -2.16127948e+01, -1.71599208e+01, -2.01260402e+01,\n",
       "        -1.93063811e+01, -2.27666336e+01, -1.62028476e+01, -2.10505332e+01,\n",
       "        -1.71882384e+01, -1.75829507e+01, -1.50300941e+01, -1.27858171e+01,\n",
       "        -1.27162930e+01, -1.83871631e+01, -1.37936976e+01, -1.49422558e+01,\n",
       "        -1.76053036e+01, -1.69669155e+01, -1.58456417e+01, -1.64062540e+01,\n",
       "        -1.67878859e+01, -1.93648871e+01, -1.85115782e+01, -1.59365538e+01,\n",
       "        -1.76189048e+01, -1.86120070e+01, -1.64694482e+01, -1.51171810e+01,\n",
       "        -1.29543258e+01, -1.04357694e+01]),\n",
       " array([-18.21172316, -18.53489905,  -0.33123743,  -8.45918794,\n",
       "         -6.74695832,  -6.24140755,  -1.95251705,  -8.76553961,\n",
       "         -8.05995077,  -9.06086864,  -2.1417945 ,  -5.65912629,\n",
       "         -4.39384316, -11.41343642, -10.52941194, -10.10455988,\n",
       "         -9.71629258, -10.31163055,  -9.76952574,  -9.77309834,\n",
       "        -12.29922222, -10.77758947, -12.22070826, -11.41649368,\n",
       "        -10.4657572 , -12.11151178, -11.17091423, -10.48500757,\n",
       "         -9.95178054, -12.17977185, -11.23389137,  -7.67112456,\n",
       "        -10.23124001,  -6.28029304]),\n",
       " array([-2.10439744e+01, -2.10674214e+01, -6.91216397e+00, -1.42624206e+01,\n",
       "        -1.28897732e+01, -1.31907319e+01, -9.88685848e+00, -1.28097637e+01,\n",
       "        -1.21377678e+01, -1.48691467e+01, -9.12331301e+00, -1.18052916e+01,\n",
       "        -1.04652773e+01, -1.57552129e+01, -1.42699869e+01, -1.30878147e+01,\n",
       "        -1.33995061e+01, -1.58012252e+01, -1.34052863e+01, -1.39163981e+01,\n",
       "        -1.71476353e+01, -1.60822826e+01, -1.54511868e+01, -1.62635799e+01,\n",
       "        -1.55274951e+01, -1.81735126e+01, -1.55084144e+01, -1.35189931e+01,\n",
       "        -1.45188389e+01, -1.65984204e+01, -1.47156298e+01, -1.45582259e+01,\n",
       "        -1.42974899e+01, -1.21682811e-03]),\n",
       " array([-2.99514966e+01, -2.97620757e+01, -1.44590733e-04, -2.02927052e+01,\n",
       "        -1.98681674e+01, -2.01433428e+01, -1.57816918e+01, -1.91540151e+01,\n",
       "        -1.84678669e+01, -2.13092703e+01, -1.57427956e+01, -1.93783429e+01,\n",
       "        -1.61078507e+01, -1.68465701e+01, -1.39677738e+01, -1.23535774e+01,\n",
       "        -1.19135991e+01, -1.76256582e+01, -1.34116830e+01, -1.40426645e+01,\n",
       "        -1.62645045e+01, -1.63345407e+01, -1.56710087e+01, -1.56774787e+01,\n",
       "        -1.58766499e+01, -1.83228812e+01, -1.74923951e+01, -1.51807741e+01,\n",
       "        -1.66621247e+01, -1.76146980e+01, -1.54495034e+01, -1.42896534e+01,\n",
       "        -1.21143706e+01, -9.00611861e+00])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encdec.forward(\"do\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from utils import *\n",
    "from training import *\n",
    "from models import *\n",
    "\n",
    "model = EncoderDecoder(34,10,256)\n",
    "model.load_state_dict(torch.load(\"maml_yonc_256_5.weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_lstm.wo_weights.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"embedding.weights\",model.embedding.weights.data.numpy())\n",
    "\n",
    "np.savetxt(\"enc_lstm.wi_weights\",model.enc_lstm.wi_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wi_bias\",model.enc_lstm.wi_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wf_weights\",model.enc_lstm.wf_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wf_bias\",model.enc_lstm.wf_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wg_weights\",model.enc_lstm.wg_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wg_bias\",model.enc_lstm.wg_bias.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wo_weights\",model.enc_lstm.wo_weights.data.numpy())\n",
    "np.savetxt(\"enc_lstm.wo_bias\",model.enc_lstm.wo_bias.data.numpy())\n",
    "\n",
    "np.savetxt(\"dec_lstm.wi_weights\",model.dec_lstm.wi_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wi_bias\",model.dec_lstm.wi_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wf_weights\",model.dec_lstm.wf_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wf_bias\",model.dec_lstm.wf_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wg_weights\",model.dec_lstm.wg_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wg_bias\",model.dec_lstm.wg_bias.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wo_weights\",model.dec_lstm.wo_weights.data.numpy())\n",
    "np.savetxt(\"dec_lstm.wo_bias\",model.dec_lstm.wo_bias.data.numpy())\n",
    "\n",
    "np.savetxt(\"dec_output.weights\",model.dec_output.weights.data.numpy())\n",
    "np.savetxt(\"dec_output.bias\",model.dec_output.bias.data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in model.named_params()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
